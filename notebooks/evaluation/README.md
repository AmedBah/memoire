# ğŸ“Š Ã‰valuation Comparative des Architectures

Ce dossier contient les notebooks et ressources pour l'Ã©valuation des trois architectures conversationnelles dÃ©veloppÃ©es pour EasyTransfert.

## ğŸ“ Contenu

### Notebooks

- **`04_evaluation_comparative_architectures.ipynb`** : Notebook principal d'Ã©valuation
  - Description complÃ¨te des mÃ©triques communes d'Ã©valuation en franÃ§ais
  - Structure d'Ã©valuation pour comparer les 3 architectures
  - Code d'implÃ©mentation pour calculer et visualiser les mÃ©triques
  - Recommandations pour la sÃ©lection de l'architecture optimale

## ğŸ¯ Objectifs de l'Ã‰valuation

Ce framework d'Ã©valuation permet de :

1. **Quantifier les performances** de chaque architecture (1, 2, 3)
2. **Comparer objectivement** les diffÃ©rentes approches
3. **Identifier les forces et faiblesses** de chaque solution
4. **Guider la dÃ©cision** sur l'architecture Ã  dÃ©ployer
5. **Ã‰tablir des benchmarks** pour les amÃ©liorations futures

## ğŸ“‹ MÃ©triques d'Ã‰valuation Couvertes

### ğŸ”§ MÃ©triques Techniques
- **Latence** : Temps de rÃ©ponse (ms)
- **Throughput** : RequÃªtes par seconde
- **Utilisation mÃ©moire** : RAM/VRAM
- **CoÃ»t computationnel** : Ressources CPU/GPU

### ğŸ¯ MÃ©triques de QualitÃ©
- **Pertinence** : Score de similaritÃ© sÃ©mantique
- **FactualitÃ©** : Taux d'hallucinations
- **ComplÃ©tude** : Couverture des informations
- **TraÃ§abilitÃ©** : CapacitÃ© Ã  citer sources
- **CohÃ©rence** : Consistance des rÃ©ponses
- **Adaptation linguistique** : ComprÃ©hension des expressions locales

### ğŸ’¼ MÃ©triques MÃ©tier
- **Taux de rÃ©solution** : RÃ©solution au premier contact
- **Satisfaction client** : CSAT, NPS
- **Taux d'escalade** : Transferts vers agents humains
- **Temps de rÃ©solution** : DurÃ©e moyenne
- **Taux de containment** : Gestion autonome
- **ROI** : Retour sur investissement

## ğŸš€ Utilisation

### PrÃ©requis

```bash
pip install numpy pandas matplotlib seaborn
pip install sentence-transformers bert-score
pip install scikit-learn
```

### ExÃ©cution

1. Ouvrir le notebook `04_evaluation_comparative_architectures.ipynb`
2. Lire la documentation des mÃ©triques (sections markdown)
3. Adapter les fonctions de chat pour chaque architecture
4. ExÃ©cuter les cellules d'Ã©valuation
5. Analyser les rÃ©sultats et visualisations

### Exemple d'utilisation

```python
from evaluation import ArchitectureEvaluator

# CrÃ©er l'Ã©valuateur
evaluator = ArchitectureEvaluator("Architecture 1")

# Ã‰valuer sur le dataset de test
metrics = evaluator.evaluate_dataset(test_queries, chat_function)

# Afficher les rÃ©sultats
evaluator.print_summary(metrics)
```

## ğŸ“Š Benchmarks Attendus

| MÃ©trique | Arch 1 | Arch 2 | Arch 3 | Objectif |
|----------|--------|--------|--------|----------|
| Latence moyenne | ~2-3s | ~3-5s | ~4-7s | < 5s |
| Throughput | ~0.3-0.5 req/s | ~0.2-0.3 req/s | ~0.15-0.25 req/s | > 0.2 req/s |
| Taux hallucination | 20-40% | 5-15% | 2-8% | < 10% |
| Taux rÃ©solution | 40-60% | 60-75% | 75-90% | > 70% |
| TraÃ§abilitÃ© | 0% | 80-95% | 90-100% | > 80% |

## ğŸ“ MÃ©thodologie

### Dataset de Test

Le notebook inclut un dataset de test standardisÃ© avec :
- 100-200 requÃªtes couvrant tous les cas d'usage
- Distribution : 40% FAQ, 30% procÃ©dures, 20% complexe, 10% edge cases
- CatÃ©gories : informations gÃ©nÃ©rales, procÃ©dures, troubleshooting, vÃ©rification statut

### Protocole d'Ã‰valuation

1. **ExÃ©cution** : Chaque requÃªte testÃ©e sur les 3 architectures
2. **RÃ©pÃ©tition** : 3-5 runs pour stabilitÃ© des mesures
3. **Environnement** : Conditions identiques (hardware, modÃ¨les, donnÃ©es)
4. **Annotation** : Ã‰valuation humaine pour mÃ©triques qualitatives
5. **Analyse** : Statistiques agrÃ©gÃ©es et visualisations comparatives

## ğŸ“ˆ Recommandations de SÃ©lection

### Architecture 1 : Fine-tuning Simple
**âœ… Choisir si :**
- Budget limitÃ© / POC rapide
- RequÃªtes simples et rÃ©pÃ©titives
- Besoin de latence ultra-faible

**âŒ Ne pas choisir si :**
- Besoin de fiabilitÃ© critique
- Informations Ã©volutives
- TraÃ§abilitÃ© requise

### Architecture 2 : RAG Standard
**âœ… Choisir si :**
- Balance performance/complexitÃ©
- Besoin de traÃ§abilitÃ©
- Base de connaissances Ã©volutive
- RÃ©duction hallucinations

**âŒ Ne pas choisir si :**
- RequÃªtes complexes multi-Ã©tapes
- Besoin d'accÃ¨s donnÃ©es opÃ©rationnelles

### Architecture 3 : RAG-Agentique
**âœ… Choisir si :**
- Service client complet
- RequÃªtes complexes
- AccÃ¨s donnÃ©es opÃ©rationnelles
- Maximisation autonomie
- Budget infrastructure suffisant

**âš ï¸ Attention :**
- NÃ©cessite infrastructure robuste
- Maintenance plus complexe

## ğŸ”— Liens Connexes

- **Documentation principale** : `ARCHITECTURE_README.md`
- **Architecture 1** : `notebooks/architecture_1/01_architecture_1_simple_agent_finetuning.ipynb`
- **Architecture 2** : `notebooks/architecture_2/02_architecture_2_rag_standard.ipynb`
- **Architecture 3** : `notebooks/architecture_3/03_architecture_3_rag_agentique.ipynb`
- **Exemples de donnÃ©es** : `notebooks/data_examples/data_sources_examples.ipynb`

## ğŸ“š Ressources

### MÃ©triques et Frameworks
- BLEU, ROUGE, BERTScore pour l'Ã©valuation NLP
- RAGAS pour l'Ã©valuation RAG
- Sentence-Transformers pour similaritÃ© sÃ©mantique

### Outils
- Weights & Biases : Tracking expÃ©rimentations
- MLflow : Gestion des expÃ©riences
- Hugging Face Evaluate : BibliothÃ¨que de mÃ©triques

## ğŸ“ Support

Pour toute question sur l'Ã©valuation :
- **Documentation** : Voir `ARCHITECTURE_README.md`
- **Support EasyTransfert** : 2522018730 (WhatsApp 24h/24)
- **Issues GitHub** : [github.com/AmedBah/memoire/issues](https://github.com/AmedBah/memoire/issues)

---

*Framework d'Ã©valuation dÃ©veloppÃ© pour le projet de mÃ©moire EasyTransfert - KAYBIC AFRICA*
