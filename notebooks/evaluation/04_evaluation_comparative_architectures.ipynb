{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä √âvaluation Comparative des Architectures Conversationnelles\n",
    "\n",
    "Ce notebook pr√©sente une structure d'√©valuation compl√®te pour les trois architectures exp√©rimentales d√©velopp√©es pour EasyTransfert. Il d√©crit les m√©triques communes d'√©valuation et fournit un cadre pour comparer les performances de chaque architecture.\n",
    "\n",
    "## üéØ Objectifs de l'√âvaluation\n",
    "\n",
    "1. **Quantifier les performances** de chaque architecture\n",
    "2. **Identifier les forces et faiblesses** de chaque approche\n",
    "3. **Guider la s√©lection** de l'architecture optimale selon le contexte\n",
    "4. **√âtablir des benchmarks** pour les am√©liorations futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AmedBah/memoire/blob/main/notebooks/evaluation/04_evaluation_comparative_architectures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab-setup"
   },
   "source": [
    "## üöÄ Configuration pour Google Colab\n",
    "\n",
    "**Note**: Cette section est sp√©cifique √† Google Colab. Si vous ex√©cutez ce notebook localement, vous pouvez ignorer ces cellules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "runtime-check"
   },
   "outputs": [],
   "source": [
    "# V√©rifier le type de runtime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# D√©tecter si on est sur Colab\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"‚úì Ex√©cution sur Google Colab\")\n",
    "    \n",
    "    # V√©rifier le type de GPU\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"‚úì GPU d√©tect√©: {gpu_name}\")\n",
    "        print(f\"‚úì M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        \n",
    "        # Recommandations selon le GPU\n",
    "        if 'T4' in gpu_name:\n",
    "            print(\"\\n‚ö†Ô∏è  GPU T4 d√©tect√© (16 GB): Convient pour ce notebook mais les temps d'entra√Ænement seront plus longs\")\n",
    "        elif 'V100' in gpu_name or 'A100' in gpu_name:\n",
    "            print(f\"\\n‚úì {gpu_name}: Parfait pour ce notebook!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå ATTENTION: Aucun GPU d√©tect√©!\")\n",
    "        print(\"   Pour activer le GPU: Runtime > Change runtime type > GPU\")\n",
    "        print(\"   Pour Colab Pro: Choisir 'High-RAM' ou 'Premium GPU'\")\n",
    "else:\n",
    "    print(\"‚úì Ex√©cution locale\")\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úì GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Aucun GPU d√©tect√© - l'entra√Ænement sera tr√®s lent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Monter Google Drive (uniquement sur Colab)\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # D√©finir le chemin vers les donn√©es\n",
    "    # OPTION 1: Donn√©es dans Google Drive\n",
    "    # DATA_DIR = '/content/drive/MyDrive/memoire/data'\n",
    "    \n",
    "    # OPTION 2: Cloner le repo et utiliser les donn√©es locales\n",
    "    print(\"\\nüì• Clonage du repository...\")\n",
    "    !git clone https://github.com/AmedBah/memoire.git /content/memoire\n",
    "    DATA_DIR = '/content/memoire/data'\n",
    "    \n",
    "    print(f\"\\n‚úì R√©pertoire de donn√©es: {DATA_DIR}\")\n",
    "    \n",
    "    # V√©rifier que les donn√©es sont pr√©sentes\n",
    "    if os.path.exists(DATA_DIR):\n",
    "        print(\"‚úì Donn√©es trouv√©es!\")\n",
    "        !ls -lh {DATA_DIR}\n",
    "    else:\n",
    "        print(f\"‚ùå ERREUR: R√©pertoire {DATA_DIR} non trouv√©!\")\n",
    "        print(\"   Veuillez soit:\")\n",
    "        print(\"   1. Copier le dossier 'data' dans votre Google Drive\")\n",
    "        print(\"   2. Ou le repository sera clon√© automatiquement\")\n",
    "else:\n",
    "    # Ex√©cution locale\n",
    "    DATA_DIR = '../../data'\n",
    "    print(f\"‚úì R√©pertoire de donn√©es local: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-path-helper"
   },
   "outputs": [],
   "source": [
    "# Fonction helper pour obtenir les chemins de donn√©es\n",
    "def get_data_path(relative_path):\n",
    "    \"\"\"Obtenir le chemin absolu d'un fichier de donn√©es\"\"\"\n",
    "    return os.path.join(DATA_DIR, relative_path)\n",
    "\n",
    "# Exemples de chemins\n",
    "print(\"Chemins de donn√©es configur√©s:\")\n",
    "print(f\"  Conversations: {get_data_path('conversations/conversation_1000_finetune.jsonl')}\")\n",
    "print(f\"  FAQs: {get_data_path('faqs/faq_easytransfert.json')}\")\n",
    "print(f\"  Op√©rateurs: {get_data_path('operators/operators_info.json')}\")\n",
    "print(f\"  Proc√©dures: {get_data_path('procedures/procedures_resolution.json')}\")\n",
    "print(f\"  Expressions: {get_data_path('expressions/expressions_ivoiriennes.json')}\")\n",
    "print(f\"  Documents: {get_data_path('documents/doc.txt.txt')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Table des M√©triques d'√âvaluation\n",
    "\n",
    "Les m√©triques sont organis√©es en trois cat√©gories principales :\n",
    "\n",
    "1. **M√©triques Techniques** : Performance syst√®me et ressources\n",
    "2. **M√©triques de Qualit√©** : Qualit√© des r√©ponses et pertinence\n",
    "3. **M√©triques M√©tier** : Impact sur le service client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß M√©triques Techniques\n",
    "\n",
    "Les m√©triques techniques √©valuent les performances syst√®me et l'utilisation des ressources.\n",
    "\n",
    "### 1. Latence (Temps de R√©ponse)\n",
    "\n",
    "**Description :** Temps √©coul√© entre la soumission d'une requ√™te et la r√©ception de la r√©ponse compl√®te.\n",
    "\n",
    "**Unit√© :** Millisecondes (ms) ou Secondes (s)\n",
    "\n",
    "**Calcul :**\n",
    "```python\n",
    "latence = temps_fin_r√©ponse - temps_d√©but_requ√™te\n",
    "```\n",
    "\n",
    "**Importance :** Une faible latence am√©liore l'exp√©rience utilisateur et permet de traiter plus de requ√™tes.\n",
    "\n",
    "**Benchmarks par architecture :**\n",
    "- **Architecture 1** : ~2-3 secondes\n",
    "- **Architecture 2** : ~3-5 secondes (150-200ms r√©cup√©ration + 2-3s g√©n√©ration)\n",
    "- **Architecture 3** : ~4-7 secondes (overhead agentique)\n",
    "\n",
    "**M√©triques d√©riv√©es :**\n",
    "- Latence moyenne (mean)\n",
    "- Latence m√©diane (median)\n",
    "- Latence percentile 95 (p95)\n",
    "- Latence percentile 99 (p99)\n",
    "- √âcart-type (std)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Throughput (D√©bit)\n",
    "\n",
    "**Description :** Nombre de requ√™tes trait√©es par unit√© de temps.\n",
    "\n",
    "**Unit√© :** Requ√™tes par seconde (req/s)\n",
    "\n",
    "**Calcul :**\n",
    "```python\n",
    "throughput = nombre_requ√™tes / temps_total\n",
    "```\n",
    "\n",
    "**Importance :** Indique la capacit√© de l'architecture √† g√©rer la charge.\n",
    "\n",
    "**Benchmarks attendus :**\n",
    "- **Architecture 1** : ~0.3-0.5 req/s (latence faible)\n",
    "- **Architecture 2** : ~0.2-0.3 req/s\n",
    "- **Architecture 3** : ~0.15-0.25 req/s (plus complexe)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Utilisation M√©moire\n",
    "\n",
    "**Description :** Quantit√© de m√©moire RAM et VRAM utilis√©e pendant l'ex√©cution.\n",
    "\n",
    "**Unit√© :** M√©gaoctets (MB) ou Gigaoctets (GB)\n",
    "\n",
    "**Composants mesur√©s :**\n",
    "- **RAM** : Mod√®le, donn√©es, cache\n",
    "- **VRAM** : Mod√®le sur GPU, activations\n",
    "- **Stockage** : Base vectorielle (ChromaDB)\n",
    "\n",
    "**Benchmarks par architecture :**\n",
    "- **Architecture 1** : ~50 MB (adaptateurs LoRA seuls)\n",
    "- **Architecture 2** : ~150-300 MB (base vectorielle + mod√®le)\n",
    "- **Architecture 3** : ~200-400 MB (outils + agent + base)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Co√ªt Computationnel\n",
    "\n",
    "**Description :** Ressources de calcul n√©cessaires (CPU/GPU).\n",
    "\n",
    "**M√©triques :**\n",
    "- **FLOPs** : Op√©rations en virgule flottante\n",
    "- **Utilisation GPU** : Pourcentage d'utilisation\n",
    "- **Co√ªt par requ√™te** : Co√ªt infrastructure par requ√™te\n",
    "\n",
    "**Importance :** D√©termine le co√ªt op√©rationnel et la scalabilit√©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ M√©triques de Qualit√©\n",
    "\n",
    "Les m√©triques de qualit√© √©valuent la pertinence et la fiabilit√© des r√©ponses g√©n√©r√©es.\n",
    "\n",
    "### 1. Pertinence des R√©ponses\n",
    "\n",
    "**Description :** Degr√© auquel la r√©ponse est appropri√©e et r√©pond √† la question pos√©e.\n",
    "\n",
    "**M√©thodes d'√©valuation :**\n",
    "\n",
    "#### a) √âvaluation Humaine\n",
    "- √âchelle de 1 √† 5\n",
    "- Crit√®res : coh√©rence, compl√©tude, utilit√©\n",
    "- N√©cessite annotateurs form√©s\n",
    "\n",
    "#### b) Similarit√© S√©mantique\n",
    "```python\n",
    "# Cosine similarity entre r√©ponse et r√©f√©rence\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "embedding_reponse = model.encode(reponse_generee)\n",
    "embedding_reference = model.encode(reponse_reference)\n",
    "score_pertinence = util.cos_sim(embedding_reponse, embedding_reference)\n",
    "```\n",
    "\n",
    "#### c) BERTScore\n",
    "- Utilise embeddings contextuels\n",
    "- Plus robuste que BLEU/ROUGE\n",
    "- Score de pr√©cision, rappel, F1\n",
    "\n",
    "**Seuils d'acceptabilit√© :**\n",
    "- Excellent : > 0.85\n",
    "- Bon : 0.70 - 0.85\n",
    "- Acceptable : 0.50 - 0.70\n",
    "- Insuffisant : < 0.50\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Factualit√© et Hallucinations\n",
    "\n",
    "**Description :** Mesure la v√©racit√© des informations fournies et d√©tecte les inventions (hallucinations).\n",
    "\n",
    "**Taux d'hallucination :**\n",
    "```python\n",
    "taux_hallucination = (nombre_reponses_avec_erreurs / nombre_total_reponses) * 100\n",
    "```\n",
    "\n",
    "**Types d'hallucinations :**\n",
    "1. **Intrins√®ques** : Information contradictoire avec la source\n",
    "2. **Extrins√®ques** : Information non v√©rifiable dans les sources\n",
    "\n",
    "**M√©thodes de d√©tection :**\n",
    "- V√©rification contre base de connaissances\n",
    "- Cross-r√©f√©rencement avec sources cit√©es\n",
    "- Validation par API de donn√©es op√©rationnelles\n",
    "\n",
    "**Benchmarks attendus :**\n",
    "- **Architecture 1** : Taux √©lev√© (~20-40%) - pas de sources\n",
    "- **Architecture 2** : Taux moyen (~5-15%) - RAG mais pas de validation\n",
    "- **Architecture 3** : Taux faible (~2-8%) - validation par outils\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Compl√©tude (Couverture des Informations)\n",
    "\n",
    "**Description :** Proportion des informations pertinentes incluses dans la r√©ponse.\n",
    "\n",
    "**Calcul :**\n",
    "```python\n",
    "completude = (elements_informatifs_presents / elements_informatifs_attendus) * 100\n",
    "```\n",
    "\n",
    "**√âl√©ments √©valu√©s :**\n",
    "- Toutes les √©tapes d'une proc√©dure mentionn√©es\n",
    "- Informations contextuelles (frais, d√©lais, limites)\n",
    "- Avertissements ou pr√©cautions\n",
    "\n",
    "**Exemple - Transfert MTN vers Orange :**\n",
    "- ‚úÖ Montant min/max\n",
    "- ‚úÖ Frais de transaction\n",
    "- ‚úÖ Format de l'identifiant\n",
    "- ‚úÖ D√©lai de traitement\n",
    "- ‚úÖ √âtapes de la proc√©dure\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Tra√ßabilit√© (Citation des Sources)\n",
    "\n",
    "**Description :** Capacit√© √† fournir et r√©f√©rencer les sources d'information.\n",
    "\n",
    "**M√©triques :**\n",
    "- **Taux de citation** : % de r√©ponses avec sources\n",
    "- **Pr√©cision des sources** : Sources cit√©es sont correctes\n",
    "- **Pertinence des sources** : Sources cit√©es sont pertinentes\n",
    "\n",
    "**Calcul :**\n",
    "```python\n",
    "taux_citation = (reponses_avec_sources / total_reponses) * 100\n",
    "precision_sources = (sources_correctes / sources_citees) * 100\n",
    "```\n",
    "\n",
    "**Benchmarks par architecture :**\n",
    "- **Architecture 1** : 0% (aucune tra√ßabilit√©)\n",
    "- **Architecture 2** : ~80-95% (sources RAG)\n",
    "- **Architecture 3** : ~90-100% (cycle ReAct transparent)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Coh√©rence et Consistance\n",
    "\n",
    "**Description :** Les r√©ponses √† des questions similaires sont coh√©rentes entre elles.\n",
    "\n",
    "**Test :** Poser la m√™me question reformul√©e plusieurs fois\n",
    "\n",
    "**Calcul :**\n",
    "```python\n",
    "# Similarit√© entre N r√©ponses √† la m√™me question\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "similarities = []\n",
    "for r1, r2 in combinations(reponses, 2):\n",
    "    sim = calculate_similarity(r1, r2)\n",
    "    similarities.append(sim)\n",
    "    \n",
    "score_coherence = np.mean(similarities)\n",
    "```\n",
    "\n",
    "**Seuil acceptable :** > 0.75\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Adaptation Linguistique\n",
    "\n",
    "**Description :** Capacit√© √† comprendre et utiliser les expressions locales (ivoiriennes).\n",
    "\n",
    "**Test :** Requ√™tes avec expressions ivoiriennes\n",
    "- \"Mon go n'est pas arriv√©\" (mon argent)\n",
    "- \"D√®h, je veux cotiser\" (transaction de groupe)\n",
    "- \"C'est combien le gb√™?\" (quel est le prix/tarif)\n",
    "\n",
    "**M√©triques :**\n",
    "- Taux de compr√©hension : % requ√™tes comprises\n",
    "- Utilisation appropri√©e dans les r√©ponses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíº M√©triques M√©tier (Business)\n",
    "\n",
    "Les m√©triques m√©tier √©valuent l'impact sur le service client et les objectifs d'entreprise.\n",
    "\n",
    "### 1. Taux de R√©solution au Premier Contact\n",
    "\n",
    "**Description :** Proportion de requ√™tes r√©solues sans n√©cessiter d'intervention humaine ou d'escalade.\n",
    "\n",
    "**Calcul :**\n",
    "```python\n",
    "taux_resolution = (requetes_resolues / total_requetes) * 100\n",
    "```\n",
    "\n",
    "**Crit√®res de r√©solution :**\n",
    "- R√©ponse compl√®te et correcte\n",
    "- Client satisfait (pas de relance)\n",
    "- Pas de transfert vers agent humain\n",
    "\n",
    "**Objectifs :**\n",
    "- **Architecture 1** : ~40-60% (limit√©e aux cas simples)\n",
    "- **Architecture 2** : ~60-75% (meilleure fiabilit√©)\n",
    "- **Architecture 3** : ~75-90% (gestion cas complexes)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Score de Satisfaction Client\n",
    "\n",
    "**Description :** Mesure la satisfaction des utilisateurs avec les r√©ponses re√ßues.\n",
    "\n",
    "**M√©thodes de collecte :**\n",
    "1. **Feedback explicite** : Pouce haut/bas apr√®s chaque r√©ponse\n",
    "2. **CSAT (Customer Satisfaction Score)** : √âchelle 1-5\n",
    "3. **NPS (Net Promoter Score)** : Probabilit√© de recommandation\n",
    "\n",
    "**Calcul CSAT :**\n",
    "```python\n",
    "csat = (notes_4_et_5 / total_reponses) * 100\n",
    "```\n",
    "\n",
    "**Benchmarks :**\n",
    "- Excellent : CSAT > 80%\n",
    "- Bon : CSAT 60-80%\n",
    "- √Ä am√©liorer : CSAT < 60%\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Taux d'Escalade (Transfert vers Agent Humain)\n",
    "\n",
    "**Description :** Proportion de conversations transf√©r√©es √† un agent humain.\n",
    "\n",
    "**Calcul :**\n",
    "```python\n",
    "taux_escalade = (conversations_escaladees / total_conversations) * 100\n",
    "```\n",
    "\n",
    "**Raisons d'escalade :**\n",
    "- Agent ne peut pas r√©soudre\n",
    "- Demande client explicite\n",
    "- Cas complexe ou sensible\n",
    "- Probl√®me technique\n",
    "\n",
    "**Objectifs :**\n",
    "- **Architecture 1** : ~40-60% (limitations importantes)\n",
    "- **Architecture 2** : ~25-40% (meilleure couverture)\n",
    "- **Architecture 3** : ~10-25% (autonomie maximale)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Temps Moyen de R√©solution\n",
    "\n",
    "**Description :** Dur√©e moyenne pour r√©soudre compl√®tement une requ√™te client.\n",
    "\n",
    "**Calcul :**\n",
    "```python\n",
    "temps_moyen = (somme_durees_conversations / nombre_conversations_resolues)\n",
    "```\n",
    "\n",
    "**Composants :**\n",
    "- Temps de latence syst√®me\n",
    "- Nombre d'√©changes n√©cessaires\n",
    "- Temps de lecture client\n",
    "\n",
    "**Objectifs :**\n",
    "- R√©ponse imm√©diate : < 1 minute\n",
    "- R√©solution simple : 2-3 minutes\n",
    "- R√©solution complexe : 5-10 minutes\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Taux de Containment\n",
    "\n",
    "**Description :** Capacit√© de l'assistant √† g√©rer les requ√™tes de bout en bout sans sortir du syst√®me.\n",
    "\n",
    "**Calcul :**\n",
    "```python\n",
    "taux_containment = ((total_conversations - escalades - abandons) / total_conversations) * 100\n",
    "```\n",
    "\n",
    "**Importance :** R√©duction des co√ªts op√©rationnels\n",
    "\n",
    "---\n",
    "\n",
    "### 6. R√©duction de la Charge des Agents\n",
    "\n",
    "**Description :** Pourcentage de requ√™tes g√©r√©es par l'assistant vs agents humains.\n",
    "\n",
    "**Impact :**\n",
    "- **R√©duction co√ªts** : Moins d'agents n√©cessaires\n",
    "- **Disponibilit√© 24/7** : Service continu\n",
    "- **Agents focalis√©s** : Cas complexes uniquement\n",
    "\n",
    "**Calcul ROI :**\n",
    "```python\n",
    "economies_annuelles = (requetes_automatisees * cout_requete_agent) - cout_infrastructure\n",
    "roi = (economies_annuelles / cout_developpement) * 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ M√©thodologie d'√âvaluation Comparative\n",
    "\n",
    "### Dataset de Test\n",
    "\n",
    "Pour une √©valuation juste, utiliser un ensemble de test standardis√© :\n",
    "\n",
    "**Composition recommand√©e :**\n",
    "- **100-200 requ√™tes** couvrant tous les cas d'usage\n",
    "- **Distribution :**\n",
    "  - 40% : Questions simples (FAQ)\n",
    "  - 30% : Requ√™tes proc√©durales (how-to)\n",
    "  - 20% : Cas complexes (multi-√©tapes)\n",
    "  - 10% : Cas edge/difficiles\n",
    "\n",
    "**Cat√©gories de requ√™tes :**\n",
    "1. Informations g√©n√©rales (horaires, limites, frais)\n",
    "2. Proc√©dures de transfert\n",
    "3. R√©solution de probl√®mes\n",
    "4. V√©rification de statut\n",
    "5. Questions sur op√©rateurs sp√©cifiques\n",
    "\n",
    "### Protocole d'√âvaluation\n",
    "\n",
    "**√âtapes :**\n",
    "1. Ex√©cuter chaque requ√™te sur les 3 architectures\n",
    "2. Enregistrer toutes les m√©triques\n",
    "3. Faire √©valuer les r√©ponses par annotateurs\n",
    "4. Calculer les statistiques agr√©g√©es\n",
    "5. Analyser les r√©sultats par cat√©gorie\n",
    "\n",
    "**Conditions de test :**\n",
    "- M√™me environnement hardware\n",
    "- M√™me version des mod√®les\n",
    "- M√™me base de connaissances\n",
    "- Mesures r√©p√©t√©es (3-5 fois) pour stabilit√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Framework de Comparaison\n",
    "\n",
    "### Tableau Comparatif des Architectures\n",
    "\n",
    "| M√©trique | Architecture 1 | Architecture 2 | Architecture 3 | Objectif |\n",
    "|----------|----------------|----------------|----------------|---------|\n",
    "| **TECHNIQUES** | | | | |\n",
    "| Latence moyenne | ~2-3s | ~3-5s | ~4-7s | < 5s |\n",
    "| Throughput | ~0.3-0.5 req/s | ~0.2-0.3 req/s | ~0.15-0.25 req/s | > 0.2 req/s |\n",
    "| M√©moire (RAM) | ~50 MB | ~150-300 MB | ~200-400 MB | < 500 MB |\n",
    "| **QUALIT√â** | | | | |\n",
    "| Pertinence | 0.65-0.75 | 0.75-0.85 | 0.80-0.90 | > 0.80 |\n",
    "| Taux hallucination | 20-40% | 5-15% | 2-8% | < 10% |\n",
    "| Compl√©tude | 60-70% | 75-85% | 85-95% | > 80% |\n",
    "| Tra√ßabilit√© | 0% | 80-95% | 90-100% | > 80% |\n",
    "| **M√âTIER** | | | | |\n",
    "| Taux r√©solution | 40-60% | 60-75% | 75-90% | > 70% |\n",
    "| CSAT | 60-70% | 70-80% | 80-90% | > 75% |\n",
    "| Taux escalade | 40-60% | 25-40% | 10-25% | < 30% |\n",
    "| Temps r√©solution | 2-3 min | 3-5 min | 3-7 min | < 5 min |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Impl√©mentation : Structure de Code pour l'√âvaluation\n",
    "\n",
    "Voici un exemple de structure de code pour impl√©menter l'√©valuation comparative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Installation des D√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des biblioth√®ques n√©cessaires\n",
    "!pip install numpy pandas matplotlib seaborn\n",
    "!pip install sentence-transformers bert-score\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset de Test Standardis√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de test avec requ√™tes repr√©sentatives\n",
    "test_queries = [\n",
    "    # Questions simples (FAQ)\n",
    "    {\"query\": \"Bonjour, c'est quoi EasyTransfert ?\", \"category\": \"general\", \"complexity\": \"simple\"},\n",
    "    {\"query\": \"Quels sont les frais de transaction ?\", \"category\": \"general\", \"complexity\": \"simple\"},\n",
    "    {\"query\": \"Quelle est la limite de transfert MTN ?\", \"category\": \"operator\", \"complexity\": \"simple\"},\n",
    "    {\"query\": \"Les horaires de service client ?\", \"category\": \"general\", \"complexity\": \"simple\"},\n",
    "    \n",
    "    # Requ√™tes proc√©durales\n",
    "    {\"query\": \"Comment faire un transfert de MTN vers Orange ?\", \"category\": \"procedure\", \"complexity\": \"medium\"},\n",
    "    {\"query\": \"Comment r√©initialiser mon mot de passe ?\", \"category\": \"procedure\", \"complexity\": \"medium\"},\n",
    "    {\"query\": \"√âtapes pour cr√©er un compte ?\", \"category\": \"procedure\", \"complexity\": \"medium\"},\n",
    "    \n",
    "    # R√©solution de probl√®mes\n",
    "    {\"query\": \"Mon transfert n'est pas arriv√©, que faire ?\", \"category\": \"troubleshooting\", \"complexity\": \"complex\"},\n",
    "    {\"query\": \"J'ai envoy√© au mauvais num√©ro, puis-je annuler ?\", \"category\": \"troubleshooting\", \"complexity\": \"complex\"},\n",
    "    {\"query\": \"Erreur: num√©ro invalide, comment corriger ?\", \"category\": \"troubleshooting\", \"complexity\": \"complex\"},\n",
    "    \n",
    "    # V√©rification statut (Architecture 3)\n",
    "    {\"query\": \"V√©rifier le statut du transfert EFB.ABC123456\", \"category\": \"status\", \"complexity\": \"medium\"},\n",
    "    {\"query\": \"Mon argent est o√π ? Ref: EFB.XYZ789012\", \"category\": \"status\", \"complexity\": \"medium\"},\n",
    "    \n",
    "    # Expressions ivoiriennes\n",
    "    {\"query\": \"D√®h, mon go n'est pas arriv√©\", \"category\": \"local\", \"complexity\": \"medium\"},\n",
    "    {\"query\": \"C'est combien le gb√™ pour Orange ?\", \"category\": \"local\", \"complexity\": \"simple\"},\n",
    "    \n",
    "    # Cas complexes\n",
    "    {\"query\": \"Diff√©rence entre transfert direct et via EasyTransfert ? Lequel est moins cher ?\", \"category\": \"complex\", \"complexity\": \"complex\"},\n",
    "    {\"query\": \"Puis-je transf√©rer 500000 FCFA de MTN vers Wave un dimanche soir ?\", \"category\": \"complex\", \"complexity\": \"complex\"},\n",
    "]\n",
    "\n",
    "print(f\"Dataset de test : {len(test_queries)} requ√™tes\")\n",
    "print(f\"Distribution par complexit√©:\")\n",
    "for complexity in ['simple', 'medium', 'complex']:\n",
    "    count = len([q for q in test_queries if q['complexity'] == complexity])\n",
    "    print(f\"  - {complexity.capitalize()}: {count} ({count/len(test_queries)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classe d'√âvaluation des M√©triques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import Dict, List\n",
    "\n",
    "class ArchitectureEvaluator:\n",
    "    \"\"\"Classe pour √©valuer les performances d'une architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, architecture_name: str):\n",
    "        self.architecture_name = architecture_name\n",
    "        self.embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "        self.results = []\n",
    "        \n",
    "    def evaluate_query(self, query: str, chat_function, reference_answer: str = None) -> Dict:\n",
    "        \"\"\"√âvalue une requ√™te unique\"\"\"\n",
    "        \n",
    "        # M√©triques techniques\n",
    "        start_time = time.time()\n",
    "        response = chat_function(query)\n",
    "        latency = (time.time() - start_time) * 1000  # en ms\n",
    "        \n",
    "        result = {\n",
    "            'query': query,\n",
    "            'response': response,\n",
    "            'latency_ms': latency,\n",
    "            'response_length': len(response.split()),\n",
    "        }\n",
    "        \n",
    "        # M√©triques de qualit√© (si r√©f√©rence disponible)\n",
    "        if reference_answer:\n",
    "            emb_response = self.embedding_model.encode(response, convert_to_tensor=True)\n",
    "            emb_reference = self.embedding_model.encode(reference_answer, convert_to_tensor=True)\n",
    "            similarity = util.cos_sim(emb_response, emb_reference).item()\n",
    "            result['relevance_score'] = similarity\n",
    "            \n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def evaluate_dataset(self, test_queries: List, chat_function, num_runs: int = 3):\n",
    "        \"\"\"√âvalue l'ensemble du dataset de test\"\"\"\n",
    "        \n",
    "        print(f\"\\nüîç √âvaluation de {self.architecture_name}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for idx, test_case in enumerate(test_queries):\n",
    "            query = test_case['query']\n",
    "            print(f\"\\nRequ√™te {idx+1}/{len(test_queries)}: {query[:60]}...\")\n",
    "            \n",
    "            # Ex√©cuter plusieurs fois pour stabilit√©\n",
    "            latencies = []\n",
    "            for run in range(num_runs):\n",
    "                result = self.evaluate_query(query, chat_function)\n",
    "                latencies.append(result['latency_ms'])\n",
    "            \n",
    "            print(f\"  Latence: {np.mean(latencies):.0f}ms (¬±{np.std(latencies):.0f}ms)\")\n",
    "            \n",
    "        return self.compute_aggregate_metrics()\n",
    "    \n",
    "    def compute_aggregate_metrics(self) -> Dict:\n",
    "        \"\"\"Calcule les m√©triques agr√©g√©es\"\"\"\n",
    "        \n",
    "        latencies = [r['latency_ms'] for r in self.results]\n",
    "        response_lengths = [r['response_length'] for r in self.results]\n",
    "        \n",
    "        metrics = {\n",
    "            'architecture': self.architecture_name,\n",
    "            'num_queries': len(self.results),\n",
    "            \n",
    "            # M√©triques techniques\n",
    "            'latency_mean': np.mean(latencies),\n",
    "            'latency_median': np.median(latencies),\n",
    "            'latency_std': np.std(latencies),\n",
    "            'latency_p95': np.percentile(latencies, 95),\n",
    "            'latency_p99': np.percentile(latencies, 99),\n",
    "            'throughput': 1000 / np.mean(latencies),  # req/s\n",
    "            \n",
    "            # M√©triques de r√©ponse\n",
    "            'response_length_mean': np.mean(response_lengths),\n",
    "            'response_length_std': np.std(response_lengths),\n",
    "        }\n",
    "        \n",
    "        # M√©triques de qualit√© (si disponibles)\n",
    "        relevance_scores = [r.get('relevance_score') for r in self.results if 'relevance_score' in r]\n",
    "        if relevance_scores:\n",
    "            metrics['relevance_mean'] = np.mean(relevance_scores)\n",
    "            metrics['relevance_std'] = np.std(relevance_scores)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def print_summary(self, metrics: Dict):\n",
    "        \"\"\"Affiche un r√©sum√© des m√©triques\"\"\"\n",
    "        \n",
    "        print(f\"\\n\\nüìä R√âSUM√â - {metrics['architecture'].upper()}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nüîß M√âTRIQUES TECHNIQUES:\")\n",
    "        print(f\"  Latence moyenne    : {metrics['latency_mean']:.0f}ms (¬±{metrics['latency_std']:.0f}ms)\")\n",
    "        print(f\"  Latence m√©diane    : {metrics['latency_median']:.0f}ms\")\n",
    "        print(f\"  Latence P95        : {metrics['latency_p95']:.0f}ms\")\n",
    "        print(f\"  Latence P99        : {metrics['latency_p99']:.0f}ms\")\n",
    "        print(f\"  Throughput         : {metrics['throughput']:.2f} req/s\")\n",
    "        \n",
    "        print(f\"\\nüìù M√âTRIQUES DE R√âPONSE:\")\n",
    "        print(f\"  Longueur moyenne   : {metrics['response_length_mean']:.1f} mots (¬±{metrics['response_length_std']:.1f})\")\n",
    "        \n",
    "        if 'relevance_mean' in metrics:\n",
    "            print(f\"\\nüéØ M√âTRIQUES DE QUALIT√â:\")\n",
    "            print(f\"  Pertinence moyenne : {metrics['relevance_mean']:.3f} (¬±{metrics['relevance_std']:.3f})\")\n",
    "        \n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. √âvaluation Comparative des 3 Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation pour comparer les 3 architectures\n",
    "# NOTE: Remplacer par les vraies fonctions de chat de chaque architecture\n",
    "\n",
    "# Simulation des fonctions de chat (√† remplacer)\n",
    "def chat_arch1(query):\n",
    "    \"\"\"Fonction de chat Architecture 1 (√† connecter au vrai mod√®le)\"\"\"\n",
    "    # return model_arch1.chat(query)\n",
    "    time.sleep(2.5)  # Simulation latence\n",
    "    return \"R√©ponse simul√©e de l'Architecture 1\"\n",
    "\n",
    "def chat_arch2(query):\n",
    "    \"\"\"Fonction de chat Architecture 2 (√† connecter au vrai mod√®le)\"\"\"\n",
    "    # return rag_chat(query)\n",
    "    time.sleep(4.0)  # Simulation latence\n",
    "    return \"R√©ponse simul√©e de l'Architecture 2 avec sources\"\n",
    "\n",
    "def chat_arch3(query):\n",
    "    \"\"\"Fonction de chat Architecture 3 (√† connecter au vrai mod√®le)\"\"\"\n",
    "    # return agentic_chat(query)\n",
    "    time.sleep(5.5)  # Simulation latence\n",
    "    return \"R√©ponse simul√©e de l'Architecture 3 avec cycle ReAct\"\n",
    "\n",
    "# Cr√©er les √©valuateurs\n",
    "evaluator1 = ArchitectureEvaluator(\"Architecture 1 - Fine-tuning Simple\")\n",
    "evaluator2 = ArchitectureEvaluator(\"Architecture 2 - RAG Standard\")\n",
    "evaluator3 = ArchitectureEvaluator(\"Architecture 3 - RAG-Agentique\")\n",
    "\n",
    "# √âvaluer chaque architecture (utiliser subset pour test rapide)\n",
    "test_subset = test_queries[:5]  # Pour test rapide, utiliser [:5]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ D√âBUT DE L'√âVALUATION COMPARATIVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "metrics1 = evaluator1.evaluate_dataset(test_subset, chat_arch1, num_runs=1)\n",
    "evaluator1.print_summary(metrics1)\n",
    "\n",
    "metrics2 = evaluator2.evaluate_dataset(test_subset, chat_arch2, num_runs=1)\n",
    "evaluator2.print_summary(metrics2)\n",
    "\n",
    "metrics3 = evaluator3.evaluate_dataset(test_subset, chat_arch3, num_runs=1)\n",
    "evaluator3.print_summary(metrics3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualisation Comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cr√©er DataFrame comparatif\n",
    "comparison_df = pd.DataFrame([\n",
    "    metrics1,\n",
    "    metrics2,\n",
    "    metrics3\n",
    "])\n",
    "\n",
    "# Visualisations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Comparaison des Performances des Architectures', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Latence moyenne\n",
    "axes[0, 0].bar(comparison_df['architecture'], comparison_df['latency_mean'], color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "axes[0, 0].set_ylabel('Latence (ms)')\n",
    "axes[0, 0].set_title('Latence Moyenne')\n",
    "axes[0, 0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# 2. Throughput\n",
    "axes[0, 1].bar(comparison_df['architecture'], comparison_df['throughput'], color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "axes[0, 1].set_ylabel('Requ√™tes/seconde')\n",
    "axes[0, 1].set_title('Throughput (D√©bit)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# 3. Latence P95 et P99\n",
    "x = range(len(comparison_df))\n",
    "width = 0.35\n",
    "axes[1, 0].bar([i - width/2 for i in x], comparison_df['latency_p95'], width, label='P95', color='#3498db')\n",
    "axes[1, 0].bar([i + width/2 for i in x], comparison_df['latency_p99'], width, label='P99', color='#e74c3c')\n",
    "axes[1, 0].set_ylabel('Latence (ms)')\n",
    "axes[1, 0].set_title('Latence Percentiles')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(comparison_df['architecture'], rotation=15)\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Longueur des r√©ponses\n",
    "axes[1, 1].bar(comparison_df['architecture'], comparison_df['response_length_mean'], color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "axes[1, 1].set_ylabel('Nombre de mots')\n",
    "axes[1, 1].set_title('Longueur Moyenne des R√©ponses')\n",
    "axes[1, 1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison_architectures.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Graphiques sauvegard√©s : comparison_architectures.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Export des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter les r√©sultats en CSV\n",
    "comparison_df.to_csv('evaluation_results.csv', index=False)\n",
    "print(\"\\n‚úÖ R√©sultats export√©s : evaluation_results.csv\")\n",
    "\n",
    "# Afficher tableau comparatif\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä TABLEAU COMPARATIF FINAL\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df[['architecture', 'latency_mean', 'latency_median', 'throughput', 'response_length_mean']].to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Recommandations Finales\n",
    "\n",
    "### S√©lection de l'Architecture Selon le Contexte\n",
    "\n",
    "#### Choisir Architecture 1 si :\n",
    "- ‚úÖ Budget limit√© / POC rapide\n",
    "- ‚úÖ Requ√™tes tr√®s simples et r√©p√©titives\n",
    "- ‚úÖ Besoin de latence ultra-faible\n",
    "- ‚úÖ Donn√©es d'entra√Ænement de qualit√© disponibles\n",
    "- ‚ùå Mais : Risque d'hallucinations √©lev√©\n",
    "\n",
    "#### Choisir Architecture 2 si :\n",
    "- ‚úÖ Besoin de fiabilit√© et tra√ßabilit√©\n",
    "- ‚úÖ Base de connaissances √©volutive\n",
    "- ‚úÖ Balance performance/complexit√©\n",
    "- ‚úÖ R√©duction des hallucinations prioritaire\n",
    "- ‚ùå Mais : Limit√© pour requ√™tes multi-√©tapes\n",
    "\n",
    "#### Choisir Architecture 3 si :\n",
    "- ‚úÖ Service client complet et automatis√©\n",
    "- ‚úÖ Requ√™tes complexes n√©cessitant raisonnement\n",
    "- ‚úÖ Besoin d'acc√®s donn√©es op√©rationnelles\n",
    "- ‚úÖ Maximisation de l'autonomie\n",
    "- ‚úÖ Budget infrastructure suffisant\n",
    "- ‚ö†Ô∏è N√©cessite infrastructure robuste et maintenance\n",
    "\n",
    "### M√©triques Prioritaires par Objectif\n",
    "\n",
    "**Si priorit√© = Co√ªt :**\n",
    "- Minimiser : Utilisation m√©moire, co√ªt computationnel\n",
    "- ‚Üí **Architecture 1**\n",
    "\n",
    "**Si priorit√© = Fiabilit√© :**\n",
    "- Maximiser : Factualit√©, tra√ßabilit√©, pertinence\n",
    "- ‚Üí **Architecture 2 ou 3**\n",
    "\n",
    "**Si priorit√© = Autonomie :**\n",
    "- Maximiser : Taux r√©solution, taux containment\n",
    "- Minimiser : Taux escalade\n",
    "- ‚Üí **Architecture 3**\n",
    "\n",
    "**Si priorit√© = Performance :**\n",
    "- Minimiser : Latence\n",
    "- Maximiser : Throughput\n",
    "- ‚Üí **Architecture 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö R√©f√©rences et Ressources\n",
    "\n",
    "### M√©triques NLP\n",
    "- **BLEU** : Papineni et al. (2002) - \"BLEU: a Method for Automatic Evaluation of Machine Translation\"\n",
    "- **ROUGE** : Lin (2004) - \"ROUGE: A Package for Automatic Evaluation of Summaries\"\n",
    "- **BERTScore** : Zhang et al. (2020) - \"BERTScore: Evaluating Text Generation with BERT\"\n",
    "\n",
    "### M√©triques RAG\n",
    "- **RAGAS** : Framework d'√©valuation RAG (Retrieval-Augmented Generation Assessment)\n",
    "- **Faithfulness** : Mesure de fid√©lit√© aux sources\n",
    "- **Answer Relevancy** : Pertinence de la r√©ponse √† la question\n",
    "\n",
    "### Outils d'√âvaluation\n",
    "- **Sentence-Transformers** : Similarit√© s√©mantique\n",
    "- **Hugging Face Evaluate** : Biblioth√®que de m√©triques\n",
    "- **MLflow** : Tracking d'exp√©rimentations\n",
    "- **Weights & Biases** : Monitoring et comparaison\n",
    "\n",
    "### Benchmarks Conversationnels\n",
    "- **DSTC** : Dialog System Technology Challenge\n",
    "- **ConvAI** : Conversational Intelligence Challenge\n",
    "- **MultiWOZ** : Multi-Domain Wizard-of-Oz dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Notes Finales\n",
    "\n",
    "### Bonnes Pratiques d'√âvaluation\n",
    "\n",
    "1. **√âvaluation Continue** : Ne pas √©valuer qu'une fois, mais r√©guli√®rement\n",
    "2. **Donn√©es R√©elles** : Utiliser des vraies requ√™tes clients quand possible\n",
    "3. **√âvaluation Humaine** : Combiner m√©triques automatiques et jugement humain\n",
    "4. **A/B Testing** : Tester en production avec √©chantillons r√©els\n",
    "5. **Monitoring** : Suivre m√©triques en temps r√©el post-d√©ploiement\n",
    "\n",
    "### Limitations de l'√âvaluation Automatique\n",
    "\n",
    "- Les m√©triques automatiques ne capturent pas toute la qualit√©\n",
    "- L'√©valuation humaine reste le gold standard\n",
    "- Le contexte m√©tier est crucial pour interpr√©ter les r√©sultats\n",
    "- Les m√©triques doivent √™tre adapt√©es au cas d'usage sp√©cifique\n",
    "\n",
    "### Prochaines √âtapes\n",
    "\n",
    "1. Adapter ce notebook aux architectures impl√©ment√©es\n",
    "2. Enrichir le dataset de test avec vraies requ√™tes EasyTransfert\n",
    "3. Mettre en place √©valuation humaine avec annotateurs\n",
    "4. Int√©grer tracking avec Weights & Biases ou MLflow\n",
    "5. D√©ployer monitoring en production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìû Contact et Support\n",
    "\n",
    "Pour toute question sur l'√©valuation ou les architectures :\n",
    "\n",
    "- **Projet** : EasyTransfert - KAYBIC AFRICA\n",
    "- **Documentation** : Voir `ARCHITECTURE_README.md`\n",
    "- **Support** : 2522018730 (WhatsApp 24h/24)\n",
    "\n",
    "---\n",
    "\n",
    "*Ce notebook fait partie du projet de m√©moire sur les syst√®mes conversationnels pour EasyTransfert*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}