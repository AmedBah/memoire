{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture 1: Agent Conversationnel Simple (Baseline)\n",
    "\n",
    "Cette architecture impl√©mente un agent conversationnel bas√© sur un LLM (Llama 3.2 3B) sp√©cialement fine-tuned avec LoRA sur les donn√©es conversationnelles d'EasyTransfert.\n",
    "\n",
    "## Caract√©ristiques principales:\n",
    "- **Mod√®le**: Llama 3.2 3B Instruct\n",
    "- **Technique**: LoRA (Low-Rank Adaptation) avec rang=16, alpha=32\n",
    "- **Framework**: Unsloth pour entra√Ænement optimis√©\n",
    "- **Donn√©es**: 3000+ conversations historiques EasyTransfert\n",
    "- **Approche**: Toutes les connaissances encod√©es dans les param√®tres du mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AmedBah/memoire/blob/main/notebooks/architecture_1/01_architecture_1_simple_agent_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab-setup"
   },
   "source": [
    "## üöÄ Configuration pour Google Colab\n",
    "\n",
    "**Note**: Cette section est sp√©cifique √† Google Colab. Si vous ex√©cutez ce notebook localement, vous pouvez ignorer ces cellules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "runtime-check"
   },
   "outputs": [],
   "source": [
    "# V√©rifier le type de runtime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# D√©tecter si on est sur Colab\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"‚úì Ex√©cution sur Google Colab\")\n",
    "    \n",
    "    # V√©rifier le type de GPU\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"‚úì GPU d√©tect√©: {gpu_name}\")\n",
    "        print(f\"‚úì M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        \n",
    "        # Recommandations selon le GPU\n",
    "        if 'T4' in gpu_name:\n",
    "            print(\"\\n‚ö†Ô∏è  GPU T4 d√©tect√© (16 GB): Convient pour ce notebook mais les temps d'entra√Ænement seront plus longs\")\n",
    "        elif 'V100' in gpu_name or 'A100' in gpu_name:\n",
    "            print(f\"\\n‚úì {gpu_name}: Parfait pour ce notebook!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå ATTENTION: Aucun GPU d√©tect√©!\")\n",
    "        print(\"   Pour activer le GPU: Runtime > Change runtime type > GPU\")\n",
    "        print(\"   Pour Colab Pro: Choisir 'High-RAM' ou 'Premium GPU'\")\n",
    "else:\n",
    "    print(\"‚úì Ex√©cution locale\")\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úì GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Aucun GPU d√©tect√© - l'entra√Ænement sera tr√®s lent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Monter Google Drive (uniquement sur Colab)\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # D√©finir le chemin vers les donn√©es\n",
    "    # OPTION 1: Donn√©es dans Google Drive\n",
    "    # DATA_DIR = '/content/drive/MyDrive/memoire/data'\n",
    "    \n",
    "    # OPTION 2: Cloner le repo et utiliser les donn√©es locales\n",
    "    print(\"\\nüì• Clonage du repository...\")\n",
    "    !git clone https://github.com/AmedBah/memoire.git /content/memoire\n",
    "    DATA_DIR = '/content/memoire/data'\n",
    "    \n",
    "    print(f\"\\n‚úì R√©pertoire de donn√©es: {DATA_DIR}\")\n",
    "    \n",
    "    # V√©rifier que les donn√©es sont pr√©sentes\n",
    "    if os.path.exists(DATA_DIR):\n",
    "        print(\"‚úì Donn√©es trouv√©es!\")\n",
    "        !ls -lh {DATA_DIR}\n",
    "    else:\n",
    "        print(f\"‚ùå ERREUR: R√©pertoire {DATA_DIR} non trouv√©!\")\n",
    "        print(\"   Veuillez soit:\")\n",
    "        print(\"   1. Copier le dossier 'data' dans votre Google Drive\")\n",
    "        print(\"   2. Ou le repository sera clon√© automatiquement\")\n",
    "else:\n",
    "    # Ex√©cution locale\n",
    "    DATA_DIR = '../../data'\n",
    "    print(f\"‚úì R√©pertoire de donn√©es local: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-path-helper"
   },
   "outputs": [],
   "source": [
    "# Fonction helper pour obtenir les chemins de donn√©es\n",
    "def get_data_path(relative_path):\n",
    "    \"\"\"Obtenir le chemin absolu d'un fichier de donn√©es\"\"\"\n",
    "    return os.path.join(DATA_DIR, relative_path)\n",
    "\n",
    "# Exemples de chemins\n",
    "print(\"Chemins de donn√©es configur√©s:\")\n",
    "print(f\"  Conversations: {get_data_path('conversations/conversation_1000_finetune.jsonl')}\")\n",
    "print(f\"  FAQs: {get_data_path('faqs/faq_easytransfert.json')}\")\n",
    "print(f\"  Op√©rateurs: {get_data_path('operators/operators_info.json')}\")\n",
    "print(f\"  Proc√©dures: {get_data_path('procedures/procedures_resolution.json')}\")\n",
    "print(f\"  Expressions: {get_data_path('expressions/expressions_ivoiriennes.json')}\")\n",
    "print(f\"  Documents: {get_data_path('documents/doc.txt.txt')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances\n",
    "!pip install -q unsloth transformers datasets accelerate peft bitsandbytes trl wandb\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import wandb\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(f\"GPU disponible: {torch.cuda.is_available()}\")\n",
    "print(f\"Nombre de GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration du Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "max_seq_length = 2048  # Longueur maximale de s√©quence\n",
    "dtype = None  # Auto-d√©tection (Float16 pour Tesla T4, V100, Bfloat16 pour Ampere+)\n",
    "load_in_4bit = True  # Quantification 4-bit pour √©conomiser la m√©moire\n",
    "\n",
    "# Chargement du mod√®le avec Unsloth\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"Mod√®le charg√© avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration LoRA\n",
    "\n",
    "Configuration selon les sp√©cifications de l'architecture:\n",
    "- Rang (r): 16 - Dimension des matrices de faible rang\n",
    "- Alpha (Œ±): 32 - Facteur de mise √† l'√©chelle\n",
    "- Dropout: 0.05 - R√©gularisation\n",
    "- Modules cibles: Query/Key/Value des couches d'attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration LoRA selon l'architecture d√©finie\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # Rang des matrices de faible rang\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=32,  # Facteur de mise √† l'√©chelle\n",
    "    lora_dropout=0.05,  # R√©gularisation\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Optimisation m√©moire\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# Afficher les param√®tres entra√Ænables (~1% du total, ~30M sur 3B)\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Param√®tres entra√Ænables: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pr√©paration des Donn√©es\n",
    "\n",
    "Chargement et formatage des conversations EasyTransfert selon le template Llama 3 Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template syst√®me pour EasyTransfert\n",
    "system_prompt = \"\"\"Tu es un assistant intelligent du service client EasyTransfert, une application de transfert d'argent mobile en C√¥te d'Ivoire.\n",
    "\n",
    "R√®gles importantes:\n",
    "- M√©morisation obligatoire: Ne jamais redemander une information d√©j√† fournie\n",
    "- Formats d'identifiants: \n",
    "  * EasyTransfert: EFB.*\n",
    "  * Orange envoi: MP*\n",
    "  * MTN: chiffres uniquement\n",
    "  * Moov envoi: MRCH*/CF*\n",
    "  * Wave: format variable (souvent T*)\n",
    "- Ton chaleureux: Utilise des √©mojis appropri√©s ü§óüòä\n",
    "- Mention r√©guli√®re d'EasyTransfert\n",
    "- Pas de r√©p√©tition des salutations\n",
    "- Adaptation au niveau d'urgence\n",
    "- Contact service client: 2522018730 (disponible 24h/24 via WhatsApp)\n",
    "\n",
    "Op√©rateurs support√©s: MTN, Orange, Moov, Wave, Tr√©sor Money\"\"\"\n",
    "\n",
    "# Template de conversation Llama 3\n",
    "alpaca_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{}<|eot_id|>\"\"\"\n",
    "\n",
    "# Fonction de formatage des conversations\n",
    "def format_conversation(messages):\n",
    "    \"\"\"Formate une conversation multi-tours en prompt Llama 3\"\"\"\n",
    "    formatted_messages = []\n",
    "    \n",
    "    # Construire la conversation\n",
    "    conversation = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    conversation += system_prompt + \"<|eot_id|>\"\n",
    "    \n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"]\n",
    "        content = msg[\"content\"]\n",
    "        \n",
    "        conversation += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}<|eot_id|>\"\n",
    "    \n",
    "    return {\"text\": conversation}\n",
    "\n",
    "# Charger les donn√©es de conversation\n",
    "dataset = load_dataset(\"json\", data_files=\"get_data_path('conversations/conversation_1000_finetune.jsonl')\", split=\"train\")\n",
    "print(f\"Nombre de conversations: {len(dataset)}\")\n",
    "\n",
    "# Formater les conversations\n",
    "formatted_dataset = dataset.map(\n",
    "    lambda x: format_conversation(x[\"messages\"]),\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"\\nExemple de conversation format√©e:\")\n",
    "print(formatted_dataset[0][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configuration de l'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser Weights & Biases pour le suivi\n",
    "wandb.init(\n",
    "    project=\"easytransfert-architecture1\",\n",
    "    name=\"llama-3.2-3b-lora-finetuning\",\n",
    "    config={\n",
    "        \"model\": \"Llama-3.2-3B-Instruct\",\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"max_seq_length\": max_seq_length,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configuration de l'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs_arch1\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=3407,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "print(\"Configuration d'entra√Ænement d√©finie.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entra√Ænement du Mod√®le\n",
    "\n",
    "Entra√Ænement uniquement sur les r√©ponses de l'assistant (train_on_responses_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "# Cr√©er le trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Entra√Æner uniquement sur les r√©ponses de l'assistant\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")\n",
    "\n",
    "print(\"Trainer configur√©. D√©but de l'entra√Ænement...\")\n",
    "\n",
    "# Lancer l'entra√Ænement\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\nEntra√Ænement termin√©!\")\n",
    "print(f\"Temps d'entra√Ænement: {trainer_stats.metrics['train_runtime']:.2f} secondes\")\n",
    "print(f\"Loss finale: {trainer_stats.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sauvegarde du Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le fine-tun√©\n",
    "model.save_pretrained(\"./models/architecture1_lora\")\n",
    "tokenizer.save_pretrained(\"./models/architecture1_lora\")\n",
    "\n",
    "print(\"Mod√®le sauvegard√© dans ./models/architecture1_lora\")\n",
    "\n",
    "# Sauvegarder aussi en format GGUF pour d√©ploiement (optionnel)\n",
    "# model.save_pretrained_gguf(\"architecture1_model\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inf√©rence et Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activer le mode inf√©rence rapide\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def chat(user_message, conversation_history=[]):\n",
    "    \"\"\"Fonction de chat avec le mod√®le fine-tun√©\"\"\"\n",
    "    \n",
    "    # Construire le prompt avec l'historique\n",
    "    messages = conversation_history + [{\"role\": \"user\", \"content\": user_message}]\n",
    "    \n",
    "    prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "    prompt += system_prompt + \"<|eot_id|>\"\n",
    "    \n",
    "    for msg in messages:\n",
    "        prompt += f\"<|start_header_id|>{msg['role']}<|end_header_id|>\\n\\n{msg['content']}<|eot_id|>\"\n",
    "    \n",
    "    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    \n",
    "    # G√©n√©rer la r√©ponse\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        min_p=0.1,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "# Tests\n",
    "print(\"=\" * 80)\n",
    "print(\"Test 1: Salutation initiale\")\n",
    "print(\"=\" * 80)\n",
    "response1 = chat(\"Bonjour\")\n",
    "print(f\"Assistant: {response1}\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Test 2: Probl√®me de transaction\")\n",
    "print(\"=\" * 80)\n",
    "conversation = [{\"role\": \"user\", \"content\": \"Bonjour\"}, \n",
    "                {\"role\": \"assistant\", \"content\": response1}]\n",
    "response2 = chat(\"Mon argent n'est pas arriv√©\", conversation)\n",
    "print(f\"Assistant: {response2}\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Test 3: Question sur les op√©rateurs\")\n",
    "print(\"=\" * 80)\n",
    "response3 = chat(\"Quels sont les op√©rateurs support√©s par EasyTransfert ?\")\n",
    "print(f\"Assistant: {response3}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. √âvaluation Quantitative\n",
    "\n",
    "M√©triques de performance de l'Architecture 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(test_queries, num_runs=5):\n",
    "    \"\"\"√âvaluation des performances du mod√®le\"\"\"\n",
    "    \n",
    "    latencies = []\n",
    "    response_lengths = []\n",
    "    \n",
    "    for query in test_queries:\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            response = chat(query)\n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            latencies.append(latency)\n",
    "            response_lengths.append(len(response.split()))\n",
    "    \n",
    "    print(\"M√©triques de Performance - Architecture 1\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Latence moyenne: {np.mean(latencies):.2f}s (¬±{np.std(latencies):.2f}s)\")\n",
    "    print(f\"Latence m√©diane: {np.median(latencies):.2f}s\")\n",
    "    print(f\"Latence min/max: {np.min(latencies):.2f}s / {np.max(latencies):.2f}s\")\n",
    "    print(f\"Longueur moyenne r√©ponse: {np.mean(response_lengths):.1f} mots\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        \"latency_mean\": np.mean(latencies),\n",
    "        \"latency_std\": np.std(latencies),\n",
    "        \"latency_median\": np.median(latencies),\n",
    "        \"response_length_mean\": np.mean(response_lengths)\n",
    "    }\n",
    "\n",
    "# Requ√™tes de test\n",
    "test_queries = [\n",
    "    \"Bonjour\",\n",
    "    \"Mon transfert n'est pas arriv√©\",\n",
    "    \"Comment faire un transfert de MTN vers Orange ?\",\n",
    "    \"J'ai oubli√© mon mot de passe\",\n",
    "    \"Quels sont les frais de transaction ?\"\n",
    "]\n",
    "\n",
    "metrics = evaluate_model(test_queries, num_runs=3)\n",
    "\n",
    "# Logger les m√©triques dans WandB\n",
    "wandb.log(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Avantages et Limites de l'Architecture 1\n",
    "\n",
    "### Avantages:\n",
    "- ‚úÖ **Simplicit√©**: Architecture directe sans d√©pendances externes\n",
    "- ‚úÖ **Efficacit√©**: Inf√©rence rapide (~2-3s par r√©ponse)\n",
    "- ‚úÖ **M√©moire**: Adaptateurs LoRA l√©gers (~50 MB)\n",
    "- ‚úÖ **D√©ploiement**: Facile sur GPU grand public\n",
    "- ‚úÖ **Coh√©rence**: Style conversationnel homog√®ne\n",
    "\n",
    "### Limites:\n",
    "- ‚ùå **Hallucinations**: Risque d'inventer des informations\n",
    "- ‚ùå **Actualisation**: N√©cessite r√©entra√Ænement pour nouvelles donn√©es\n",
    "- ‚ùå **Tra√ßabilit√©**: Pas de citation de sources\n",
    "- ‚ùå **Connaissances**: Limit√©es aux donn√©es d'entra√Ænement\n",
    "- ‚ùå **Complexit√©**: Difficult√©s avec requ√™tes multi-√©tapes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}