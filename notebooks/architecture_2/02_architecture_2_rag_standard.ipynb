{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture 2: RAG Standard (Retrieval-Augmented Generation)\n",
    "\n",
    "Cette architecture impl√©mente un syst√®me RAG qui s√©pare la capacit√© de raisonnement (LLM) de la base de connaissances factuelles (ChromaDB).\n",
    "\n",
    "## Caract√©ristiques principales:\n",
    "- **Base vectorielle**: ChromaDB\n",
    "- **Embedding**: paraphrase-multilingual-mpnet-base-v2 (768 dimensions)\n",
    "- **Chunking**: 512 tokens max, 50 tokens overlap\n",
    "- **LLM**: Llama 3.2 3B (peut √™tre utilis√© sans fine-tuning ou avec)\n",
    "- **Sources**: FAQ, proc√©dures, documentation op√©rateurs, conversations historiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AmedBah/memoire/blob/main/notebooks/architecture_2/02_architecture_2_rag_standard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab-setup"
   },
   "source": [
    "## üöÄ Configuration pour Google Colab\n",
    "\n",
    "**Note**: Cette section est sp√©cifique √† Google Colab. Si vous ex√©cutez ce notebook localement, vous pouvez ignorer ces cellules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "runtime-check"
   },
   "outputs": [],
   "source": [
    "# V√©rifier le type de runtime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# D√©tecter si on est sur Colab\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"‚úì Ex√©cution sur Google Colab\")\n",
    "    \n",
    "    # V√©rifier le type de GPU\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"‚úì GPU d√©tect√©: {gpu_name}\")\n",
    "        print(f\"‚úì M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        \n",
    "        # Recommandations selon le GPU\n",
    "        if 'T4' in gpu_name:\n",
    "            print(\"\\n‚ö†Ô∏è  GPU T4 d√©tect√© (16 GB): Convient pour ce notebook mais les temps d'entra√Ænement seront plus longs\")\n",
    "        elif 'V100' in gpu_name or 'A100' in gpu_name:\n",
    "            print(f\"\\n‚úì {gpu_name}: Parfait pour ce notebook!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå ATTENTION: Aucun GPU d√©tect√©!\")\n",
    "        print(\"   Pour activer le GPU: Runtime > Change runtime type > GPU\")\n",
    "        print(\"   Pour Colab Pro: Choisir 'High-RAM' ou 'Premium GPU'\")\n",
    "else:\n",
    "    print(\"‚úì Ex√©cution locale\")\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úì GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Aucun GPU d√©tect√© - l'entra√Ænement sera tr√®s lent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Monter Google Drive (uniquement sur Colab)\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # D√©finir le chemin vers les donn√©es\n",
    "    # OPTION 1: Donn√©es dans Google Drive\n",
    "    # DATA_DIR = '/content/drive/MyDrive/memoire/data'\n",
    "    \n",
    "    # OPTION 2: Cloner le repo et utiliser les donn√©es locales\n",
    "    print(\"\\nüì• Clonage du repository...\")\n",
    "    !git clone https://github.com/AmedBah/memoire.git /content/memoire\n",
    "    DATA_DIR = '/content/memoire/data'\n",
    "    \n",
    "    print(f\"\\n‚úì R√©pertoire de donn√©es: {DATA_DIR}\")\n",
    "    \n",
    "    # V√©rifier que les donn√©es sont pr√©sentes\n",
    "    if os.path.exists(DATA_DIR):\n",
    "        print(\"‚úì Donn√©es trouv√©es!\")\n",
    "        !ls -lh {DATA_DIR}\n",
    "    else:\n",
    "        print(f\"‚ùå ERREUR: R√©pertoire {DATA_DIR} non trouv√©!\")\n",
    "        print(\"   Veuillez soit:\")\n",
    "        print(\"   1. Copier le dossier 'data' dans votre Google Drive\")\n",
    "        print(\"   2. Ou le repository sera clon√© automatiquement\")\n",
    "else:\n",
    "    # Ex√©cution locale\n",
    "    DATA_DIR = '../../data'\n",
    "    print(f\"‚úì R√©pertoire de donn√©es local: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-path-helper"
   },
   "outputs": [],
   "source": [
    "# Fonction helper pour obtenir les chemins de donn√©es\n",
    "def get_data_path(relative_path):\n",
    "    \"\"\"Obtenir le chemin absolu d'un fichier de donn√©es\"\"\"\n",
    "    return os.path.join(DATA_DIR, relative_path)\n",
    "\n",
    "# Exemples de chemins\n",
    "print(\"Chemins de donn√©es configur√©s:\")\n",
    "print(f\"  Conversations: {get_data_path('conversations/conversation_1000_finetune.jsonl')}\")\n",
    "print(f\"  FAQs: {get_data_path('faqs/faq_easytransfert.json')}\")\n",
    "print(f\"  Op√©rateurs: {get_data_path('operators/operators_info.json')}\")\n",
    "print(f\"  Proc√©dures: {get_data_path('procedures/procedures_resolution.json')}\")\n",
    "print(f\"  Expressions: {get_data_path('expressions/expressions_ivoiriennes.json')}\")\n",
    "print(f\"  Documents: {get_data_path('documents/doc.txt.txt')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances\n",
    "!pip install -q chromadb sentence-transformers langchain langchain-community transformers torch\n",
    "\n",
    "import os\n",
    "import json\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import time\n",
    "\n",
    "print(f\"GPU disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration ChromaDB et Mod√®le d'Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chromadb_easytransfert\")\n",
    "\n",
    "# Charger le mod√®le d'embedding multilingue\n",
    "embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "print(f\"Dimension des embeddings: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"Performance estim√©e: ~2000 phrases/seconde sur CPU\")\n",
    "\n",
    "# Cr√©er ou r√©cup√©rer la collection\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"easytransfert_knowledge_base\",\n",
    "    metadata={\"description\": \"Base de connaissances EasyTransfert pour RAG\"}\n",
    ")\n",
    "\n",
    "print(f\"Collection cr√©√©e/r√©cup√©r√©e: {collection.name}\")\n",
    "print(f\"Nombre de documents existants: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pr√©paration des Donn√©es et Chunking\n",
    "\n",
    "Strat√©gie de chunking adaptative:\n",
    "- Taille maximale: 512 tokens (~400 mots fran√ßais)\n",
    "- Chevauchement: 50 tokens (10%)\n",
    "- S√©parateurs hi√©rarchiques: double saut de ligne, point, virgule\n",
    "- M√©tadonn√©es enrichies: cat√©gorie, op√©rateur, mots-cl√©s, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "def create_chunks_with_metadata(documents: List[Dict]) -> Tuple[List[str], List[Dict], List[str]]:\n",
    "    \"\"\"\n",
    "    Cr√©e des chunks avec m√©tadonn√©es enrichies\n",
    "    \n",
    "    Args:\n",
    "        documents: Liste de documents avec contenu et m√©tadonn√©es\n",
    "    \n",
    "    Returns:\n",
    "        Tuple de (chunks, metadatas, ids)\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    all_metadatas = []\n",
    "    all_ids = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        # D√©couper le texte\n",
    "        chunks = text_splitter.split_text(doc[\"content\"])\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            all_chunks.append(chunk)\n",
    "            \n",
    "            # Cr√©er les m√©tadonn√©es\n",
    "            metadata = {\n",
    "                \"category\": doc.get(\"category\", \"unknown\"),\n",
    "                \"operator\": doc.get(\"operator\", \"Tous\"),\n",
    "                \"source\": doc.get(\"source\", \"internal\"),\n",
    "                \"chunk_index\": chunk_idx,\n",
    "                \"total_chunks\": len(chunks),\n",
    "            }\n",
    "            \n",
    "            # Ajouter les mots-cl√©s si disponibles\n",
    "            if \"keywords\" in doc:\n",
    "                metadata[\"keywords\"] = \",\".join(doc[\"keywords\"])\n",
    "            \n",
    "            all_metadatas.append(metadata)\n",
    "            all_ids.append(f\"doc_{doc_idx}_chunk_{chunk_idx}\")\n",
    "    \n",
    "    return all_chunks, all_metadatas, all_ids\n",
    "\n",
    "print(\"Fonction de chunking d√©finie.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cr√©ation de la Base de Connaissances\n",
    "\n",
    "Sources de donn√©es:\n",
    "1. FAQ EasyTransfert\n",
    "2. Proc√©dures op√©rationnelles\n",
    "3. Documentation des op√©rateurs\n",
    "4. Conversations historiques r√©ussies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemples de documents de connaissances\n",
    "knowledge_base_documents = [\n",
    "    # FAQ\n",
    "    {\n",
    "        \"content\": \"\"\"EasyTransfert permet de transf√©rer de l'argent entre diff√©rents op√©rateurs mobiles en C√¥te d'Ivoire. \n",
    "        Les op√©rateurs support√©s sont: MTN Mobile Money, Orange Money, Moov Money, Wave et Tr√©sor Money. \n",
    "        Pour effectuer un transfert, ouvrez l'application, s√©lectionnez l'op√©rateur d'envoi et de r√©ception, \n",
    "        entrez le num√©ro du destinataire et le montant, puis confirmez la transaction.\"\"\",\n",
    "        \"category\": \"faq\",\n",
    "        \"operator\": \"Tous\",\n",
    "        \"source\": \"FAQ officielle\",\n",
    "        \"keywords\": [\"transfert\", \"op√©rateurs\", \"fonctionnement\"]\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Les frais de transaction EasyTransfert varient selon le montant et les op√©rateurs concern√©s.\n",
    "        En g√©n√©ral, les frais sont de 1% √† 2% du montant transf√©r√©, avec un minimum de 25 FCFA et un maximum de 500 FCFA.\n",
    "        Les transferts entre certains op√©rateurs peuvent b√©n√©ficier de tarifs promotionnels.\"\"\",\n",
    "        \"category\": \"faq\",\n",
    "        \"operator\": \"Tous\",\n",
    "        \"source\": \"FAQ officielle\",\n",
    "        \"keywords\": [\"frais\", \"tarifs\", \"co√ªt\"]\n",
    "    },\n",
    "    \n",
    "    # Proc√©dures\n",
    "    {\n",
    "        \"content\": \"\"\"Proc√©dure de r√©solution pour transaction non aboutie:\n",
    "        1. Demander √† l'utilisateur l'identifiant EasyTransfert (commence par EFB.)\n",
    "        2. Demander l'identifiant de l'op√©rateur d'envoi (format varie selon op√©rateur)\n",
    "        3. V√©rifier le statut de la transaction dans le syst√®me\n",
    "        4. Si la transaction est bloqu√©e, initier un remboursement\n",
    "        5. Si le probl√®me persiste, escalader vers le service technique\n",
    "        Contact service client: 2522018730 (WhatsApp 24h/24)\"\"\",\n",
    "        \"category\": \"procedure\",\n",
    "        \"operator\": \"Tous\",\n",
    "        \"source\": \"Manuel de proc√©dures\",\n",
    "        \"keywords\": [\"transaction √©chou√©e\", \"remboursement\", \"r√©solution\"]\n",
    "    },\n",
    "    \n",
    "    # Documentation op√©rateurs\n",
    "    {\n",
    "        \"content\": \"\"\"Format des identifiants de transaction par op√©rateur:\n",
    "        - EasyTransfert: EFB.XXXXXXXXX (commence toujours par EFB.)\n",
    "        - MTN Mobile Money: S√©rie de chiffres uniquement, g√©n√©ralement 10 chiffres\n",
    "        - Orange Money (envoi): MP suivi de 10 chiffres (ex: MP1234567890)\n",
    "        - Moov Money (envoi): MRCH ou CF suivi de caract√®res alphanum√©riques\n",
    "        - Wave: Format variable, souvent commence par T suivi de chiffres\n",
    "        - Tr√©sor Money: Format variable selon la transaction\"\"\",\n",
    "        \"category\": \"operator_info\",\n",
    "        \"operator\": \"Tous\",\n",
    "        \"source\": \"Documentation technique\",\n",
    "        \"keywords\": [\"identifiants\", \"formats\", \"r√©f√©rence transaction\"]\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"\"\"Limites de transaction par op√©rateur:\n",
    "        - MTN: Minimum 100 FCFA, Maximum 2 000 000 FCFA par transaction\n",
    "        - Orange: Minimum 100 FCFA, Maximum 1 500 000 FCFA par transaction\n",
    "        - Moov: Minimum 100 FCFA, Maximum 1 000 000 FCFA par transaction\n",
    "        - Wave: Minimum 100 FCFA, Maximum 5 000 000 FCFA par transaction\n",
    "        - Tr√©sor Money: Minimum 100 FCFA, Maximum 1 000 000 FCFA par transaction\"\"\",\n",
    "        \"category\": \"operator_info\",\n",
    "        \"operator\": \"Tous\",\n",
    "        \"source\": \"Documentation technique\",\n",
    "        \"keywords\": [\"limites\", \"montants\", \"maximum\", \"minimum\"]\n",
    "    },\n",
    "    \n",
    "    # Probl√®mes courants\n",
    "    {\n",
    "        \"content\": \"\"\"Mot de passe oubli√© - Proc√©dure de r√©initialisation:\n",
    "        1. Ouvrir l'application EasyTransfert\n",
    "        2. Cliquer sur \"Mot de passe oubli√©\" sur l'√©cran de connexion\n",
    "        3. Entrer votre num√©ro de t√©l√©phone enregistr√©\n",
    "        4. Vous recevrez un code de v√©rification par SMS\n",
    "        5. Entrer le code re√ßu\n",
    "        6. Cr√©er un nouveau mot de passe (minimum 8 caract√®res)\n",
    "        Si vous ne recevez pas le SMS, contactez le 2522018730\"\"\",\n",
    "        \"category\": \"procedure\",\n",
    "        \"operator\": \"Tous\",\n",
    "        \"source\": \"Guide utilisateur\",\n",
    "        \"keywords\": [\"mot de passe\", \"r√©initialisation\", \"connexion\"]\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Nombre de documents √† indexer: {len(knowledge_base_documents)}\")\n",
    "\n",
    "# Cr√©er les chunks\n",
    "chunks, metadatas, ids = create_chunks_with_metadata(knowledge_base_documents)\n",
    "\n",
    "print(f\"Nombre de chunks cr√©√©s: {len(chunks)}\")\n",
    "print(f\"\\nExemple de chunk:\")\n",
    "print(f\"Contenu: {chunks[0][:200]}...\")\n",
    "print(f\"M√©tadonn√©es: {metadatas[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vectorisation et Indexation dans ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer les embeddings\n",
    "print(\"G√©n√©ration des embeddings...\")\n",
    "start_time = time.time()\n",
    "\n",
    "embeddings = embedding_model.encode(chunks, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "print(f\"Temps de vectorisation: {embedding_time:.2f}s\")\n",
    "print(f\"Performance: {len(chunks) / embedding_time:.1f} chunks/seconde\")\n",
    "\n",
    "# Ajouter √† la collection ChromaDB\n",
    "if collection.count() == 0:  # √âviter les doublons\n",
    "    collection.add(\n",
    "        embeddings=embeddings.tolist(),\n",
    "        documents=chunks,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "    print(f\"\\n{len(chunks)} chunks index√©s dans ChromaDB\")\n",
    "else:\n",
    "    print(f\"\\nCollection existe d√©j√† avec {collection.count()} documents\")\n",
    "\n",
    "print(f\"Base de connaissances pr√™te!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fonction de R√©cup√©ration (Retrieval)\n",
    "\n",
    "Phase de r√©cup√©ration (~150-200ms):\n",
    "- Vectorisation de la question\n",
    "- Recherche de similarit√© cosinus\n",
    "- S√©lection des top-k chunks (d√©faut: 3)\n",
    "- Filtrage par score (> 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query: str, top_k: int = 3, min_score: float = 0.5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    R√©cup√®re les chunks les plus pertinents pour une requ√™te\n",
    "    \n",
    "    Args:\n",
    "        query: Question de l'utilisateur\n",
    "        top_k: Nombre de chunks √† retourner\n",
    "        min_score: Score minimum de pertinence (similarit√© cosinus)\n",
    "    \n",
    "    Returns:\n",
    "        Liste de dictionnaires contenant chunks, scores et m√©tadonn√©es\n",
    "    \"\"\"\n",
    "    # Vectoriser la requ√™te\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    \n",
    "    # Recherche dans ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=top_k,\n",
    "    )\n",
    "    \n",
    "    # Formater les r√©sultats\n",
    "    retrieved_docs = []\n",
    "    \n",
    "    for i in range(len(results['documents'][0])):\n",
    "        # Calculer le score de similarit√© (distance cosinus -> similarit√©)\n",
    "        distance = results['distances'][0][i]\n",
    "        similarity = 1 - distance  # ChromaDB retourne la distance, on veut la similarit√©\n",
    "        \n",
    "        if similarity >= min_score:\n",
    "            retrieved_docs.append({\n",
    "                \"content\": results['documents'][0][i],\n",
    "                \"metadata\": results['metadatas'][0][i],\n",
    "                \"score\": similarity,\n",
    "                \"id\": results['ids'][0][i]\n",
    "            })\n",
    "    \n",
    "    return retrieved_docs\n",
    "\n",
    "# Test de r√©cup√©ration\n",
    "test_query = \"Comment faire un transfert de MTN vers Orange ?\"\n",
    "print(f\"Requ√™te de test: {test_query}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "retrieved = retrieve_context(test_query, top_k=3)\n",
    "retrieval_time = time.time() - start_time\n",
    "\n",
    "print(f\"Temps de r√©cup√©ration: {retrieval_time * 1000:.0f}ms\\n\")\n",
    "print(f\"Nombre de documents r√©cup√©r√©s: {len(retrieved)}\\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved, 1):\n",
    "    print(f\"Document {i} (score: {doc['score']:.3f})\")\n",
    "    print(f\"Cat√©gorie: {doc['metadata']['category']}\")\n",
    "    print(f\"Contenu: {doc['content'][:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chargement du LLM pour la G√©n√©ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Configuration pour quantification 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Charger le mod√®le Llama 3.2 3B\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# Ou utiliser le mod√®le fine-tun√© de l'Architecture 1 si disponible\n",
    "# model_name = \"./models/architecture1_lora\"\n",
    "\n",
    "print(f\"Chargement du mod√®le {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"Mod√®le charg√© avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prompt RAG Augment√©\n",
    "\n",
    "Format structur√© avec:\n",
    "- Section contexte r√©cup√©r√© (documents + m√©tadonn√©es)\n",
    "- Instructions strictes contre hallucinations\n",
    "- Comportements attendus (empathie, citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(query: str, retrieved_docs: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Construit un prompt RAG structur√© avec contexte et instructions\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"Tu es un assistant intelligent du service client EasyTransfert.\n",
    "\n",
    "R√àGLES IMPORTANTES:\n",
    "- R√©ponds UNIQUEMENT en te basant sur le contexte fourni ci-dessous\n",
    "- Ne JAMAIS inventer d'informations\n",
    "- Si l'information n'est pas dans le contexte, dis \"Je n'ai pas cette information, contactez le 2522018730\"\n",
    "- Cite les sources quand c'est pertinent\n",
    "- Utilise un ton chaleureux avec des √©mojis appropri√©s ü§óüòä\n",
    "- Mentionne r√©guli√®rement EasyTransfert\n",
    "\"\"\"\n",
    "    \n",
    "    # Construire le contexte r√©cup√©r√©\n",
    "    context_section = \"CONTEXTE R√âCUP√âR√â:\\n\\n\"\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        context_section += f\"Document {i} (Pertinence: {doc['score']:.2f})\\n\"\n",
    "        context_section += f\"Source: {doc['metadata'].get('source', 'interne')}\\n\"\n",
    "        context_section += f\"Cat√©gorie: {doc['metadata']['category']}\\n\"\n",
    "        context_section += f\"Contenu: {doc['content']}\\n\\n\"\n",
    "    \n",
    "    # Construire le prompt complet au format Llama 3\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{context_section}\n",
    "QUESTION: {query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Test\n",
    "test_prompt = build_rag_prompt(test_query, retrieved)\n",
    "print(\"Exemple de prompt RAG:\")\n",
    "print(\"=\" * 80)\n",
    "print(test_prompt[:1000] + \"...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pipeline RAG Complet\n",
    "\n",
    "Int√©gration R√©cup√©ration + G√©n√©ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat(query: str, top_k: int = 3, min_score: float = 0.5) -> Dict:\n",
    "    \"\"\"\n",
    "    Pipeline RAG complet: R√©cup√©ration + G√©n√©ration\n",
    "    \n",
    "    Returns:\n",
    "        Dict avec response, sources, et m√©triques de performance\n",
    "    \"\"\"\n",
    "    # Phase 1: R√©cup√©ration\n",
    "    retrieval_start = time.time()\n",
    "    retrieved_docs = retrieve_context(query, top_k, min_score)\n",
    "    retrieval_time = time.time() - retrieval_start\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        return {\n",
    "            \"response\": \"Je n'ai pas trouv√© d'information pertinente dans ma base de connaissances. Contactez le service client EasyTransfert au 2522018730 pour une assistance personnalis√©e ü§ó\",\n",
    "            \"sources\": [],\n",
    "            \"retrieval_time_ms\": retrieval_time * 1000,\n",
    "            \"generation_time_ms\": 0,\n",
    "            \"total_time_ms\": retrieval_time * 1000\n",
    "        }\n",
    "    \n",
    "    # Construire le prompt\n",
    "    prompt = build_rag_prompt(query, retrieved_docs)\n",
    "    \n",
    "    # Phase 2: G√©n√©ration\n",
    "    generation_start = time.time()\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(llm_model.device)\n",
    "    \n",
    "    outputs = llm_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    generation_time = time.time() - generation_start\n",
    "    \n",
    "    # Extraire les sources\n",
    "    sources = [\n",
    "        {\n",
    "            \"category\": doc[\"metadata\"][\"category\"],\n",
    "            \"source\": doc[\"metadata\"].get(\"source\", \"interne\"),\n",
    "            \"score\": doc[\"score\"]\n",
    "        }\n",
    "        for doc in retrieved_docs\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"response\": response.strip(),\n",
    "        \"sources\": sources,\n",
    "        \"retrieval_time_ms\": retrieval_time * 1000,\n",
    "        \"generation_time_ms\": generation_time * 1000,\n",
    "        \"total_time_ms\": (retrieval_time + generation_time) * 1000,\n",
    "        \"num_retrieved_docs\": len(retrieved_docs)\n",
    "    }\n",
    "\n",
    "print(\"Pipeline RAG d√©fini.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Tests et √âvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "test_queries = [\n",
    "    \"Comment faire un transfert entre op√©rateurs ?\",\n",
    "    \"Quels sont les frais de transaction ?\",\n",
    "    \"Mon transfert n'est pas arriv√©, que faire ?\",\n",
    "    \"J'ai oubli√© mon mot de passe\",\n",
    "    \"Quel est le format de l'identifiant Orange ?\",\n",
    "    \"Quelle est la limite de transfert pour Wave ?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTS DU SYST√àME RAG\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\nTest {i}: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    result = rag_chat(query)\n",
    "    \n",
    "    print(f\"R√©ponse: {result['response']}\")\n",
    "    print(f\"\\nPerformance:\")\n",
    "    print(f\"  - R√©cup√©ration: {result['retrieval_time_ms']:.0f}ms\")\n",
    "    print(f\"  - G√©n√©ration: {result['generation_time_ms']:.0f}ms\")\n",
    "    print(f\"  - Total: {result['total_time_ms']:.0f}ms\")\n",
    "    print(f\"  - Documents r√©cup√©r√©s: {result['num_retrieved_docs']}\")\n",
    "    print(f\"\\nSources utilis√©es:\")\n",
    "    for src in result['sources']:\n",
    "        print(f\"  - {src['category']} (score: {src['score']:.2f})\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. M√©triques de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Collecter les m√©triques\n",
    "metrics = {\n",
    "    \"retrieval_times\": [],\n",
    "    \"generation_times\": [],\n",
    "    \"total_times\": [],\n",
    "    \"num_docs_retrieved\": []\n",
    "}\n",
    "\n",
    "for query in test_queries:\n",
    "    result = rag_chat(query)\n",
    "    metrics[\"retrieval_times\"].append(result[\"retrieval_time_ms\"])\n",
    "    metrics[\"generation_times\"].append(result[\"generation_time_ms\"])\n",
    "    metrics[\"total_times\"].append(result[\"total_time_ms\"])\n",
    "    metrics[\"num_docs_retrieved\"].append(result[\"num_retrieved_docs\"])\n",
    "\n",
    "print(\"\\nM√âTRIQUES DE PERFORMANCE - ARCHITECTURE 2 (RAG)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"R√©cup√©ration:\")\n",
    "print(f\"  - Moyenne: {np.mean(metrics['retrieval_times']):.0f}ms\")\n",
    "print(f\"  - Min/Max: {np.min(metrics['retrieval_times']):.0f}ms / {np.max(metrics['retrieval_times']):.0f}ms\")\n",
    "print(f\"\\nG√©n√©ration:\")\n",
    "print(f\"  - Moyenne: {np.mean(metrics['generation_times']):.0f}ms\")\n",
    "print(f\"  - Min/Max: {np.min(metrics['generation_times']):.0f}ms / {np.max(metrics['generation_times']):.0f}ms\")\n",
    "print(f\"\\nTemps total:\")\n",
    "print(f\"  - Moyenne: {np.mean(metrics['total_times']):.0f}ms\")\n",
    "print(f\"  - √âcart-type: {np.std(metrics['total_times']):.0f}ms\")\n",
    "print(f\"\\nDocuments r√©cup√©r√©s:\")\n",
    "print(f\"  - Moyenne: {np.mean(metrics['num_docs_retrieved']):.1f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Avantages et Limites de l'Architecture 2\n",
    "\n",
    "### Avantages:\n",
    "- ‚úÖ **V√©racit√©**: R√©ponses ancr√©es dans sources v√©rifiables\n",
    "- ‚úÖ **Tra√ßabilit√©**: Citations des sources utilis√©es\n",
    "- ‚úÖ **Actualisation**: Ajout de documents sans r√©entra√Ænement\n",
    "- ‚úÖ **R√©duction hallucinations**: Contexte factuel fourni au LLM\n",
    "- ‚úÖ **Transparence**: Scores de pertinence et m√©tadonn√©es\n",
    "\n",
    "### Limites:\n",
    "- ‚ùå **Latence**: ~2-3s (r√©cup√©ration + g√©n√©ration)\n",
    "- ‚ùå **Complexit√©**: Pipeline multi-composants\n",
    "- ‚ùå **D√©pendances**: ChromaDB + LLM + embedding model\n",
    "- ‚ùå **Requ√™tes complexes**: Pas de raisonnement multi-√©tapes\n",
    "- ‚ùå **Donn√©es temps r√©el**: Pas d'acc√®s aux bases transactionnelles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}