INTRODUCTION 

Dans un contexte de digitalisation acc√©l√©r√©e des services financiers en Afrique de l'Ouest, les fintechs sp√©cialis√©es dans l'interop√©rabilit√© des transferts d'argent mobile sont confront√©es √† une croissance exponentielle du volume de donn√©es et d'interactions clients. Cette dynamique engendre une accumulation massive d'informations h√©t√©rog√®nes: conversations WhatsApp, emails, captures d'√©cran de transactions, re√ßus d'op√©rateurs et journaux applicatifs. Ces donn√©es, produites quotidiennement par les utilisateurs et les syst√®mes interconnect√©s, sont essentielles pour assurer un suivi pr√©cis des transactions, r√©soudre les incidents et maintenir la qualit√© de service. Toutefois, leur exploitation efficace demeure une t√¢che fastidieuse et chronophage, particuli√®rement pour les √©quipes de support client qui doivent r√©agir promptement face aux probl√®mes. L'identification des identifiants de transaction, la compr√©hension des flux entre op√©rateurs et l'extraction d'informations pertinentes se r√©v√®lent souvent complexes, notamment dans un contexte multilingue franco-africain.
Soucieuse d'am√©liorer l'efficacit√© de son service client, KAYBIC AFRICA, entreprise ivoirienne innovante dans le domaine des services financiers digitaux, a choisi d'explorer les possibilit√©s offertes par l'intelligence artificielle g√©n√©rative pour automatiser et renforcer la qualit√© de son support utilisateur. √Ä travers sa solution phare EasyTransfert, pionnier dans les transferts inter-r√©seaux de mobile money (Wave, Orange, MTN, Moov), KAYBIC AFRICA fait face aux d√©fis d'un support client encore largement manuel: d√©lais de r√©ponse variables, surcharge des agents et h√©t√©rog√©n√©it√© des r√©ponses. C'est dans cette optique que s'inscrit notre projet intitul√© : ¬´ Mise en place d'un syst√®me conversationnel intelligent fond√© sur l'IA g√©n√©rative en vue de l'automatisation int√©grale du service client chez EasyTransfert ¬ª.
Ce travail vise √† concevoir un assistant conversationnel bas√© sur un grand mod√®le de langage (LLM), dont les r√©ponses sont enrichies et contextualis√©es gr√¢ce √† une base de connaissances vectorielle (ChromaDB), structur√©e √† partir des FAQ, proc√©dures, tickets historiques et r√®gles m√©tier d'EasyTransfert. L'objectif est de fournir un support intelligent, rapide et contextuel aux utilisateurs, disponible 24h/24 sur les canaux privil√©gi√©s (WhatsApp Business). Pour y parvenir, nous tenterons de r√©pondre aux interrogations suivantes : Comment structurer et indexer efficacement les connaissances h√©t√©rog√®nes d'EasyTransfert pour optimiser la pertinence des r√©ponses ? Quelle architecture combinant RAG (Retrieval-Augmented Generation) et approche agentique permet de garantir des r√©ponses pr√©cises, conformes aux r√®gles m√©tier et tra√ßables ? Comment int√©grer l'agent conversationnel aux processus existants et en mesurer l'impact op√©rationnel sans perturber l'organisation ?
La r√©ponse √† ces interrogations orientera la structuration de ce m√©moire en trois grandes parties : la premi√®re partie pose le cadre th√©orique et analyse l'existant chez EasyTransfert (donn√©es, canaux, processus, typologies de requ√™tes), la deuxi√®me partie d√©taille la conception technique (mod√®le de langage, RAG avec ChromaDB, architecture agentique, int√©gration multicanale, analyse des sentiments), et la troisi√®me partie pr√©sente les r√©sultats exp√©rimentaux, l'√©valuation des performances (m√©triques NLP et m√©tier) et les perspectives d'√©volution vers un service client enti√®rement automatis√©.















PREMI√àRE PARTIE : G√âN√âRALIT√âS

















CHAPITRE I : ENVIRONNEMENT DE TRAVAIL
I-	Pr√©sentation de la structure d‚Äôaccueil 
1.	Pr√©sentation g√©n√©rale de KayBic Africa
KAYBIC AFRICA est une Startup ivoirienne innovante fond√© en 2020 et sp√©cialis√©e dans l'agr√©gation de services de paiement mobile en Afrique de l'Ouest. Elle offre une plateforme technologique avanc√©e permettant aux entreprises et aux particuliers d'effectuer des transactions financi√®res s√©curis√©es et interop√©rables au sein de la zone CEDEAO. L'entreprise se distingue par sa capacit√© √† connecter tous les principaux op√©rateurs de mobile money de la r√©gion via une API unifi√©e, simplifiant ainsi les processus de paiement pour ses partenaires et utilisateurs.
	Parmi ses produits phares, EasyTransfert se distingue comme un service de transfert d‚Äôargent rapide et fiable, offrant √† la fois aux particuliers et aux entreprises une solution simple pour effectuer des transactions financi√®res multi-op√©rateurs.
2.	Mission et Vision
	La mission de KAYBIC AFRICA est de faciliter l‚Äôinclusion financi√®re en Afrique de l‚ÄôOuest, en fournissant des solutions de paiement mobile accessibles, s√©curis√©es et interop√©rables. Sa vision √† long terme est d‚Äôinterconnecter tous les syst√®mes de paiement existants afin de fluidifier les √©changes financiers et promouvoir une √©conomie num√©rique inclusive, o√π les services financiers sont accessibles √† tous.
	Dans ce cadre, EasyTransfert joue un r√¥le central, en incarnant l‚Äôengagement de KAYBIC AFRICA √† simplifier les transactions et √† am√©liorer l‚Äôexp√©rience utilisateur, tout en ouvrant la voie √† des innovations comme l‚Äôautomatisation du service client via des syst√®mes intelligents. 
3.	Produits et services
KAYBIC AFRICA propose une gamme compl√®te de services adapt√©s aux besoins des particuliers, des entreprises et des professionnels :
‚Ä¢	EasyTransfert : application mobile permettant des transferts d‚Äôargent instantan√©s entre diff√©rents op√©rateurs de mobile money tels que ORANGE Money, MTN Mobile Money, MOOV Money, WAVE et Tr√©sor Money. Cette solution permet de faciliter les transactions inter-op√©rateurs pour tous les utilisateurs. Elle constitue le pilier du service client digital de KAYBIC, sur lequel s‚Äôappuie la mise en place d‚Äôun syst√®me conversationnel intelligent pour automatiser le support client.
‚Ä¢	Service Agr√©gateur : API unifi√©e permettant aux entreprises d‚Äôint√©grer facilement les services de paiement mobile de diff√©rents op√©rateurs, facilitant ainsi les transactions multi-op√©rateurs et la gestion centralis√©e des flux financiers.
‚Ä¢	Bulk Payment : solution de paiement de masse d√©di√©e aux entreprises pour effectuer des paiements group√©s, tels que le versement de salaires ou de primes, avec une fiabilit√© et une rapidit√© accrues.
‚Ä¢	QR Pay : solution de paiement par QR Code compatible avec tous les op√©rateurs r√©gionaux, offrant une m√©thode de paiement rapide, s√©curis√©e et pratique pour les commer√ßants et leurs clients.
‚Ä¢	D√©veloppement d‚Äôapplications personnalis√©es : cr√©ation de solutions sur mesure pour r√©pondre aux besoins sp√©cifiques des entreprises en mati√®re de paiement mobile, incluant la gestion automatis√©e des interactions clients et l‚Äôint√©gration de technologies innovantes.
II-	Pr√©sentation du projet  
1.	Contexte du projet
EasyTransfert est un produit phare de Kaybic Africa, une startup ivoirienne innovante sp√©cialis√©e dans l‚Äôagr√©gation de services de paiement mobile en Afrique de l‚ÄôOuest. En tant que solution fintech pionni√®re, cette solution offre une alternative pratique, rapide, s√©curis√©e pour les transferts d'argent entre les diff√©rents mobile money existant en C√¥te d'Ivoire.
Le service client de EasyTransfert repose actuellement sur une gestion manuelle, ce qui entra√Æne des d√©lais de r√©ponse importants, une surcharge des agents et une incoh√©rence dans la qualit√© du support.
Le projet vise √† mettre en place une architecture combinant la r√©cup√©ration d'informations pertinentes √† partir de sources de donn√©es (logs, bases de donn√©es, documentation etc‚Ä¶) et la g√©n√©ration de r√©ponses contextuelles adapt√©es, afin d'am√©liorer l'efficacit√© du service, de r√©duire la charge des agents et d‚Äôoffrir une assistance plus rapide et coh√©rente aux utilisateurs.
2.	Probl√©matiques
Afin de r√©pondre efficacement aux d√©fis actuels rencontr√©s par EasyTransfert en mati√®re de gestion de son service client, il est essentiel de s‚Äôinterroger sur les approches technologiques et organisationnelles les plus adapt√©es pour atteindre une automatisation intelligente et durable. Ce projet soul√®ve ainsi plusieurs questionnements cl√©s :
Comment concevoir un assistant conversationnel intelligent capable d‚Äôautomatiser efficacement le service client de EasyTransfert tout en garantissant la coh√©rence et la pertinence des r√©ponses fournies ?
Comment centraliser et exploiter les diff√©rentes sources d‚Äôinformations internes (FAQ, bases de donn√©es, historiques de tickets) afin d‚Äôalimenter le mod√®le d‚ÄôIA et d‚Äôassurer une compr√©hension fine des demandes des utilisateurs ?
Comment int√©grer une technologie fond√©e sur l‚ÄôIA g√©n√©rative et le traitement automatique du langage naturel (NLP) dans l‚Äô√©cosyst√®me existant de EasyTransfert, tout en maintenant la qualit√© du support et la satisfaction client ?
Enfin, comment assurer une adoption fluide de cette solution par les √©quipes internes et les utilisateurs finaux afin d‚Äôen maximiser l‚Äôimpact op√©rationnel et strat√©gique ?

CHAPITRE II : √âTUDE DU PROJET 
Ce chapitre a pour objectif de pr√©senter une analyse approfondie de l‚Äôenvironnement op√©rationnel de EasyTransfert, en mettant particuli√®rement l‚Äôaccent sur le fonctionnement actuel de son service client et les limites rencontr√©es dans la gestion des requ√™tes utilisateurs. Il s‚Äôagira, dans un premier temps, d‚Äô√©tudier l‚Äôexistant afin de comprendre les processus en place et les outils actuellement utilis√©s. Ensuite, le cahier des charges d√©finira les besoins fonctionnels et techniques n√©cessaires √† la conception du syst√®me conversationnel intelligent. Enfin, seront expos√©s les objectifs et les hypoth√®ses de recherche qui guideront la mise en ≈ìuvre et l‚Äô√©valuation de la solution propos√©e.

I-	√âtude de l‚Äôexistant
1.	Analyse du Processus Actuel
	La gestion du service client chez EasyTransfert repose actuellement sur un ensemble de pratiques internes majoritairement manuelles et centralis√©es sur les agents du support. Ces derniers assurent la r√©ception, le traitement et le suivi des requ√™tes √©mises par les utilisateurs √† travers plusieurs canaux de communication : WhatsApp Business, Facebook Messenger, Manychat et par Email.
Lorsqu‚Äôun client rencontre une difficult√© li√©e √† une transaction (erreur d‚Äôenvoi, probl√®me de r√©ception, retard, ou question sur les frais ou les op√©rateurs pris en charge), il contacte directement le service client via l‚Äôun de ces canaux. Les agents de support collectent alors les informations n√©cessaires (num√©ro de t√©l√©phone, montant, op√©rateur concern√©, r√©f√©rence de transaction, etc.) et effectuent des v√©rifications manuelles dans les outils internes ou via la plateforme d‚Äôadministration de EasyTransfert.
Les principales sources d‚Äôinformations mobilis√©es dans ce processus sont :
‚Ä¢	Les messages clients √©chang√©s sur les plateformes sociales (WhatsApp, Facebook, Manychat) ;
‚Ä¢	Les bases de donn√©es transactionnelles, consult√©es pour v√©rifier l‚Äô√©tat d‚Äôune op√©ration;
‚Ä¢	Les rapports journaliers et historiques de transactions, utilis√©s pour identifier les r√©currences ou anomalies ;
‚Ä¢	L‚Äôexp√©rience des agents de support, qui repose largement sur leur connaissance empirique du syst√®me et des cas rencontr√©s.
En mati√®re d‚Äôoutils, la gestion s‚Äôeffectue essentiellement via les interfaces natives des plateformes de messagerie (WhatsApp Web, Messenger Business Suite, etc.), parfois appuy√©es par des outils bureautiques internes (fichiers Excel, captures d‚Äô√©cran ou suivis manuels).
2.	Analyse de Solutions Alternatives
L'analyse du processus actuel de gestion du service client met en √©vidence plusieurs limites :
‚Ä¢	D√©pendance √©lev√©e aux agents humains : Les agents sont sollicit√©s pour chaque demande, ce qui entra√Æne des d√©lais de r√©ponse variables et une capacit√© limit√©e √† traiter un grand volume de requ√™tes simultan√©ment.
‚Ä¢	Incoh√©rence et variabilit√© des r√©ponses : L'absence d'un syst√®me centralis√© et l'utilisation de connaissances empiriques peuvent conduire √† des r√©ponses incoh√©rentes ou inexactes.
‚Ä¢	Tra√ßabilit√© limit√©e : Le suivi des interactions et la g√©n√©ration de rapports sont rendus difficiles en raison de la dispersion des informations sur diff√©rentes plateformes.
‚Ä¢	Surcharge progressive des agents : L'augmentation du nombre de clients et de demandes entra√Æne une pression accrue sur les agents, affectant la qualit√© du service.
Ces d√©fis sont √©galement observ√©s dans le secteur des fintechs en Afrique de l'Ouest. Une √©tude men√©e par MSC et Africa Fintech Forum en 2021 a r√©v√©l√© que pr√®s de la moiti√© du financement des fintechs en Afrique provenait de la Communaut√© √âconomique des √âtats de l'Afrique de l'Ouest (CEDEAO), mettant en lumi√®re la croissance rapide du secteur. Cependant, cette expansion s'accompagne de d√©fis, notamment en mati√®re de gestion du service client. 
Pour r√©pondre √† ces d√©fis, il est imp√©ratif de d√©velopper un syst√®me automatis√© et intelligent capable de :
‚Ä¢	Centraliser et exploiter efficacement les donn√©es provenant de tous les canaux de communication.
‚Ä¢	Fournir des r√©ponses coh√©rentes, rapides et personnalis√©es, r√©duisant ainsi la charge de travail des agents.
‚Ä¢	Assurer une tra√ßabilit√© compl√®te des interactions, facilitant le suivi et l'analyse des performances.
‚Ä¢	S'int√©grer harmonieusement aux canaux existants et permettre un suivi statistique des interactions pour am√©liorer la qualit√© du service.
Ces besoins sont essentiels pour maintenir la comp√©titivit√© de EasyTransfert dans un environnement fintech en pleine expansion et pour garantir une exp√©rience client optimale.




II-	Cahier des charges
1.	Besoins fonctionnels
Les besoins fonctionnels d√©crivent les fonctionnalit√©s principales attendues du syst√®me afin de r√©pondre efficacement aux objectifs fix√©s.

Fonctionnalit√©	Description
Base de connaissances interne (FAQ structur√©e)	Collecte et structuration d‚Äôun corpus limit√© (FAQ, tickets r√©currents) pour alimenter le mod√®le.
Recherche et g√©n√©ration de r√©ponses automatiques	Int√©gration d‚Äôun petit mod√®le LLM ou API (ex. GPT, Mistral, etc.) pour g√©n√©rer des r√©ponses √† partir de la FAQ.
Connexion √† un canal unique (WhatsApp)	Int√©gration via API (WhatsApp Business) pour automatiser les √©changes.
Historique des conversations	Enregistrement des interactions (texte + m√©tadonn√©es) pour analyse et suivi.
Interface d‚Äôadministration simple (console ou dashboard basique)	Suivi du nombre de requ√™tes trait√©es, des r√©ponses automatiques, et des transferts vers un agent.

2.	Besoins non fonctionnels
Cat√©gorie	Exigence
Performance	Le syst√®me doit √™tre capable de traiter simultan√©ment au moins 100 requ√™tes clients sans d√©gradation des performances.
Disponibilit√©	Le chatbot doit √™tre disponible 24h/24 et 7j/7, avec un taux de disponibilit√© sup√©rieur √† 99 %.
S√©curit√© et confidentialit√©	Les donn√©es clients doivent √™tre trait√©es conform√©ment aux normes RGPD et stock√©es de mani√®re s√©curis√©e (chiffrement des √©changes et des bases de donn√©es).
Scalabilit√©	L‚Äôarchitecture doit pouvoir √™tre √©tendue pour prendre en charge de nouveaux canaux (Telegram, site web, etc.) sans refonte majeure.
Interop√©rabilit√©	Le syst√®me doit pouvoir interagir avec les API internes (transaction, support, CRM) et externes (Meta, Twilio).
Maintenabilit√©	Le code et la documentation doivent √™tre clairs, modulaires et facilement maintenables par l‚Äô√©quipe technique.
Ergonomie	L‚Äôinterface d‚Äôadministration doit √™tre intuitive, accessible et adapt√©e √† diff√©rents niveaux d‚Äôutilisateurs (agents, superviseurs, administrateurs).
Mesurabilit√©	Les performances du mod√®le de langage et du moteur RAG doivent √™tre √©valu√©es selon des m√©triques telles que Recall@k, F1-score, MRR, BLEU, etc.

3.	Objectifs
ÔÉò	Objectif g√©n√©ral:
	L‚Äôobjectif principal est de concevoir, impl√©menter et √©valuer un assistant conversationnel intelligent bas√© sur le traitement automatique du langage naturel et l‚ÄôIA g√©n√©rative, pour automatiser le service client de EasyTransfert.
	Le projet couvrira la conception technique, l‚Äôint√©gration de mod√®le de langage dans une architecture adapt√©e, le traitement et la structuration des donn√©es, l‚Äôint√©gration multicanale, ainsi que l‚Äôanalyse des sentiments pour am√©liorer la qualit√© des r√©ponses et la priorisation des requ√™tes clients.

ÔÉò	Objectifs sp√©cifiques :
‚Ä¢	Collecter et structurer les donn√©es internes (FAQ, tickets, historiques)
‚Ä¢	Indexer ces donn√©es dans une base vectorielle (FAISS ou ChromaDB) et √©valuer l'efficacit√© des recherches s√©mantiques √† l‚Äôaide de m√©triques comme le Mean Reciprocal Rank (MRR), le Recall@k, le Precision@k etc...
‚Ä¢	Exploiter des techniques avanc√©es de NLP pour comprendre et g√©n√©rer des r√©ponses pr√©cises et contextuelles
‚Ä¢	D√©ployer un mod√®le de langage dans une architecture adapt√©e, afin d‚Äôam√©liorer la qualit√© et la pertinence des interactions conversationnelles
‚Ä¢	Int√©grer le syst√®me aux canaux de communication existants (WhatsApp Business, email)
‚Ä¢	Int√©grer un module d‚Äôanalyse des sentiments pour adapter les r√©ponses et prioriser les requ√™tes
‚Ä¢	Mettre en place un tableau de bord pour le suivi des performances et de la satisfaction client
‚Ä¢	√âvaluer les performances globales du syst√®me (mod√®le de langage et moteur NLP) √† l‚Äôaide de m√©triques standard et sp√©cifiques √† chaque m√©thode : pr√©cision, rappel, F1-score, taux d‚Äôautomatisation, score de satisfaction, etc.

4.	Planning


Chapitre III : √âTAT DE L‚ÄôART
I-	D√©finitions et concepts cl√©s 
1.	L‚ÄôIntelligence Artificielle (IA) 
L'Intelligence Artificielle (IA) est une discipline scientifique visant √† doter les syst√®mes informatiques de facult√©s cognitives analogues √† celles de l'√™tre humain, telles que l'apprentissage, la compr√©hension du langage, la r√©solution de probl√®mes ou la prise de d√©cision. Reposant sur des m√©canismes d'am√©lioration it√©rative par l'analyse de donn√©es, son champ d'application s'√©tend √† des secteurs strat√©giques comme la sant√©, la robotique, le commerce ou encore l'a√©ronautique. 
2.	L'apprentissage machine (Machine Learning) 
L'apprentissage automatique, ou Machine Learning, est un sous-domaine fondamental de l'intelligence artificielle qui dote les syst√®mes informatiques de la capacit√© d'apprendre sans √™tre explicitement programm√©s pour chaque t√¢che. Son principe repose sur l'utilisation d'algorithmes qui analysent des ensembles de donn√©es pour identifier des sch√©mas, faire des pr√©dictions et am√©liorer leurs propres performances de mani√®re it√©rative √† mesure qu'ils sont expos√©s √† de nouvelles informations.
3.	Les neurones biologiques 
L'architecture fondamentale du cerveau biologique repose sur un vaste r√©seau de cellules sp√©cialis√©es, les neurones. Chaque neurone est structur√© pour recevoir des signaux entrants, de nature √©lectrique et chimique, via ses nombreuses dendrites. √Ä la suite d‚Äôun traitement interne, il propage un signal de sortie le long de son axone. La communication interneuronale s'effectue au niveau de jonctions sp√©cifiques, les synapses, o√π l'axone d'un neurone transmet l'information aux dendrites d'autres neurones. Ce cycle de transmission et de r√©ception se r√©p√®te √† une √©chelle massive, constituant la base du traitement de l'information c√©r√©brale.
4.	R√©seau Neuronal Artificiel (Artificial Neural Network)
Inspir√©s de la neurobiologie, les r√©seaux de neurones artificiels mod√©lisent le flux de l'information √† travers un r√©seau de n≈ìuds interconnect√©s. Le concept, popularis√© d√®s 1943 par McCulloch et Pitts , repose sur une unit√© de calcul de base : le neurone artificiel. Celuici traite l'information selon une s√©quence pr√©cise, comme illustr√© √† la figure 4. 
Le processus d√©bute par la r√©ception de multiples signaux d'entr√©e (X‚ÇÅ, ..., X‚Çô). Chacun de ces signaux est pond√©r√© par un poids de connexion (w‚±º) qui simule la force synaptique. Ces entr√©es pond√©r√©es sont ensuite agr√©g√©es par une sommation pour former l'entr√©e net :  
ùëõùëíùë° =‚àëùëõùëó=1ùë§ùëóùë•ùëó 
Cette valeur agr√©g√©e est ensuite pass√©e √† travers une fonction de transfert non-lin√©aire f, qui d√©termine la sortie finale O du neurone : 
ùëÇ = ùëì(ùëõùëíùë°) 
Ce m√©canisme de traitement distribu√© et l'ajustement des poids de connexion constituent le fondement de l'apprentissage et du fonctionnement des syst√®mes connexionnistes.
5.	L‚ÄôApprentissage Profond (Deep Learning) 
L‚Äôapprentissage profond ou le Deep Learning en anglais est un sous-domaine de l‚Äôapprentissage machine (machine Learning) qui se distingue par l‚Äôutilisation de r√©seaux de neurones dits ¬´ profonds ¬ª. Ces architectures sont compos√©es de nombreuses couches de calcul interconnect√©es (souvent des dizaines, voire des centaines), contrairement aux r√©seaux de neurones ¬´ peu profonds ¬ª qui n‚Äôen comptent qu‚Äôune ou deux. La v√©ritable force du Deep Learning r√©side dans la fonction de cette profondeur : elle permet au mod√®le d‚Äôapprendre de mani√®re autonome une hi√©rarchie de caract√©ristiques (ou de repr√©sentations) de plus en plus abstraites, directement √† partir des donn√©es brutes. Cette capacit√© constitue sa diff√©rence fondamentale avec les approches de Machine Learning dites ¬´ traditionnelles ¬ª (telles que les SVM ou les For√™ts Al√©atoires), qui requi√®rent une expertise humaine pour extraire et formater manuellement les caract√©ristiques pertinentes. Le Deep Learning automatise donc cette √©tape cruciale, ce qui lui permet de traiter des donn√©es extr√™mement complexes comme les images, le son ou le langage naturel.
Gr√¢ce √† cette facult√© d'apprentissage de repr√©sentations, le Deep Learning est devenu le moteur de la plupart des applications d'intelligence artificielle avanc√©es. Il alimente aujourd'hui une vaste gamme de produits et de services du quotidien, incluant les assistants num√©riques, les t√©l√©commandes √† contr√¥le vocal, la reconnaissance faciale, les syst√®mes de d√©tection de fraude, le d√©veloppement de v√©hicules autonomes et, plus r√©cemment, l'essor de l'IA g√©n√©rative. 
6.	Le Traitement du Langage Naturel  
Le Traitement du Langage Naturel (TLN), √©galement appel√© traitement automatique des langues, est un domaine de l'intelligence artificielle qui se concentre sur l'interaction entre les ordinateurs et les langues humaines. Il permet aux machines de comprendre, interpr√©ter et g√©n√©rer du langage humain, ce qui est utilis√© dans diverses applications comme les chatbots, l'analyse des sentiments, la traduction automatique et la reconnaissance de la parole. Le TLN utilise le machine Learning pour r√©v√©ler la structure et la signification du texte, permettant aux organisations d'analyser du texte et d'extraire des informations sur des personnes, des lieux et des √©v√©nements. Les entreprises utilisent des outils de TLN pour traiter automatiquement des donn√©es textuelles et vocales, analyser l'intention ou le sentiment contenu dans les messages et r√©pondre en temps r√©el aux communications humaines. Le TLN combine la linguistique informatique, le Machine Learning et des mod√®les de Deep Learning pour traiter le langage humain.
7.	Les Grands Mod√®les de Langage (LLM) 
	Les LLM reposent g√©n√©ralement sur des architectures de r√©seaux de neurones profonds connues sous le nom de ‚ÄúTransformers‚Äù, qui ont √©t√© introduites par Google en 2017. Les Transformers ont apport√© une v√©ritable r√©volution dans le traitement du langage naturel en am√©liorant la capacit√© √† comprendre le contexte et √† g√©rer des phrases longues de mani√®re plus efficace. Ces architectures ont permis des avanc√©es significatives en mati√®re de compr√©hension et de g√©n√©ration de texte, ouvrant ainsi la voie √† des mod√®les plus puissants et plus pr√©cis dans le domaine de l‚ÄôIA.
	L‚Äôarchitecture typique d‚Äôun LLM comprend une couche d‚Äôentr√©e, des couches cach√©es et une couche de sortie. La couche d‚Äôentr√©e re√ßoit les donn√©es textuelles en tant que s√©quence de mots ou de caract√®res. Chaque mot ou caract√®re est repr√©sent√© sous forme de vecteur num√©rique, appel√© ‚Äúembedding‚Äù, qui capture les informations s√©mantiques et syntaxiques. Les couches cach√©es sont responsables du traitement et de l‚Äôapprentissage des informations. Elles utilisent des m√©canismes tels que les r√©seaux de neurones r√©currents (RNN) ou les transformers pour capturer les relations et les d√©pendances entre les mots dans le texte. Les RNN sont particuli√®rement adapt√©s pour prendre en compte le contexte s√©quentiel, tandis que les transformers se concentrent sur les relations globales entre les mots.
	Enfin, la couche de sortie g√©n√®re les pr√©dictions ou les r√©ponses en fonction des informations trait√©es. Elle peut √™tre con√ßue pour effectuer diverses t√¢ches, telles que la g√©n√©ration de texte, la classification, la traduction, etc.
 
Figure 1: Architecture type d‚Äôun LLM
8.	L‚ÄôIntelligence Artificielle Generative
L'Intelligence Artificielle (IA) G√©n√©rative est une branche de l'IA qui se concentre sur la cr√©ation de nouveaux contenus originaux, plut√¥t que sur la simple analyse ou classification de donn√©es existantes. Alors que l'IA traditionnelle identifie des sch√©mas, l'IA g√©n√©rative utilise ces sch√©mas appris pour produire du contenu in√©dit, qu'il s'agisse de texte, d'images, de musique, de code ou de donn√©es synth√©tiques. Elle s'appuie sur des mod√®les complexes, notamment les Grands Mod√®les de Langage (LLM) pour le texte ou les mod√®les de diffusion pour les images. Ces syst√®mes apprennent les structures et les caract√©ristiques des donn√©es d'entra√Ænement pour ensuite g√©n√©rer des ≈ìuvres nouvelles qui en respectent le style et la coh√©rence. 
9.	Les chatbots 
Un chatbot est un programme informatique con√ßu pour simuler une conversation avec des utilisateurs humains, en particulier sur Internet. Il s'agit donc d'un robot conversationnel, capable d'interagir en langage naturel et en temps r√©el, r√©pondre aux questions, proposer des solutions et services adapt√©s en fonction des requ√™tes.



II-	Technologies et M√©thodes pour les Agents conversationnels Intelligents 
 
1.	Syst√®mes Bas√©s sur des R√®gles et la reconnaissance de Mots-Cl√©s 
Historiquement, les premiers agents conversationnels (ou chatbots) reposaient sur des ensembles de r√®gles pr√©d√©finies et la reconnaissance de mots-cl√©s. Les d√©veloppeurs d√©finissaient manuellement des paires de questions-r√©ponses ou des arbres de d√©cision pour guider la conversation. Si l'utilisateur employait des mots-cl√©s sp√©cifiques, le syst√®me d√©clenchait une r√©ponse pr√©programm√©e. 
Comme tout syst√®me, il existe des avantages et des inconvenants. 
Avantages : Simplicit√© de conception pour des sc√©narios limit√©s ; pr√©visibilit√© des r√©ponses, contr√¥le total sur le dialogue. 
Inconv√©nients : Manque de flexibilit√© face √† des formulations vari√©es, incapacit√© √† g√©rer des requ√™tes complexes ou impr√©vues ; maintenance lourde et co√ªteuse √† mesure que la base de connaissances s'√©tend ; faible capacit√© de compr√©hension du contexte r√©el de la question. Pour un domaine technique comme une passerelle mon√©tique ; la diversit√© des probl√®mes et des formulations rend cette approche rapidement obsol√®te. 
2.	Apprentissage Automatique (Machine Learning) pour la Compr√©hension du Langage Naturel 
L'av√®nement du Machine Learning, et plus particuli√®rement du Traitement du Langage Naturel (TLN), a permis des avanc√©es significatives. Des techniques comme la classification d'intentions et l'extraction d'entit√©s nomm√©es ont permis aux agents de mieux comprendre la s√©mantique des requ√™tes utilisateurs. Des algorithmes comme les SVM (Support Vecteur Machine), Naive Bayes (Na√Øve Bayessienne), ou les premiers r√©seaux de neurones √©taient entra√Æn√©s sur des corpus de donn√©es √©tiquet√©es pour identifier l'intention derri√®re une question (exemple : "demande de statut d'une transaction", "probl√®me de connexion API") et extraire les informations cl√©s (ex : ID de transaction, nom de l'API) 
‚Ä¢	Avantages : Meilleure compr√©hension des variations linguistiques par rapport aux syst√®mes √† r√®gles, capacit√© √† g√©n√©raliser √† partir des donn√©es d'entra√Ænement. 
‚Ä¢	Inconv√©nients : N√©cessit√© de corpus d'entra√Ænement √©tiquet√©s cons√©quents et de qualit√©, difficult√© √† maintenir une conversation coh√©rente sur plusieurs tours, g√©n√©ration de r√©ponses souvent limit√©e √† des mod√®les pr√©d√©finis ou √† la s√©lection de r√©ponses pr√©-√©crites. La g√©n√©ration de r√©ponses v√©ritablement dynamiques et contextuelles reste un d√©fi. 



3.	Grands Mod√®les de Langage (LLM) Pr√©-entra√Æn√©s (Finetuning)
L'√©mergence des Grands Mod√®les de Langage (LLM) tels que GPT (Generative Pre-trained Transformer), LLaMA ou Mistral a marqu√© une r√©volution. Entra√Æn√©s sur d'immenses quantit√©s de texte, ces mod√®les d√©montrent des capacit√©s impressionnantes de compr√©hension et de g√©n√©ration de langage naturel, leur permettant de tenir des conversations fluides, de r√©sumer des textes, de r√©pondre √† des questions de connaissance g√©n√©rale, et m√™me de g√©n√©rer du code. Comme Avantage et inconv√©nient : 
‚Ä¢	Avantages : Capacit√©s de compr√©hension et de g√©n√©ration de langage sans pr√©c√©dent, flexibilit√© pour s'adapter √† divers styles de conversation, r√©duction du besoin de donn√©es d'entra√Ænement √©tiquet√©es massives pour des t√¢ches g√©n√©rales. 
‚Ä¢	Inconv√©nients :Tendance √† g√©n√©rer des informations plausibles mais incorrectes ou invent√©es, ce qui est critique dans un contexte d'assistance technique o√π la pr√©cision est primordiale, Les LLMs sont entra√Æn√©s sur des donn√©es jusqu'√† une certaine date et ne poss√®dent pas nativement les connaissances sp√©cifiques √† un domaine m√©tier pr√©cis (comme les informations de transaction √† r√©cup√©rer) ni les informations les plus r√©centes, Il peut √™tre difficile de savoir d'o√π provient une information g√©n√©r√©e par le LLM, rendant la v√©rification complexe, L'utilisation des LLMs les plus performants via des API peut engendrer des co√ªts, et l'h√©bergement de mod√®les open-source puissants n√©cessite des ressources mat√©rielles importantes.  
4.	G√©n√©ration Augment√©e par R√©cup√©ration (RAG -Retrieval Augmented Generation) 
Pour pallier les limitations des LLMs purs, notamment leur manque de connaissances sp√©cifiques et leur tendance aux hallucinations, l'approche RAG a gagn√© en popularit√© Le principe est de coupler un LLM avec une base de connaissances externe. Lorsqu'une question est pos√©e, un module de "retriever" recherche d'abord les informations les plus pertinentes dans la base de connaissances (souvent vectoris√©e). Ces informations sont ensuite fournies au LLM comme contexte pour qu'il g√©n√®re une r√©ponse ancr√©e et factuelle. 
‚Ä¢	Avantages : les r√©ponses sont bas√©es sur des documents sources sp√©cifiques ; permet d'int√©grer facilement la documentation propre √† Easytransfert et de la maintenir √† jour sans r√©entra√Æner le LLM ; possibilit√© de citer les sources utilis√©es pour g√©n√©rer la r√©ponse, augmentant la confiance et la v√©rifiabilit√© ; moins d√©pendant du r√©entra√Ænement co√ªteux du LLM pour int√©grer de nouvelles informations. 
‚Ä¢	Inconv√©nients : qualit√© du retriever, La performance globale d√©pend fortement de la capacit√© du retriever √† trouver les informations les plus pertinentes, n√©cessite la mise en place d‚Äôun pipeline d'indexation, de vectorisation et de recherche. 
 
Figure 2: Fonctionnement du RAG
5.	Agents LLM (Approches Agentiques) 
Au-del√† de la simple g√©n√©ration de texte, les recherches r√©centes explorent l'utilisation des LLM comme "cerveau" d'agents capables de raisonner, de planifier des actions, et d'utiliser des outils pour accomplir des t√¢ches complexes. Un agent LLM peut d√©composer un probl√®me, d√©cider d'utiliser un outil sp√©cifique (comme une calculatrice, une API externe, ou m√™me le module RAG lui-m√™me), observer le r√©sultat, et it√©rer jusqu'√† la r√©solution. 
‚Ä¢	Avantages : capacit√© √† effectuer des t√¢ches complexes multi-√©tapes ; peut interroger des bases de donn√©es, ex√©cuter des scripts, ou appeler des API pour obtenir des informations en temps r√©el ou effectuer des actions ; adaptabilit√© accrue √† des situations impr√©vues.
‚Ä¢	Inconv√©nients : complexit√© de conception et de d√©bogage ; risques d‚Äôins√©curit√© accrus si l'agent a acc√®s √† des outils puissants ; peut n√©cessiter des LLMs plus puissants (et donc plus co√ªteux) pour un raisonnement efficace. 

III-	Choix de l‚Äôapproche 
Compte tenu du contexte du service client de EasyTransfert o√π la rapidit√©, la fiabilit√© et la pr√©cision des r√©ponses sont essentielles afin de garantir la satisfaction client, le choix de l‚Äôapproche technologique doit r√©pondre √† plusieurs imp√©ratifs :
‚Ä¢	√©viter les erreurs critiques li√©es aux hallucinations des mod√®les de langage,
‚Ä¢	int√©grer la documentation interne, les Faqs et l‚Äôhistorique des interactions,
‚Ä¢	s‚Äôadapter √† une grande vari√©t√© de requ√™tes (techniques, transactionnelles, ou li√©es aux canaux de communication),
‚Ä¢	offrir un syst√®me √©volutif et maintenable.
Apr√®s l‚Äô√©tude des diff√©rentes approches existantes, trois architectures ont √©t√© retenues pour une √©tude comparative approfondie. Ce choix m√©thodologique permet d'√©valuer progressivement l'apport de chaque composante technologique et de justifier empiriquement la solution finale adopt√©e.
1.	Architecture 1 : Agent Simple 
Cette premi√®re architecture repose sur un agent conversationnel bas√© sur un LLM (GPT-3.5 Turbo, Mistral-7B ou LLaMA-2) sp√©cialement fine-tuned sur des donn√©es sp√©cifiques au domaine de EasyTransfert. Le mod√®le est entra√Æn√© sur un corpus d‚Äôhistorique de conversations et de FAQ pour acqu√©rir une connaissance sp√©cialis√©e du service de transfert d'argent.
Cette architecture constitue notre r√©f√©rence de base sp√©cialis√©e pour l'√©tude comparative. Contrairement √† un LLM g√©n√©rique, l'agent simple avec fine-tuning permet d'√©valuer l'efficacit√© d'une approche d'adaptation par l'entra√Ænement sur des donn√©es m√©tier sp√©cifiques. 

2.	Architecture 2 : RAG (Retrieval-Augmented Generation)
L'architecture RAG r√©pond directement √† la probl√©matique centrale du projet : comment centraliser et exploiter les diff√©rentes sources d'informations internes (FAQ, bases de donn√©es, historiques de conversations) pour garantir la pertinence et la coh√©rence des r√©ponses ? 
Dans le contexte de EasyTransfert, cette approche permet d'ancrer les r√©ponses dans la documentation officielle et les proc√©dures valid√©es, de r√©duire consid√©rablement les hallucinations en fournissant au LLM un contexte factuel v√©rifi√©, d'assurer la tra√ßabilit√© des sources d'information utilis√©es pour chaque r√©ponse, et de maintenir le syst√®me √† jour en enrichissant simplement la base vectorielle, sans n√©cessiter de r√©entra√Ænement du mod√®le. Cette architecture am√©liore la v√©racit√©, la sp√©cificit√© et la fiabilit√© des r√©ponses, tout en conservant la fluidit√© conversationnelle des LLM. Elle constitue le standard actuel pour les syst√®mes de question-r√©ponse en domaine sp√©cialis√©.

3.	Architecture 3 : RAG-Agentique avec outils dynamiques

Cette troisi√®me architecture repr√©sente une √©volution avanc√©e du RAG standard en int√©grant des capacit√©s agentiques (agentic capabilities). Le syst√®me ne se contente plus de r√©cup√©rer et g√©n√©rer, mais devient capable de raisonner sur les actions √† entreprendre (utilisation du protocole ReAct - Reasoning + Acting, Yao et al., 2023), d'utiliser des outils dynamiques pour acc√©der √† des informations en temps r√©el (base de donn√©es transactionnelle, API de v√©rification de statut), et d'orchestrer plusieurs √©tapes de traitement de mani√®re autonome (analyse de sentiment, routage vers agent humain si n√©cessaire). Les composants additionnels incluent un module d'analyse des sentiments pour la d√©tection de la frustration, urgence ou satisfaction client, un acc√®s √† la base de donn√©es transactionnelle pour la consultation de l'√©tat r√©el d'une transaction (r√©f√©rence, montant, statut), un syst√®me de routage intelligent pour l'escalade automatique vers un agent humain en cas de requ√™te complexe ou √©motionnellement charg√©e, et un m√©canisme de raisonnement pour la planification multi-√©tapes n√©cessaire √† r√©soudre des requ√™tes complexes.
Cette architecture r√©pond √† l'objectif d'automatisation intelligente et durable du service client d'EasyTransfert. Elle permet de traiter des requ√™tes complexes n√©cessitant l'acc√®s √† des donn√©es op√©rationnelles en temps r√©el (¬´ O√π en est mon transfert de 50 000 FCFA vers MTN ? ¬ª), d'adapter les r√©ponses en fonction du contexte √©motionnel d√©tect√© (client m√©content ‚Üí ton empathique, escalade rapide), de prioriser les requ√™tes selon leur urgence et leur complexit√©, et d'am√©liorer continuellement le syst√®me en enregistrant les interactions et les d√©cisions prises. Alors que l'Architecture 2 excelle dans la r√©ponse √† des questions factuelles bas√©es sur la documentation, l'Architecture 3 ajoute une dimension op√©rationnelle et contextuelle indispensable pour un service client complet. Elle permet de passer d'un syst√®me de FAQ augment√©e √† un v√©ritable assistant intelligent et autonome.






































DEUXI√àME PARTIE √âTUDE: TH√âORIQUE ET TECHNIQUE DU SYST√àME















CHAPITRE IV : COLLECTE ET PR√âPARATION DES DONN√âES  
	Ce chapitre pr√©sente l‚Äôensemble des m√©thodes et traitements appliqu√©s √† la collecte, au nettoyage et √† la structuration des donn√©es utilis√©es pour le d√©veloppement et l‚Äô√©valuation des architectures d‚Äôagents conversationnels d‚ÄôEasyTransfert.
L‚Äôobjectif de cette phase est de constituer un corpus textuel exploitable par les mod√®les de langage et d‚Äôassurer la qualit√©, la coh√©rence et la pertinence des donn√©es avant leur vectorisation et leur indexation dans le cadre de l‚Äôapproche RAG (Retrieval-Augmented Generation).
I-	Sources et nature des donn√©es 
	Afin de permettre √† l‚Äôagent conversationnel d‚ÄôEasyTransfert de r√©pondre de mani√®re fiable et contextuelle aux demandes des utilisateurs, plusieurs sources internes et externes ont √©t√© identifi√©es et exploit√©es. Ces sources refl√®tent la diversit√© des interactions et des informations manipul√©es par le service client.
1.	Corpus conversationnels issus du service client
Ces donn√©es proviennent principalement des √©changes entre les clients et les agents d‚Äôassistance sur les canaux WhatsApp Business.
Elles repr√©sentent le c≈ìur du corpus d‚Äôentra√Ænement, car elles capturent les formulations r√©elles des utilisateurs, leurs probl√©matiques fr√©quentes et les r√©ponses apport√©es par les agents.
Les messages sont export√©s sous format JSON, comprenant les m√©tadonn√©es suivantes :
‚Ä¢	Identifiant de la conversation et du message,
‚Ä¢	R√¥le de l‚Äô√©metteur (agent ou client),
‚Ä¢	Timestamp,
‚Ä¢	Contenu textuel du message,
‚Ä¢	Cat√©gorie de la requ√™te (transaction, erreur, information g√©n√©rale, etc.).
Ce jeu de donn√©es constitue une base riche mais non structur√©e, n√©cessitant un pr√©traitement approfondi.


 
Figure : Structure des Conversations Collect√©es
2.	Documentation technique et FAQ internes
Une seconde source essentielle provient de la documentation interne d‚ÄôEasyTransfert, comprenant :
‚Ä¢	la FAQ (Foire aux questions) officielle,
‚Ä¢	les proc√©dures de r√©solution d‚Äôincidents,
‚Ä¢	et les notes techniques utilis√©es par le support.
Ces documents constituent une base de connaissances factuelle qui alimente la composante RAG. Leur contenu textuel est exploit√© pour fournir des extraits pr√©cis lors des recherches s√©mantiques, garantissant que les r√©ponses g√©n√©r√©es par le mod√®le sont bas√©es sur des sources v√©rifi√©es et √† jour.
3.	Documentation linguistique locale : expressions et abr√©viations ivoiriennes
Dans le but d‚Äôadapter l‚Äôagent conversationnel au registre linguistique r√©el des utilisateurs ivoiriens, une base documentaire d‚Äôexpressions et d‚Äôabr√©viations locales a √©t√© int√©gr√©e.
Cette ressource, construite √† partir d‚Äôun corpus linguistique collect√© sur les r√©seaux sociaux, forums et conversations clients, contient des expressions idiomatiques, des tournures orales et des abr√©viations courantes en C√¥te d‚ÄôIvoire.
4.	Consid√©rations de confidentialit√© et de s√©curit√©
	√âtant donn√© la nature sensible des donn√©es manipul√©es (transactions financi√®res, identifiants utilisateurs, num√©ros de t√©l√©phone), des m√©canismes d‚Äôanonymisation et de masquage ont √©t√© syst√©matiquement appliqu√©s. Toutes les op√©rations de traitement sont conformes aux bonnes pratiques de protection des donn√©es (RGPD). Les identifiants et donn√©es personnelles sont remplac√©s par des marqueurs standardis√©s tels que <PHONE>, <TRANSACTION_ID> ou <CUSTOMER_REF> afin de pr√©server la structure conversationnelle tout en supprimant les informations sensibles.
II-	Pr√©traitement et structuration 
	Apr√®s l'identification et la collecte des donn√©es, leur pr√©traitement et leur structuration sont essentiels pour l'agent conversationnel. Cette phase est cruciale pour le m√©canisme RAG appliqu√© aux logs et √† la documentation, visant √† optimiser ces donn√©es pour la recherche s√©mantique et √† d√©finir l'interaction avec la base de donn√©es SQL.
1.	Le R√¥le Fondamental du D√©coupage en Segments (Chunking) dans une Architecture RAG
Au c≈ìur de l'approche RAG se trouve la capacit√© √† fournir au grand mod√®le de langage des extraits d'information contextuels et pertinents issus d'une vaste base de connaissances. √âtant donn√© que les LLMs ont une fen√™tre de contexte limit√©e (c'est-√†-dire une quantit√© maximale de texte qu'ils peuvent traiter en une seule fois), il est impossible de leur fournir l'int√©gralit√© de l‚Äôhistorique des conversations ou des manuels techniques volumineux pour chaque requ√™te. 
Le d√©coupage en segments (chunking) est le processus qui consiste √† diviser ces grandes sources de donn√©es textuelles en morceaux plus petits et g√©rables, appel√©s "chunks". Ces chunks pr√©sentent plusieurs avantages: 
‚Ä¢	Adaptation √† la Fen√™tre de Contexte du LLM : Les chunks sont dimensionn√©s pour tenir dans la fen√™tre de contexte du LLM, lui permettant de traiter efficacement l'information r√©cup√©r√©e.
‚Ä¢	Pr√©cision de la Recherche S√©mantique : La vectorisation est effectu√©e au niveau de chaque chunk. Des chunks plus petits et s√©mantiquement coh√©rents permettent une recherche plus cibl√©e et des r√©sultats plus pertinents. 
‚Ä¢	Granularit√© de l'Information : le chunking permet de r√©cup√©rer des passages sp√©cifiques et pertinents plut√¥t que des documents entiers, ce qui am√©liore la concision des informations fournies au LLM.
La strat√©gie de chunking est donc une d√©cision de conception cl√©, visant √† cr√©er des segments s√©mantiquement coh√©rents, de taille appropri√©e, et potentiellement avec un chevauchement optimal pour pr√©server le contexte.


2.	Nettoyage et Anonymisation des Donn√©es
Les jeux de donn√©es issus des conversations clients, de la FAQ et des documents internes ont √©t√© soumis √† une √©tape de nettoyage automatis√©e. Cette op√©ration comprend :
‚Ä¢	la suppression des caract√®res sp√©ciaux, balises et artefacts d‚Äôencodage ;
‚Ä¢	la correction de messages vides, tronqu√©s ou dupliqu√©s ;
‚Ä¢	l‚Äôapplication d‚Äôun module d‚Äôanonymisation contextuelle sp√©cifique au domaine FinTech, permettant de remplacer les informations sensibles (num√©ros de t√©l√©phone, identifiants de transaction, r√©f√©rences clients) par des marqueurs g√©n√©riques tels que <PHONE>, <TX_ID>, <USER_REF>.
Cette anonymisation garantit la conformit√© au RGPD et permet l‚Äôexploitation s√©curis√©e des donn√©es pour l‚Äôentra√Ænement et l‚Äôinf√©rence.
3.	Structuration des Conversations
Les conversations issues des canaux WhatsApp et autres plateformes sont converties en un format hi√©rarchique exploitable, conforme aux standards d‚Äôindexation et de recherche s√©mantique utilis√©s dans le cadre du RAG-Agentique. Chaque conversation est organis√©e selon la structure suivante :
 
Cette organisation facilite l‚Äôextraction contextuelle des √©changes entre le client et le support, tout en permettant √† l‚Äôagent d‚Äôeffectuer des recherches s√©mantiques cibl√©es dans les conversations pass√©es. Elle garantit √©galement la tra√ßabilit√© des intentions, la segmentation des th√®mes de requ√™tes (transaction, probl√®me technique, information g√©n√©rale), et l‚Äôint√©gration fluide avec le pipeline de vectorisation et d‚Äôindexation.


4.	Enrichissement S√©mantique et Uniformisation Linguistique
	Pour am√©liorer la coh√©rence lexicale entre les donn√©es formelles (FAQ, documentation) et les expressions informelles des clients, un dictionnaire d‚Äôexpressions et d‚Äôabr√©viations ivoiriennes est int√©gr√©.
	Ce corpus enrichit la base de connaissances de l‚Äôagent sans transformation du texte initial. Ainsi, lorsqu‚Äôune expression locale est d√©tect√©e (‚ÄúBjr‚Äù, ‚Äú pq‚Äù, ‚Äú Nn merci‚Äù), elle est reli√©e √† sa signification √©quivalente au moment de la recherche vectorielle, sans normalisation pr√©alable du texte source.
Cette approche favorise une compr√©hension contextuelle, tout en conservant la richesse linguistique propre au registre ivoirien.
5.	D√©coupage en Segments (Chunking)
	Le chunking permet de diviser les textes longs (FAQ, proc√©dures internes, historiques de logs) en fragments de taille adapt√©e √† la fen√™tre contextuelle du mod√®le de langage.
Les crit√®res retenus sont :
‚Ä¢	Taille : 512 tokens par chunk en moyenne, avec un chevauchement de 50 tokens ;
‚Ä¢	Coh√©rence s√©mantique : regroupement des phrases li√©es par th√®me ;
‚Ä¢	Conservation des m√©tadonn√©es : chaque chunk garde des informations telles que la source, la cat√©gorie ou la date.
Ces segments constituent l‚Äôunit√© de base pour la vectorisation et la recherche s√©mantique.

III-	Mod√©lisation vectorielle et indexation 
	L‚Äô√©tape suivante consiste √† repr√©senter les donn√©es textuelles sous une forme math√©matique exploitable par les mod√®les de r√©cup√©ration augment√©e (RAG). L‚Äôobjectif est de permettre √† l‚Äôagent de rechercher dynamiquement les informations pertinentes avant la g√©n√©ration de sa r√©ponse.
1.	Le Processus de Vectorisation (Embedding)
Le processus de vectorisation, ou embedding, consiste √† transformer les segments de texte (appel√©s chunks), qu‚Äôils proviennent de la documentation, des FAQ ou des conversations, en vecteurs num√©riques de haute dimensiodn. Ces vecteurs, appel√©s embeddings, capturent la signification s√©mantique des textes : cela signifie que deux textes ayant un sens proche seront repr√©sent√©s par des vecteurs proches dans un espace vectoriel, m√™me s‚Äôils n‚Äôutilisent pas les m√™mes mots.
Il existe plusieurs types d‚Äôembeddings utilis√©s en NLP. Parmi les plus connus, on trouve :
‚Ä¢	Les embeddings bas√©s sur la fr√©quence, comme TF-IDF ou les matrices de cooccurrence, qui encodent la fr√©quence des mots mais manquent de compr√©hension contextuelle.
‚Ä¢	Les word embeddings classiques de type Word2Vec (Skip-gram, CBOW), GloVe et FastText, qui apprennent des repr√©sentations denses des mots en se basant sur leur contexte local. FastText se distingue en prenant en compte la morphologie des mots via les sous-mots, ce qui est utile pour les langues avec de nombreuses variations morphologiques.
‚Ä¢	Les embeddings contextuels, produits par des mod√®les r√©cents de type Transformer comme BERT, RoBERTa, ou leurs variantes sp√©cialis√©es telles que Sentence-BERT, qui g√©n√®rent des vecteurs en tenant compte du contexte pr√©cis dans lequel les mots apparaissent. Ces embeddings sont les plus adapt√©s aux t√¢ches complexes de compr√©hension et de recherche s√©mantique.
	Pour notre √©tude, les embeddings contextuels produits par des mod√®les Transformers sont privil√©gi√©s car ils offrent une meilleure capture du sens dans les interactions clients r√©elles, incluant souvent des tournures informelles ou sp√©cifiques au domaine fintech d‚ÄôEasyTransfert.  Par exemple, dans une conversation client, la phrase "Ma transaction a √©chou√©" et "mon transfert d‚Äôargent n‚Äôest pas pass√©" seront repr√©sent√©es par des vecteurs refl√©tant leurs diff√©rences s√©mantiques avec pr√©cision Pour notre √©tude, nous avons choisi d‚Äôutiliser des mod√®les comme ceux disponibles sur Hugging Face (par exemple ¬´ sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 ¬ª) qui sont optimis√©s pour des langues multiples et adapt√©s aux moteurs de recherche semantic tels que la base vectorielle ChromaDB. Ainsi, le choix du mod√®le d‚Äôembedding conditionne directement la qualit√© des recherches s√©mantiques : des embeddings pr√©cis et contextuels permettent de retrouver les passages les plus pertinents dans la base documentaire et d‚Äôam√©liorer la pertinence des r√©ponses g√©n√©r√©es par l‚Äôassistant conversationnel intelligent.
2.	S√©lection de ChromaDB comme base de donn√©es vectorielle	
	ChromaDB est une base de donn√©es vectorielle open source permettant de stocker et d‚Äôinterroger efficacement des vecteurs issus des mod√®les d‚Äôapprentissage automatique utilis√©s en traitement du langage naturel. Elle facilite la recherche rapide de documents proches s√©mantiquement d‚Äôune requ√™te, gr√¢ce √† des algorithmes de recherche de voisins de haute performance. Son fonctionnement en m√©moire assure des temps d‚Äôacc√®s tr√®s faibles, capital pour un assistant conversationnel en temps r√©el. De plus, ChromaDB propose une API simple, favorisant une int√©gration fluide dans les pipelines d‚Äôindexation et de recherche documentaires.
	Le choix de ChromaDB pour EasyTransfert repose sur sa l√©g√®ret√©, sa facilit√© d‚Äôinstallation, son caract√®re open source, ainsi que sa compatibilit√© native avec les biblioth√®ques NLP courantes comme Hugging Face. Cette base combine performance, souplesse et contr√¥le total des donn√©es, qualit√©s essentielles pour respecter les contraintes de s√©curit√© et de confidentialit√© dans le secteur fintech. Son interface √©pur√©e permet √©galement une maintenance simplifi√©e et une rapide mont√©e en comp√©tence des √©quipes techniques.
	Compar√©e √† d‚Äôautres solutions vectorielles telles que FAISS ou Pinecone, ChromaDB offre un excellent compromis entre performance et simplicit√©. FAISS, bien que tr√®s performant, demande plus de ressources et d‚Äôexpertise pour une optimisation fine. Pinecone est un service h√©berg√© moins flexible et potentiellement plus co√ªteux. ChromaDB, quant √† elle, assure un d√©ploiement local ou cloud avec un contr√¥le complet sur les donn√©es et une bonne scalabilit√©, tout en maintenant une rapidit√© de recherche adapt√©e aux volumes attendus. Ces avantages en font un choix judicieux dans le cadre du projet d‚Äôassistant conversationnel intelligent de EasyTransfert.

CHAPITRE V : CONCEPTION DES ARCHITECTURES EXP√âRIMENTALES

I-	Architecture 1 : Agent Conversationnel Simple (Baseline)
1.	Pr√©sentation g√©n√©rale
	L'Architecture 1 constitue notre solution de r√©f√©rence (baseline) pour l'√©valuation comparative. Elle repose sur un mod√®le de langage de grande taille (Large Language Model - LLM) affin√© sp√©cifiquement sur le corpus conversationnel d'EasyTransfert. Cette approche privil√©gie la simplicit√© architecturale et l'efficacit√© computationnelle, en s'inspirant des m√©thodologies d'optimisation propos√©es par Unsloth pour l'entra√Ænement rapide et √©conome en ressources des mod√®les de langage.
Principe fondamental : Dans cette architecture, toutes les connaissances n√©cessaires au support client sont encod√©es directement dans les param√®tres du mod√®le durant la phase de fine-tuning. Le mod√®le apprend √† reconna√Ætre les intentions des utilisateurs (transaction non aboutie, mot de passe oubli√©, erreur de num√©ro, etc.), √† identifier les entit√©s pertinentes (identifiants EasyTransfert, identifiants op√©rateurs, num√©ros de t√©l√©phone, montants) et √† g√©n√©rer des r√©ponses contextuelles appropri√©es sans recourir √† des sources d'information externes.
Le mod√®le encode dans sa m√©moire param√©trique :
‚Ä¢	Les patterns conversationnels observ√©s dans les 10 000+ conversations historiques
‚Ä¢	Les connaissances sur les cinq op√©rateurs mobiles (MTN, Orange, Moov, Wave, Tr√©sor Money)
‚Ä¢	Les proc√©dures de r√©solution des probl√®mes fr√©quents (transaction √©chou√©e, remboursement)
‚Ä¢	Les formats de r√©ponse et le ton de communication empathique de la marque EasyTransfert
‚Ä¢	Les r√®gles m√©tier (formats d'identifiants, compatibilit√©s entre op√©rateurs)


2.	Architecture technique et flux de traitement
 
Figure 3: Architecture de l'Agent Conversationnel Simple

Le flux de traitement se d√©compose en trois phases :
Phase 1 - Pr√©paration : La requ√™te utilisateur subit un pr√©traitement (nettoyage des √©mojis, normalisation des accents, conversion en minuscules) puis est structur√©e selon le template de conversation Llama 3 avec balises syst√®me/utilisateur/assistant.
Phase 2 - Inf√©rence : Le mod√®le LLM quantifi√© en 4-bit traite la s√©quence tokenis√©e et g√©n√®re la r√©ponse token par token de mani√®re autor√©gressive, avec une temp√©rature de 0.7 pour √©quilibrer coh√©rence et diversit√©.
Phase 3 - Finalisation : Les tokens g√©n√©r√©s sont reconvertis en texte naturel, format√©s avec les √©mojis appropri√©s et structur√©s selon les conventions d'EasyTransfert.
3.	Composants fondamentaux
Le mod√®le choisi est Llama 3.2 3B Instruct qui est un mod√®le open-source de 3 milliards de param√®tres offrant un compromis optimal entre performance conversationnelle et efficacit√© computationnelle pour notre cas d'usage. Ses caract√©ristiques cl√©s incluent un support natif du fran√ßais (essentiel pour la C√¥te d'Ivoire), une orientation instruction suivant les tours de conversation, et une empreinte m√©moire r√©duite adapt√©e au d√©ploiement sur infrastructure modeste.
Pour adapter le mod√®le au domaine sp√©cifique de EasyTransfert, nous utilisons la technique LoRA (Low-Rank Adaptation) qui permet un entra√Ænement efficace en ne modifiant qu'un sous-ensemble de param√®tres via des matrices de faible rang.
 
Figure 4Principe de l'adaptation LoRA
Configuration LoRA :
‚Ä¢	Rang (r) : 16 - Dimension des matrices de faible rang
‚Ä¢	Alpha (Œ±) : 32 - Facteur de mise √† l'√©chelle des adaptateurs
‚Ä¢	Dropout : 0.05 - R√©gularisation contre le surapprentissage
‚Ä¢	Modules cibles : Matrices Query/Key/Value des couches d'attention
‚Ä¢	Param√®tres entra√Ænables : ~1% du mod√®le total (~30M sur 3B)
Avantages : Entra√Ænement 3-4√ó plus rapide qu'un fine-tuning complet, adaptateurs l√©gers (~50 MB) contre 12 GB pour le mod√®le complet, fonctionnement sur GPU grand public (RTX 4090, A10).
Le mod√®le utilise un template conforme au format Llama 3 Chat pour maintenir la coh√©rence. Ce prompt syst√®me encode les connaissances essentielles : r√¥le de l'agent, comportements attendus (empathie, m√©morisation contextuelle), r√®gles m√©tier (formats d'identifiants par op√©rateur), et informations de contact pour escalade vers le service client humain (2522018730, disponible 24h/24 via WhatsApp).
Le prompt int√®gre les directives critiques du syst√®me EasyTransfert :
‚Ä¢	M√©morisation obligatoire : Ne jamais redemander une information d√©j√† fournie
‚Ä¢	Analyse d'images : Identifier m√©ticuleusement les identifiants sur les captures d'√©cran
‚Ä¢	Formats d'identifiants : EFB.* (EasyTransfert), MP* (Orange envoi), chiffres uniquement (MTN), MRCH*/CF* (Moov envoi), format variable souvent T* (Wave)
‚Ä¢	Ton chaleureux : Utilisation d'√©mojis appropri√©s , mentions r√©guli√®res d'EasyTransfert
‚Ä¢	Conscience contextuelle : Pas de r√©p√©tition des salutations, adaptation au niveau d'urgence de l'utilisateur


II-	Architecture 2 : RAG Standard (G√©n√©ration Augment√©e par R√©cup√©ration)
1.	Pr√©sentation g√©n√©rale et principe du RAG
L'Architecture 2 introduit un syst√®me de G√©n√©ration Augment√©e par R√©cup√©ration (Retrieval-Augmented Generation - RAG) qui repr√©sente une √©volution significative. Le principe fondamental consiste √† s√©parer la capacit√© de raisonnement (port√©e par le LLM) de la base de connaissances factuelles (stock√©e dans une base vectorielle externe ChromaDB).
Contrairement aux mod√®les purement g√©n√©ratifs qui s'appuient uniquement sur leurs param√®tres pr√©-entra√Æn√©s (m√©moire param√©trique), le RAG permet de :
ÔÇß	R√©cup√©rer des informations pertinentes √† partir de sources de donn√©es actualis√©es
ÔÇß	Augmenter le contexte du prompt avec ces informations factuelles v√©rifi√©es
ÔÇß	G√©n√©rer une r√©ponse fond√©e sur ce contexte enrichi
Cette s√©paration offre plusieurs avantages conceptuels : connaissances actualisables sans r√©-entra√Ænement (ajout de nouveaux documents dans ChromaDB), pr√©cision factuelle accrue (r√©ponses ancr√©es dans des sources v√©rifiables), tra√ßabilit√© (possibilit√© de citer les sources), et r√©duction drastique des hallucinations (le mod√®le s'appuie sur des faits r√©cup√©r√©s).
Application au contexte EasyTransfert : 
La base vectorielle ChromaDB contient des documents segment√©s (chunks) issus de :
‚Ä¢	FAQ EasyTransfert officielles (50-100 paires question-r√©ponse)
‚Ä¢	Proc√©dures op√©rationnelles de r√©solution (transaction √©chou√©e, remboursement, erreur de num√©ro)
‚Ä¢	Documentation des cinq op√©rateurs (formats identifiants, compatibilit√©s, limites de transaction)
‚Ä¢	Conversations historiques anonymis√©es repr√©sentant des r√©solutions r√©ussies (√©chantillons de 500-1000 √©changes)
‚Ä¢	Guides utilisateur et tutoriels de l'application





2.	Architecture technique et flux RAG
 
Figure 5Architecture compl√®te du syst√®me RAG Standard
Le flux de traitement RAG se d√©compose en deux phases principales :
Phase de R√©cup√©ration (~150-200ms) :
‚Ä¢	La question utilisateur est vectoris√©e via le mod√®le d'embedding multilingue (768 dimensions)
‚Ä¢	ChromaDB effectue une recherche de similarit√© cosinus sur les embeddings index√©s
‚Ä¢	Les 3 chunks les plus pertinents (score > 0.5) sont s√©lectionn√©s
‚Ä¢	Le prompt est enrichi avec ces chunks + m√©tadonn√©es 
Phase de G√©n√©ration (~2-3s) : 
‚Ä¢	Le LLM re√ßoit le prompt augment√© (contexte + question)
‚Ä¢	G√©n√©ration conditionn√©e sur le contexte fourni avec instructions strictes (pas d'hallucination) 
‚Ä¢	Post-traitement incluant les citations des sources utilis√©es
3.	Composants fondamentaux
ÔÅ∂	Module d'embedding et base vectorielle ChromaDB :
Mod√®le de vectorisation : paraphrase-multilingual-mpnet-base-v2
Ce mod√®le Sentence-Transformer produit des embeddings de 768 dimensions capturant la s√©mantique du texte. Ses caract√©ristiques cl√©s : support de 50+ langues incluant fran√ßais et anglais (essentiel pour le code-switching en C√¥te d'Ivoire), architecture MPNet (12 couches Transformer), normalisation L2 des vecteurs pour similarit√© cosinus, performance de ~2000 phrases/seconde sur CPU.
Structure de la base vectorielle ChromaDB :




ÔÅ∂	Strat√©gie de chunking adaptative :
Les param√®tres de chunking sont optimis√©s pour le contexte de EasyTransfert : taille maximale de 512 tokens (~400 mots fran√ßais), chevauchement de 50 tokens (10%) pour √©viter la perte de contexte aux fronti√®res, s√©parateurs hi√©rarchiques (double saut de ligne, point, virgule). Chaque chunk est enrichi de m√©tadonn√©es : cat√©gorie (faq, procedure, operator_info, conversation), op√©rateur concern√© (MTN, Orange, Moov, Wave, Tous), mots-cl√©s extraits, date de cr√©ation.
4.	Format de prompt augment√©
Le prompt RAG int√®gre le contexte r√©cup√©r√© de mani√®re structur√©e. Ce format pr√©sente trois avantages cl√©s : s√©paration visuelle claire entre contexte et question, m√©tadonn√©es explicites pour tra√ßabilit√©, instructions strictes contre les hallucinations.
Le syst√®me prompt inclut :
‚Ä¢	Section contexte r√©cup√©r√© : 3 documents num√©rot√©s avec scores de pertinence, contenus complets, sources et cat√©gories
‚Ä¢	Instructions critiques : Contraintes explicites (se baser UNIQUEMENT sur le contexte fourni, ne jamais inventer d'informations, citer les sources si pertinent, avouer l'absence d'information si n√©cessaire)
‚Ä¢	Comportements attendus : Ton courtois et empathique, utilisation d'√©mojis appropri√©s, respect des directives de m√©morisation contextuelle de EasyTransfert.

III-	Architecture 3 : RAG-Agentique (Agent Augment√© par Outils)

1.	Pr√©sentation g√©n√©rale et paradigme agentique
L'Architecture 3 repr√©sente l'approche la plus sophistiqu√©e, combinant les avantages du RAG (Architecture 2) avec des capacit√©s agentiques : un syst√®me autonome capable de raisonner, de planifier et d'utiliser des outils externes pour r√©soudre des requ√™tes complexes multi-√©tapes typiques du support client EasyTransfert.
L'aspect agentique conf√®re au syst√®me plusieurs capacit√©s avanc√©es absentes des architectures pr√©c√©dentes :
ÔÇß	Autonomie d√©cisionnelle : L'agent d√©termine lui-m√™me quelles actions entreprendre selon le contexte
ÔÇß	Planification : D√©composition de probl√®mes complexes en sous-t√¢ches s√©quentielles avec d√©pendances
ÔÇß	Utilisation d'outils : Acc√®s dynamique √† APIs backend, bases de donn√©es transactionnelles, fonctions sp√©cialis√©es
ÔÇß	Raisonnement it√©ratif : Cycle Pens√©e-Action-Observation r√©p√©t√© jusqu'√† r√©solution compl√®te
ÔÇß	Adaptation contextuelle : Ajustement du comportement selon les observations interm√©diaires
Paradigme ReAct (Reasoning + Acting) : L'architecture s'inspire du framework ReAct qui structure le raisonnement en cycles explicites :


 
Figure 8 Architecture compl√®te du syst√®me RAG-Agentique
Le cycle ReAct fonctionne comme une machine √† √©tats :
 
Figure 9Machine √† √©tats du cycle ReAct
2.	Composants fondamentaux
La toolbox comprend quatre outils m√©tier encapsulant les fonctionnalit√©s critiques d'EasyTransfert :
ÔÅ∂	Outil 1 : RAG Retriever
-	Recherche vectorielle dans ChromaDB (identique Architecture 2, encapsul√© comme outil invocable).
-	Param√®tres : query (question), top_k (d√©faut 3), filters (cat√©gorie, date, op√©rateur).
-	Retour : Liste de chunks avec contenus, m√©tadonn√©es, scores de pertinence.

ÔÅ∂	Outil 2 : Operator Info
-	Consulte base PostgreSQL contenant informations structur√©es sur les cinq op√©rateurs.
-	Donn√©es : Nom complet, formats identifiants, compatibilit√©s, frais de transaction, limites (min/max), pr√©fixes num√©ros t√©l√©phone.
ÔÅ∂	Outil 3 : Entity Extractor
Extraction d'entit√©s nomm√©es via regex + r√®gles m√©tier EasyTransfert.
Patterns reconnus :
‚Ä¢	Identifiant EasyTransfert : EFB\.[A-Z0-9]+
‚Ä¢	Identifiant MTN : S√©rie de chiffres uniquement
‚Ä¢	Identifiant Orange : MP[0-9]{10}
‚Ä¢	Identifiant Moov : (MRCH|CF)[A-Z0-9]+
‚Ä¢	Identifiant Wave : Variable (souvent d√©bute par T)
‚Ä¢	Num√©ros t√©l√©phone : (\+225)?[0-9]{8,10}
‚Ä¢	Montants : [0-9]+\s?(FCFA|CFA|francs?)?
‚Ä¢	Op√©rateurs : (MTN|Orange|Moov|Wave|Tr√©sor\s?Money|Tremo)
‚Ä¢	Retour : Dictionnaire avec listes d'entit√©s extraites par cat√©gorie.

ÔÅ∂	Outil 4 : Conversation Memory
G√®re l'historique conversationnel pour maintenir le contexte multi-tours.
Param√®tres : user_id, action (get/update/search).
Retour : Historique messages, contexte actuel, probl√®mes similaires pass√©s.
Module de planification (Planning)
Pour les requ√™tes complexes multi-√©tapes, l'agent d√©compose la t√¢che en DAG (Directed Acyclic Graph) :











