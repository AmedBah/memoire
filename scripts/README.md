# Scripts d'√âvaluation des Architectures

Ce dossier contient les scripts pour √©valuer et comparer les diff√©rentes architectures conversationnelles du projet EasyTransfert.

## üìÅ Fichiers

### 1. `split_data.py`
Script pour diviser les donn√©es de conversation en ensembles train/validation/test.

**Usage:**
```bash
python split_data.py \
    --input data/conversations/conversation_1000_finetune.jsonl \
    --output data/conversations/splits \
    --train-ratio 0.80 \
    --val-ratio 0.15 \
    --test-ratio 0.05 \
    --seed 42
```

**R√©sultat:**
- `conversations_train.jsonl` (80% - 2424 conversations)
- `conversations_validation.jsonl` (15% - 454 conversations)
- `conversations_test.jsonl` (5% - 153 conversations)

### 2. `evaluation_metrics.py`
Module contenant toutes les m√©triques d'√©valuation.

**M√©triques impl√©ment√©es:**
- **Perplexity**: Mesure l'incertitude du mod√®le (n√©cessite PyTorch)
- **BLEU Score** (BLEU-1 √† BLEU-4): Similarit√© entre texte g√©n√©r√© et r√©f√©rence
- **ROUGE Scores** (ROUGE-1, ROUGE-2, ROUGE-L): Rappel et chevauchement de tokens
- **F1 Score**: Moyenne harmonique de pr√©cision et rappel
- **Similarit√© s√©mantique**: Cosine similarity ou Jaccard similarity
- **Pertinence de la r√©ponse**: Ad√©quation entre r√©ponse et requ√™te

**Usage dans du code:**
```python
from evaluation_metrics import evaluate_response

reference = "Bonjour, pour transf√©rer de l'argent..."
hypothesis = "Salut, pour faire un transfert..."

metrics = evaluate_response(reference, hypothesis)
print(metrics)
# Output: {'bleu-1': 0.67, 'rouge-1': 0.63, ...}
```

### 3. `compare_architectures.py`
Script pour √©valuer et comparer plusieurs architectures.

**Classes principales:**

#### `ArchitectureEvaluator`
√âvalue une architecture sur un dataset de test.

```python
from compare_architectures import ArchitectureEvaluator, load_test_data

# Charger les donn√©es
test_data = load_test_data('data/conversations/splits/conversations_test.jsonl')

# Cr√©er un √©valuateur
evaluator = ArchitectureEvaluator('Architecture 1')

# D√©finir votre fonction de chat
def my_chat_function(query):
    # Votre logique ici
    return "R√©ponse du chatbot"

# √âvaluer
evaluator.evaluate_dataset(test_data, my_chat_function, max_samples=10)
evaluator.print_summary()
evaluator.save_results('results/arch1_results.json')
```

#### `compare_architectures()`
Compare les r√©sultats de plusieurs architectures.

```bash
python compare_architectures.py compare \
    results/architecture_1_results.json \
    results/architecture_2_results.json \
    results/architecture_3_results.json
```

### 4. `example_evaluation.py`
Exemple complet d'utilisation du syst√®me d'√©valuation avec des architectures simul√©es.

**Usage:**
```bash
python example_evaluation.py
```

Ce script:
1. Charge les donn√©es de test
2. √âvalue 3 architectures simul√©es sur 10 conversations
3. Affiche les r√©sultats comparatifs
4. Sauvegarde les r√©sultats dans `results/`

## üìä M√©triques d'√âvaluation

### M√©triques de Performance

| M√©trique | Description | Unit√© | Meilleur |
|----------|-------------|-------|----------|
| **Latence** | Temps de r√©ponse moyen | ms | Plus bas |
| **Throughput** | Nombre de requ√™tes par seconde | req/s | Plus haut |
| **√âcart-type latence** | Variabilit√© du temps de r√©ponse | ms | Plus bas |

### M√©triques de Qualit√© NLP

| M√©trique | Description | Plage | Meilleur |
|----------|-------------|-------|----------|
| **BLEU-1 √† BLEU-4** | Pr√©cision des n-grammes (1 √† 4) | 0-1 | Plus haut |
| **ROUGE-1** | Chevauchement des unigrammes | 0-1 | Plus haut |
| **ROUGE-2** | Chevauchement des bigrammes | 0-1 | Plus haut |
| **ROUGE-L** | Plus longue sous-s√©quence commune | 0-1 | Plus haut |
| **Similarit√© s√©mantique** | Similarit√© contextuelle | 0-1 | Plus haut |
| **Pertinence** | Ad√©quation r√©ponse/requ√™te | 0-1 | Plus haut |

### Interpr√©tation des Scores

#### BLEU Score
- **> 0.70**: Excellent
- **0.50 - 0.70**: Bon
- **0.30 - 0.50**: Acceptable
- **< 0.30**: Faible

#### ROUGE Score
- **> 0.70**: Tr√®s bon
- **0.50 - 0.70**: Bon
- **0.30 - 0.50**: Moyen
- **< 0.30**: Faible

#### Similarit√© S√©mantique
- **> 0.85**: Excellent
- **0.70 - 0.85**: Bon
- **0.50 - 0.70**: Acceptable
- **< 0.50**: Insuffisant

## üöÄ Utilisation Compl√®te

### √âtape 1: Pr√©parer les Donn√©es

```bash
# Diviser les donn√©es en train/val/test
python split_data.py
```

### √âtape 2: √âvaluer Chaque Architecture

Cr√©ez un script pour chaque architecture:

```python
# evaluate_arch1.py
from compare_architectures import ArchitectureEvaluator, load_test_data

# Charger votre mod√®le
# model = load_your_model()

def chat_function(query):
    # Logique de votre architecture
    response = model.generate(query)
    return response

# √âvaluer
test_data = load_test_data('data/conversations/splits/conversations_test.jsonl')
evaluator = ArchitectureEvaluator('Architecture 1')
evaluator.evaluate_dataset(test_data, chat_function, num_runs=3)
evaluator.print_summary()
evaluator.save_results('results/arch1_results.json')
```

### √âtape 3: Comparer les R√©sultats

```bash
python compare_architectures.py compare \
    results/arch1_results.json \
    results/arch2_results.json \
    results/arch3_results.json
```

## üìù Format des R√©sultats

Les r√©sultats sont sauvegard√©s au format JSON:

```json
{
  "architecture": "Architecture 1",
  "num_samples": 153,
  "statistics": {
    "latency_ms_mean": 2847.5,
    "latency_ms_std": 85.2,
    "bleu-1_mean": 0.68,
    "bleu-4_mean": 0.56,
    "rouge-1_mean": 0.72,
    "rouge-l_mean": 0.69,
    "semantic_similarity_mean": 0.78,
    "relevance_mean": 0.82
  },
  "detailed_results": [
    {
      "query": "...",
      "reference": "...",
      "generated": "...",
      "latency_ms": 2845.3,
      "bleu-1": 0.72,
      ...
    }
  ]
}
```

## üîß D√©pendances

**Minimales (pour m√©triques de base):**
```bash
pip install numpy
```

**Compl√®tes (avec PyTorch et embeddings):**
```bash
pip install torch numpy sentence-transformers
```

## üìö R√©f√©rences

- **BLEU**: Papineni et al. (2002) - "BLEU: a Method for Automatic Evaluation of Machine Translation"
- **ROUGE**: Lin (2004) - "ROUGE: A Package for Automatic Evaluation of Summaries"
- **BERTScore**: Zhang et al. (2020) - "BERTScore: Evaluating Text Generation with BERT"

## üÜò Support

Pour des questions ou des probl√®mes, consultez:
- La documentation principale: `../ARCHITECTURE_README.md`
- Les notebooks d'exemples: `../notebooks/evaluation/`
- Le guide d'impl√©mentation: `../IMPLEMENTATION_SUMMARY.md`
