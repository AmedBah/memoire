# Architectures Exp√©rimentales - Syst√®me Conversationnel EasyTransfert

Ce d√©p√¥t impl√©mente trois architectures exp√©rimentales pour un assistant conversationnel intelligent destin√© au service client d'EasyTransfert, une application de transfert d'argent mobile en C√¥te d'Ivoire.

## üìã Table des Mati√®res

- [Vue d'ensemble](#vue-densemble)
- [üöÄ Utilisation sur Google Colab Pro](#-utilisation-sur-google-colab-pro)
- [Architecture 1: Agent Simple (Baseline)](#architecture-1-agent-simple-baseline)
- [Architecture 2: RAG Standard](#architecture-2-rag-standard)
- [Architecture 3: RAG-Agentique](#architecture-3-rag-agentique)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Sources de Donn√©es](#sources-de-donn√©es)
- [Comparaison des Architectures](#comparaison-des-architectures)

## üéØ Vue d'ensemble

Le projet vise √† automatiser le service client d'EasyTransfert en explorant trois approches progressives :

1. **Architecture 1** : Fine-tuning d'un LLM (approche baseline)
2. **Architecture 2** : Retrieval-Augmented Generation (RAG)
3. **Architecture 3** : RAG avec capacit√©s agentiques (ReAct)

### Contexte EasyTransfert

EasyTransfert permet des transferts d'argent inter-op√©rateurs entre :
- MTN Mobile Money
- Orange Money
- Moov Money
- Wave
- Tr√©sor Money

## üöÄ Utilisation sur Google Colab Pro

**Tous les notebooks sont optimis√©s pour Google Colab Pro !**

### Acc√®s Rapide

Chaque notebook dispose d'un badge "Open in Colab" pour un lancement en un clic :

| Architecture | Notebook | Badge Colab |
|-------------|----------|-------------|
| Architecture 1 | Fine-tuning LoRA | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AmedBah/memoire/blob/main/notebooks/architecture_1/01_architecture_1_simple_agent_finetuning.ipynb) |
| Architecture 2 | RAG Standard | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AmedBah/memoire/blob/main/notebooks/architecture_2/02_architecture_2_rag_standard.ipynb) |
| Architecture 3 | RAG-Agentique | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AmedBah/memoire/blob/main/notebooks/architecture_3/03_architecture_3_rag_agentique.ipynb) |

### Configuration Automatique

Les notebooks incluent automatiquement :
- ‚úÖ **D√©tection d'environnement** : Colab vs Local
- ‚úÖ **V√©rification GPU** : Type et m√©moire disponible
- ‚úÖ **Montage Google Drive** : Pour donn√©es persistantes
- ‚úÖ **Clonage du repository** : Acc√®s automatique aux donn√©es
- ‚úÖ **Chemins flexibles** : Fonctionnement sur Colab et en local

### Runtimes Recommand√©s

| Architecture | GPU Minimum | GPU Optimal | RAM | Dur√©e |
|-------------|-------------|-------------|-----|-------|
| Architecture 1 | T4 (16 GB) | V100/A100 | High-RAM | 2-4h |
| Architecture 2 | T4 (16 GB) | T4/V100 | Standard | 30-60min |
| Architecture 3 | T4 (16 GB) | V100 | High-RAM | 1-2h |

### Guide Complet

Pour un guide d√©taill√© sur l'utilisation de Colab Pro, consultez : **[COLAB_GUIDE.md](./COLAB_GUIDE.md)**

Le guide couvre :
- Configuration initiale et authentification
- Gestion des donn√©es (Drive vs clonage)
- Optimisation de la m√©moire GPU
- R√©solution de probl√®mes courants
- Bonnes pratiques et conseils

## üèóÔ∏è Architecture 1: Agent Simple (Baseline)

### Description

Architecture de r√©f√©rence bas√©e sur un mod√®le Llama 3.2 3B fine-tun√© avec LoRA sur les conversations historiques d'EasyTransfert.

### Caract√©ristiques

- **Mod√®le** : Llama 3.2 3B Instruct
- **Technique** : LoRA (r=16, Œ±=32, dropout=0.05)
- **Framework** : Unsloth pour entra√Ænement optimis√©
- **Donn√©es** : 3000+ conversations historiques
- **M√©moire** : ~50 MB (adaptateurs LoRA)

### Avantages

‚úÖ Simplicit√© architecturale  
‚úÖ Inf√©rence rapide (~2-3s)  
‚úÖ Faible empreinte m√©moire  
‚úÖ D√©ploiement facile  
‚úÖ Style conversationnel coh√©rent  

### Limites

‚ùå Risque d'hallucinations  
‚ùå N√©cessite r√©entra√Ænement pour nouvelles donn√©es  
‚ùå Pas de tra√ßabilit√© des sources  
‚ùå Connaissances limit√©es aux donn√©es d'entra√Ænement  

### Notebook

```
notebooks/architecture_1/01_architecture_1_simple_agent_finetuning.ipynb
```

## üîç Architecture 2: RAG Standard

### Description

Syst√®me de G√©n√©ration Augment√©e par R√©cup√©ration qui s√©pare la capacit√© de raisonnement (LLM) de la base de connaissances (ChromaDB).

### Caract√©ristiques

- **Base vectorielle** : ChromaDB
- **Embedding** : paraphrase-multilingual-mpnet-base-v2 (768 dims)
- **Chunking** : 512 tokens max, 50 tokens overlap
- **LLM** : Llama 3.2 3B (peut utiliser le mod√®le fine-tun√©)
- **Sources** : FAQ, proc√©dures, documentation op√©rateurs, conversations

### Flux RAG

#### Phase de R√©cup√©ration (~150-200ms)
1. Vectorisation de la question utilisateur
2. Recherche de similarit√© cosinus dans ChromaDB
3. S√©lection des top-k chunks (d√©faut: 3, score > 0.5)
4. Enrichissement du prompt avec contexte + m√©tadonn√©es

#### Phase de G√©n√©ration (~2-3s)
1. LLM re√ßoit le prompt augment√©
2. G√©n√©ration conditionn√©e sur le contexte
3. Post-traitement avec citations des sources

### Avantages

‚úÖ V√©racit√© : r√©ponses ancr√©es dans sources v√©rifiables  
‚úÖ Tra√ßabilit√© : citations des sources  
‚úÖ Actualisation : ajout de documents sans r√©entra√Ænement  
‚úÖ R√©duction drastique des hallucinations  
‚úÖ Transparence avec scores de pertinence  

### Limites

‚ùå Latence plus √©lev√©e (~2-3.5s)  
‚ùå Complexit√© accrue (pipeline multi-composants)  
‚ùå D√©pendances multiples  
‚ùå Pas de raisonnement multi-√©tapes  

### Notebook

```
notebooks/architecture_2/02_architecture_2_rag_standard.ipynb
```

## ü§ñ Architecture 3: RAG-Agentique

### Description

Architecture la plus sophistiqu√©e combinant RAG avec des capacit√©s agentiques selon le paradigme ReAct (Reasoning + Acting).

### Caract√©ristiques

- **Framework** : LangChain + LangGraph
- **Paradigme** : ReAct (Thought-Action-Observation)
- **Base** : Tout de l'Architecture 2 + couche agentique
- **Capacit√©s** : Raisonnement multi-√©tapes, planification, adaptation

### Toolbox (4 Outils M√©tier)

#### 1. RAG Retriever
- Recherche vectorielle dans ChromaDB
- Param√®tres : query, top_k, filters
- Retour : Chunks avec m√©tadonn√©es et scores

#### 2. Operator Info
- Consultation base PostgreSQL (simul√©e)
- Donn√©es : Formats identifiants, limites, frais, compatibilit√©s
- Retour : Informations structur√©es par op√©rateur

#### 3. Entity Extractor
- Extraction via regex + r√®gles m√©tier
- Patterns : Identifiants EasyTransfert/op√©rateurs, t√©l√©phones, montants
- Retour : Dictionnaire d'entit√©s par cat√©gorie

#### 4. Conversation Memory
- Gestion historique conversationnel
- Actions : get, update, search
- Retour : Contexte et probl√®mes similaires pass√©s

### Cycle ReAct

```
1. Thought  : Analyse et planification
2. Action   : Choix et invocation d'outil
3. Observation : Examen du r√©sultat
4. (R√©p√©ter jusqu'√† r√©solution)
5. Final Answer : R√©ponse utilisateur
```

### Avantages

‚úÖ Tout de l'Architecture 2 +  
‚úÖ Raisonnement multi-√©tapes et planification  
‚úÖ Acc√®s bases de donn√©es et APIs  
‚úÖ Adaptation contextuelle et √©motionnelle  
‚úÖ Autonomie d√©cisionnelle  
‚úÖ Tra√ßabilit√© compl√®te (cycle ReAct visible)  

### Limites

‚ùå Latence plus √©lev√©e (~3-5s)  
‚ùå Complexit√© architecturale √©lev√©e  
‚ùå Consommation m√©moire importante  
‚ùå Configuration plus d√©licate  

### Notebook

```
notebooks/architecture_3/03_architecture_3_rag_agentique.ipynb
```

## üì¶ Installation

### Pr√©requis

- Python 3.10+
- CUDA 11.8+ (pour GPU, recommand√©)
- 16 GB RAM minimum (32 GB recommand√©)
- GPU avec 8 GB VRAM minimum (pour Llama 3.2 3B en 4-bit)

### Installation des D√©pendances

```bash
# Cloner le d√©p√¥t
git clone https://github.com/AmedBah/memoire.git
cd memoire

# Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les d√©pendances
pip install -r requirements.txt
```

### D√©pendances Principales

- **LLM & Fine-tuning** : transformers, unsloth, peft, bitsandbytes
- **RAG & Vectoriel** : chromadb, sentence-transformers, langchain
- **Agent** : langgraph, langchain-community
- **Utils** : pandas, numpy, wandb, jupyter

## üöÄ Utilisation

### Architecture 1 : Fine-tuning

```bash
jupyter notebook notebooks/architecture_1/01_architecture_1_simple_agent_finetuning.ipynb
```

**√âtapes** :
1. Charger Llama 3.2 3B avec Unsloth
2. Configurer LoRA (r=16, Œ±=32)
3. Pr√©parer les donn√©es (conversation_1000_finetune.jsonl)
4. Entra√Æner avec SFTTrainer
5. Tester l'inf√©rence

### Architecture 2 : RAG

```bash
jupyter notebook notebooks/architecture_2/02_architecture_2_rag_standard.ipynb
```

**√âtapes** :
1. Initialiser ChromaDB
2. Charger mod√®le d'embedding (paraphrase-multilingual-mpnet-base-v2)
3. Cr√©er chunks (512 tokens, overlap 50)
4. Vectoriser et indexer documents
5. Impl√©menter pipeline RAG (retrieval + generation)

### Architecture 3 : RAG-Agentique

```bash
jupyter notebook notebooks/architecture_3/03_architecture_3_rag_agentique.ipynb
```

**√âtapes** :
1. Initialiser ChromaDB (comme Architecture 2)
2. Cr√©er les 4 outils m√©tier
3. Configurer LLM avec prompt ReAct
4. Cr√©er l'agent avec LangChain
5. Tester le cycle ReAct

## üìä Sources de Donn√©es

Notebook de d√©monstration :
```bash
jupyter notebook notebooks/data_examples/data_sources_examples.ipynb
```

### Types de Donn√©es

#### 1. Conversations Historiques
- **Fichier** : `conversation_1000_finetune.jsonl`
- **Format** : JSON Lines avec messages role-based
- **Usage** : Fine-tuning Architecture 1, exemples pour RAG

#### 2. FAQ EasyTransfert
- **Contenu** : Questions-r√©ponses officielles
- **Cat√©gories** : G√©n√©ral, op√©rateurs, utilisation, tarifs, probl√®mes
- **Usage** : Base de connaissances RAG

#### 3. Documentation Op√©rateurs
- **Contenu** : Formats identifiants, limites, frais, compatibilit√©s
- **Op√©rateurs** : MTN, Orange, Moov, Wave, Tr√©sor Money
- **Usage** : Outil Operator Info (Architecture 3)

#### 4. Proc√©dures de R√©solution
- **Contenu** : Guides √©tape par √©tape pour probl√®mes courants
- **Types** : Transaction √©chou√©e, mot de passe, erreur num√©ro
- **Usage** : Base de connaissances RAG

#### 5. Expressions Ivoiriennes
- **Contenu** : Abr√©viations et expressions locales
- **Usage** : Enrichissement linguistique, Entity Extractor

#### 6. Logs de Transactions
- **Format** : JSON avec m√©tadonn√©es compl√®tes
- **Usage** : Simulation v√©rification statut (Architecture 3)

## üìà Comparaison des Architectures

| Crit√®re | Architecture 1 | Architecture 2 | Architecture 3 |
|---------|---------------|---------------|---------------|
| **Latence** | ~2-3s | ~2-3.5s | ~3-5s |
| **M√©moire** | Faible (~50 MB) | Moyenne | √âlev√©e |
| **Complexit√©** | Simple | Mod√©r√©e | √âlev√©e |
| **Hallucinations** | √âlev√© | Faible | Minimal |
| **Tra√ßabilit√©** | Aucune | Citations | Compl√®te |
| **Actualisation** | R√©entra√Ænement | Ajout docs | Ajout docs/outils |
| **Raisonnement** | Simple | Simple | Multi-√©tapes |
| **Acc√®s donn√©es** | Non | ChromaDB | ChromaDB + APIs |
| **Adaptation** | Non | Non | Contextuelle |

### Recommandations d'Utilisation

#### Architecture 1
- ‚úÖ POC rapide
- ‚úÖ Ressources limit√©es
- ‚úÖ Requ√™tes simples et r√©p√©titives
- ‚ùå Besoins de pr√©cision critique
- ‚ùå Informations √©volutives

#### Architecture 2
- ‚úÖ Balance performance/complexit√©
- ‚úÖ Besoin de tra√ßabilit√©
- ‚úÖ Base de connaissances √©volutive
- ‚úÖ R√©duction hallucinations
- ‚ùå Requ√™tes complexes multi-√©tapes

#### Architecture 3
- ‚úÖ Service client complet
- ‚úÖ Requ√™tes complexes
- ‚úÖ Acc√®s donn√©es op√©rationnelles
- ‚úÖ Adaptation contextuelle
- ‚úÖ Automatisation maximale
- ‚ö†Ô∏è N√©cessite infrastructure robuste

## üéì M√©thodologie d'√âvaluation

### M√©triques Techniques

- **Latence** : Temps de r√©ponse (ms)
- **Throughput** : Requ√™tes/seconde
- **M√©moire** : Utilisation RAM/VRAM

### M√©triques Qualit√©

- **Pertinence** : Score de pertinence des r√©ponses
- **Factualit√©** : Taux d'hallucinations
- **Compl√©tude** : Couverture des informations
- **Tra√ßabilit√©** : Capacit√© √† citer sources

### M√©triques M√©tier

- **R√©solution** : Taux de r√©solution au premier contact
- **Satisfaction** : Score de satisfaction client
- **Escalade** : Taux de transfert vers agent humain
- **Temps** : Temps moyen de r√©solution

## üìù Format des Identifiants

### EasyTransfert
- **Format** : `EFB.XXXXXXXXX`
- **Exemple** : `EFB.ABC123456`

### Par Op√©rateur
- **MTN** : Chiffres uniquement (10 chiffres)
- **Orange** : `MP` + 10 chiffres
- **Moov** : `MRCH*` ou `CF*` + alphanum√©riques
- **Wave** : Variable, souvent `T` + chiffres
- **Tr√©sor Money** : Format variable

## üîß Configuration

### Variables d'Environnement

Cr√©er un fichier `.env` :

```env
# Mod√®le LLM
MODEL_NAME=meta-llama/Llama-3.2-3B-Instruct
MAX_SEQ_LENGTH=2048
LOAD_IN_4BIT=true

# ChromaDB
CHROMA_PATH=./chromadb_easytransfert
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-mpnet-base-v2

# RAG
CHUNK_SIZE=512
CHUNK_OVERLAP=50
TOP_K=3
MIN_SIMILARITY_SCORE=0.5

# Agent
MAX_ITERATIONS=5
AGENT_VERBOSE=true

# Monitoring
WANDB_PROJECT=easytransfert-architectures
WANDB_ENTITY=your-entity
```

## üìû Support

Pour toute question ou probl√®me :

- **Issues GitHub** : [github.com/AmedBah/memoire/issues](https://github.com/AmedBah/memoire/issues)
- **Documentation** : Voir `doc.txt.txt` pour d√©tails complets
- **Contact EasyTransfert** : 2522018730 (WhatsApp 24h/24)

## üìÑ Licence

Ce projet est d√©velopp√© dans le cadre d'un m√©moire de recherche pour KAYBIC AFRICA / EasyTransfert.

## üôè Remerciements

- KAYBIC AFRICA et l'√©quipe EasyTransfert
- Unsloth pour les outils d'optimisation
- LangChain pour le framework agentique
- Hugging Face pour les mod√®les et outils

---

**Auteur** : Amed Bah  
**Organisation** : KAYBIC AFRICA  
**Projet** : Syst√®me Conversationnel Intelligent EasyTransfert  
**Date** : 2024-2025
