# BIBLIOGRAPHIE

## Large Language Models et Transformers

[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). **Attention is all you need**. *Advances in Neural Information Processing Systems*, 30, 5998-6008.

[2] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). **BERT: Pre-training of deep bidirectional transformers for language understanding**. *Proceedings of NAACL-HLT 2019*, 4171-4186.

[3] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). **Language models are few-shot learners**. *Advances in Neural Information Processing Systems*, 33, 1877-1901.

[4] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., ... & Lample, G. (2023). **Llama 2: Open foundation and fine-tuned chat models**. *arXiv preprint arXiv:2307.09288*.

[5] Meta AI (2024). **Llama 3.2: Lightweight open language models**. Technical Report, Meta AI Research.

## Fine-tuning et Adaptation Efficiente

[6] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). **LoRA: Low-rank adaptation of large language models**. *International Conference on Learning Representations (ICLR) 2022*.

[7] Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). **QLoRA: Efficient finetuning of quantized LLMs**. *Advances in Neural Information Processing Systems*, 36.

[8] Li, X. L., & Liang, P. (2021). **Prefix-tuning: Optimizing continuous prompts for generation**. *Proceedings of ACL-IJCNLP 2021*, 4582-4597.

[9] Lester, B., Al-Rfou, R., & Constant, N. (2021). **The power of scale for parameter-efficient prompt tuning**. *Proceedings of EMNLP 2021*, 3045-3059.

## NLP et Deep Learning pour le français

[10] Martin, L., Muller, B., Suárez, P. J. O., Dupont, Y., Romary, L., de la Clergerie, É. V., ... & Sagot, B. (2019). **CamemBERT: a tasty French language model**. *Proceedings of ACL 2020*, 7203-7219.

[11] Le, H., Vial, L., Frej, J., Segonne, V., Coavoux, M., Lecouteux, B., ... & Schwab, D. (2019). **FlauBERT: Unsupervised language model pre-training for French**. *Proceedings of LREC 2020*, 2479-2490.

[12] Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., ... & Stoyanov, V. (2019). **Unsupervised cross-lingual representation learning at scale**. *Proceedings of ACL 2020*, 8440-8451.

## Architectures Neuronales pour le NLP

[13] Hochreiter, S., & Schmidhuber, J. (1997). **Long short-term memory**. *Neural Computation*, 9(8), 1735-1780.

[14] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014). **Learning phrase representations using RNN encoder-decoder for statistical machine translation**. *Proceedings of EMNLP 2014*, 1724-1734.

[15] Bahdanau, D., Cho, K., & Bengio, Y. (2015). **Neural machine translation by jointly learning to align and translate**. *International Conference on Learning Representations (ICLR) 2015*.

[16] Liu, B., & Lane, I. (2016). **Attention-based recurrent neural network models for joint intent detection and slot filling**. *Proceedings of Interspeech 2016*, 685-689.

## Named Entity Recognition

[17] Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K., & Dyer, C. (2016). **Neural architectures for named entity recognition**. *Proceedings of NAACL-HLT 2016*, 260-270.

[18] Ma, X., & Hovy, E. (2016). **End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF**. *Proceedings of ACL 2016*, 1064-1074.

[19] Lafferty, J., McCallum, A., & Pereira, F. C. (2001). **Conditional random fields: Probabilistic models for segmenting and labeling sequence data**. *Proceedings of ICML 2001*, 282-289.

## Systèmes Conversationnels et Chatbots

[20] Serban, I. V., Sordoni, A., Bengio, Y., Courville, A., & Pineau, J. (2016). **Building end-to-end dialogue systems using generative hierarchical neural network models**. *Proceedings of AAAI 2016*, 3776-3784.

[21] Bordes, A., Boureau, Y. L., & Weston, J. (2017). **Learning end-to-end goal-oriented dialog**. *International Conference on Learning Representations (ICLR) 2017*.

[22] Budzianowski, P., Wen, T. H., Tseng, B. H., Casanueva, I., Ultes, S., Ramadan, O., & Gašić, M. (2018). **MultiWOZ-A large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling**. *Proceedings of EMNLP 2018*, 5016-5026.

[23] Zhang, Y., Sun, S., Galley, M., Chen, Y. C., Brockett, C., Gao, X., ... & Dolan, B. (2020). **DIALOGPT: Large-scale generative pre-training for conversational response generation**. *Proceedings of ACL 2020*, 270-278.

## Service Client et Applications Fintech

[24] Xu, A., Liu, Z., Guo, Y., Sinha, V., & Akkiraju, R. (2017). **A new chatbot for customer service on social media**. *Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems*, 3506-3510.

[25] Følstad, A., & Brandtzæg, P. B. (2017). **Chatbots and the new world of HCI**. *interactions*, 24(4), 38-42.

[26] Adamopoulou, E., & Moussiades, L. (2020). **Chatbots: History, technology, and applications**. *Machine Learning with Applications*, 2, 100006.

[27] Zumstein, D., & Hundertmark, S. (2017). **Chatbots - An interactive technology for personalized communication, transactions and services**. *IADIS International Journal on WWW/Internet*, 15(1), 96-109.

## Mobile Money et Inclusion Financière en Afrique

[28] GSMA (2023). **The State of Mobile Internet Connectivity 2023**. GSMA Intelligence Report.

[29] Suri, T., & Jack, W. (2016). **The long-run poverty and gender impacts of mobile money**. *Science*, 354(6317), 1288-1292.

[30] Ouma, S. A., Odongo, T. M., & Were, M. (2017). **Mobile financial services and financial inclusion: Is it a boon for savings mobilization?** *Review of Development Finance*, 7(1), 29-35.

## Métriques d'Évaluation NLP

[31] Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002). **BLEU: a method for automatic evaluation of machine translation**. *Proceedings of ACL 2002*, 311-318.

[32] Lin, C. Y. (2004). **ROUGE: A package for automatic evaluation of summaries**. *Text Summarization Branches Out: Proceedings of the ACL-04 Workshop*, 74-81.

[33] Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. (2019). **BERTScore: Evaluating text generation with BERT**. *International Conference on Learning Representations (ICLR) 2020*.

## Protection des Données et RGPD

[34] Règlement (UE) 2016/679 du Parlement européen et du Conseil du 27 avril 2016 relatif à la protection des personnes physiques à l'égard du traitement des données à caractère personnel et à la libre circulation de ces données (RGPD).

[35] CNIL (2018). **Guide pratique de la pseudonymisation**. Commission Nationale de l'Informatique et des Libertés.

## Contexte Linguistique et Culturel Ivoirien

[36] Aboa, A. L. (2011). **Le nouchi, identité linguistique de la jeunesse abidjanaise**. *Sudlangues*, 16, 88-98.

[37] Boutin, B. A., & Kouadio N'Guessan, J. (2015). **Le français en Côte d'Ivoire : de l'imposition à l'appropriation décomplexée d'une langue exogène**. *Cahiers internationaux de sociolinguistique*, 8(2), 179-199.

[38] Lafage, S. (2002). **Le lexique français de Côte d'Ivoire : appropriation et créativité**. *Le français en Afrique*, 16-17, 399-420.

## Intelligence Artificielle et Éthique

[39] Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). **On the dangers of stochastic parrots: Can language models be too big?** *Proceedings of FAccT 2021*, 610-623.

[40] Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). **On the opportunities and risks of foundation models**. *arXiv preprint arXiv:2108.07258*.

[41] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022). **Training language models to follow instructions with human feedback**. *Advances in Neural Information Processing Systems*, 35, 27730-27744.

## Rapports et Documents Techniques

[42] ARTCI (2023). **Rapport annuel 2023 - Autorité de Régulation des Télécommunications de Côte d'Ivoire**.

[43] Juniper Research (2023). **Chatbots: Banking, eCommerce, Retail & Healthcare 2022-2027**. Market Research Report.

[44] McKinsey & Company (2023). **The economic potential of generative AI: The next productivity frontier**. McKinsey Global Institute Report.

[45] OpenAI (2023). **GPT-4 Technical Report**. *arXiv preprint arXiv:2303.08774*.

---

## Format de citation dans le texte

Les références sont citées dans le texte selon le format IEEE :

- Citation simple : [1]
- Citations multiples : [1], [2], [3] ou [1-3]
- Citation avec auteurs : Vaswani et al. (2017) [1] ont introduit...

**Exemple** :
"L'architecture Transformer [1] a révolutionné le traitement du langage naturel en remplaçant les mécanismes récurrents par l'attention. Cette approche a été adoptée par de nombreux modèles ultérieurs [2-5]."

---

## Notes

- Toutes les URLs et DOI sont à jour au moment de la rédaction (Octobre 2024)
- Les références académiques suivent le format ACL/IEEE
- Les rapports techniques et documents institutionnels sont inclus pour le contexte business
- Les sources sur le contexte ivoirien apportent la pertinence locale
- Les références RGPD assurent la conformité légale du travail

---

*Cette bibliographie doit être complétée et mise à jour selon les besoins spécifiques du mémoire final.*
