# M√âMOIRE COMPLET - PARTIE 2

## Suite du Chapitre I et Chapitres II-III

---

# CHAPITRE II : √âTAT DE L'ART

Ce chapitre pr√©sente une synth√®se de l'√©tat de l'art scientifique et technologique dans les domaines de l'intelligence artificielle conversationnelle, du traitement automatique du langage naturel (NLP) et de leurs applications au service client. Nous explorons l'√©volution des syst√®mes conversationnels, les avanc√©es r√©centes en mati√®re de Large Language Models et de Deep Learning, puis nous analysons les applications concr√®tes dans le domaine du support client automatis√©.

## I. Intelligence Artificielle Conversationnelle

### 1. √âvolution des syst√®mes conversationnels

**Premi√®re g√©n√©ration : Syst√®mes √† base de r√®gles (1960-1990)**

Les premiers syst√®mes conversationnels, illustr√©s par ELIZA (Weizenbaum, 1966) et PARRY (Colby, 1972), reposaient sur des r√®gles manuelles et des patterns de reconnaissance. Ces syst√®mes utilisaient des techniques de pattern matching pour identifier des mots-cl√©s dans l'entr√©e utilisateur et g√©n√©rer des r√©ponses pr√©programm√©es.

Caract√©ristiques :
- Vocabulaire limit√© (50-200 patterns)
- Pas de v√©ritable compr√©hension du langage
- Dialogue rigide et pr√©visible
- Domaine d'application tr√®s restreint

Limitations : Incapacit√© √† g√©rer les variations linguistiques, absence d'apprentissage, scalabilit√© limit√©e.

**Deuxi√®me g√©n√©ration : Approches statistiques et Machine Learning (1990-2015)**

L'av√®nement des m√©thodes statistiques et du machine learning a permis de d√©velopper des syst√®mes plus robustes. Cette p√©riode a vu l'√©mergence de :

- **SVM et Naive Bayes** pour la classification d'intentions (Joachims, 1998)
- **Hidden Markov Models (HMM)** pour la mod√©lisation du dialogue (Rabiner, 1989)
- **Conditional Random Fields (CRF)** pour l'extraction d'entit√©s (Lafferty et al., 2001)
- **TF-IDF et Topic Models** pour la r√©cup√©ration d'information (Blei et al., 2003)

Avanc√©es cl√©s :
- Apprentissage automatique √† partir de donn√©es
- Mod√©lisation probabiliste du langage
- Capacit√© de g√©n√©ralisation sur des donn√©es non vues
- M√©thodes d'√©valuation quantitatives

Limitations : N√©cessit√© de feature engineering manuel, compr√©hension contextuelle limit√©e, difficult√© √† capturer les d√©pendances long-terme.

**Troisi√®me g√©n√©ration : Deep Learning et repr√©sentations distribu√©es (2015-2020)**

L'introduction des r√©seaux de neurones profonds a r√©volutionn√© le NLP :

- **Word2Vec et GloVe** (Mikolov et al., 2013; Pennington et al., 2014) : Embeddings de mots capturant les relations s√©mantiques
- **Seq2Seq avec attention** (Bahdanau et al., 2015) : G√©n√©ration de s√©quences contextuelles
- **RNN, LSTM, GRU** (Hochreiter & Schmidhuber, 1997) : Mod√©lisation des d√©pendances temporelles
- **CNN pour NLP** (Kim, 2014) : Extraction de features locales dans le texte

Applications conversationnelles :
- Chatbots de questions-r√©ponses plus robustes
- Traduction automatique neuronale
- Syst√®mes de dialogue orient√©s t√¢che
- G√©n√©ration de texte conditionn√©

Avanc√©es : √âlimination du feature engineering, apprentissage de repr√©sentations end-to-end, meilleure gestion du contexte.

Limitations persistantes : N√©cessit√© de grandes quantit√©s de donn√©es d'entra√Ænement, co√ªt computationnel √©lev√©, difficult√© de transfert cross-domain.

**Quatri√®me g√©n√©ration : Transformers et mod√®les pr√©-entra√Æn√©s (2018-pr√©sent)**

L'architecture Transformer (Vaswani et al., 2017) et les mod√®les pr√©-entra√Æn√©s ont marqu√© un tournant majeur :

- **BERT** (Devlin et al., 2019) : Pr√©-entra√Ænement bidirectionnel
- **GPT** s√©rie (OpenAI, 2018-2024) : Mod√®les autor√©gressifs √† grande √©chelle
- **T5, BART** (Raffel et al., 2020; Lewis et al., 2020) : Mod√®les encodeur-d√©codeur unifi√©s
- **Llama** s√©rie (Meta, 2023-2024) : LLM open-source performants

Paradigme du pr√©-entra√Ænement + fine-tuning :
1. **Pr√©-entra√Ænement** sur de vastes corpus (Common Crawl, Wikipedia, livres)
2. **Fine-tuning** sur des t√¢ches sp√©cifiques avec moins de donn√©es
3. **Prompting** et **in-context learning** sans fine-tuning

Capacit√©s √©mergentes :
- Compr√©hension du contexte long (jusqu'√† 128k tokens)
- Raisonnement multi-√©tapes (Chain-of-Thought)
- G√©n√©ralisation zero-shot et few-shot
- Multilinguisme natif

Ces avanc√©es rendent aujourd'hui possible des syst√®mes conversationnels hautement performants n√©cessitant moins de donn√©es d'entra√Ænement sp√©cifiques.

### 2. Large Language Models (LLM)

**D√©finition et caract√©ristiques**

Les Large Language Models sont des mod√®les de r√©seaux de neurones entra√Æn√©s sur de vastes corpus textuels (plusieurs t√©raoctets) pour pr√©dire le prochain token d'une s√©quence. Leur taille se mesure en nombre de param√®tres, allant de quelques milliards √† plusieurs centaines de milliards.

Caract√©ristiques cl√©s :
- **√âchelle massive** : Milliards de param√®tres (GPT-4 : ~1.7T, Llama 3.1 70B : 70B)
- **Architecture Transformer** : M√©canismes d'attention multi-t√™tes
- **Pr√©-entra√Ænement non supervis√©** : Objectif de mod√©lisation du langage
- **Tokenisation subword** : BPE, WordPiece, SentencePiece
- **Contextual embeddings** : Repr√©sentations d√©pendant du contexte

**Mod√®les de r√©f√©rence (2024)**

**GPT-4 (OpenAI)** :
- Mod√®le propri√©taire multimodal (texte + images)
- Environ 1.7 trillion de param√®tres
- Contexte jusqu'√† 128k tokens
- Performances SOTA sur de nombreux benchmarks
- Co√ªt : $0.03/1k tokens (input), $0.06/1k tokens (output)
- Limitation : API seulement, pas d'auto-h√©bergement

**Claude 3 (Anthropic)** :
- Famille de mod√®les (Opus, Sonnet, Haiku)
- Contexte jusqu'√† 200k tokens
- Focus sur la s√©curit√© et l'alignement
- Performances comparables √† GPT-4
- Co√ªt similaire √† GPT-4

**Llama 3.1 (Meta)** :
- Open-source sous licence permissive
- Variantes : 8B, 70B, 405B param√®tres
- Multilingue (10+ langues dont fran√ßais)
- Auto-h√©bergement possible
- Performance : 70B comparable √† GPT-3.5
- Avantage : Contr√¥le complet, personnalisation

**Mistral 7B et Mixtral 8x7B (Mistral AI)** :
- Mod√®les europ√©ens open-source
- Excellent rapport performance/taille
- Mistral 7B : Performant pour sa taille compacte
- Mixtral 8x7B : Architecture Mixture of Experts
- Optimis√© pour le fran√ßais

**Capacit√©s des LLM pour le service client**

1. **Compr√©hension en langage naturel** :
   - Parsing de requ√™tes complexes et ambigu√´s
   - Gestion du code-switching et erreurs orthographiques
   - Extraction implicite d'intentions et entit√©s

2. **G√©n√©ration de r√©ponses contextuelles** :
   - R√©ponses coh√©rentes et fluides en fran√ßais
   - Adaptation du ton et du style
   - Personnalisation selon le profil utilisateur

3. **Raisonnement et r√©solution de probl√®mes** :
   - Chain-of-Thought reasoning
   - D√©composition de probl√®mes complexes
   - Acc√®s √† des bases de connaissances (via RAG)

4. **Apprentissage contextuel (In-Context Learning)** :
   - Few-shot learning : exemples dans le prompt
   - Adaptation rapide sans r√©entra√Ænement
   - Personnalisation par utilisateur/session

**Limites et d√©fis des LLM**

1. **Hallucinations** :
   - G√©n√©ration de faits incorrects avec grande confiance
   - Particuli√®rement probl√©matique pour le service client
   - Mitigation : RAG, calibration, fact-checking

2. **Co√ªt computationnel** :
   - Inf√©rence co√ªteuse (GPU n√©cessaire)
   - Latence proportionnelle √† la longueur de g√©n√©ration
   - Co√ªt par requ√™te significatif en production

3. **Contr√¥le et pr√©visibilit√©** :
   - Comportement parfois impr√©visible
   - Difficult√© de garantir des contraintes strictes
   - N√©cessit√© de guardrails et validation

4. **Biais et s√©curit√©** :
   - Reproduction de biais du corpus d'entra√Ænement
   - Potentiel de g√©n√©rer du contenu inappropri√©
   - N√©cessit√© de mod√©ration et filtrage

5. **D√©pendance aux donn√©es** :
   - Knowledge cutoff (connaissances fig√©es √† la date d'entra√Ænement)
   - N√©cessit√© de mise √† jour via RAG ou fine-tuning
   - Gestion des informations √©volutives

### 3. Architectures d'agents intelligents

**Concept d'agent conversationnel**

Un agent conversationnel intelligent est un syst√®me qui :
- **Per√ßoit** son environnement (messages utilisateurs, contexte)
- **Raisonne** sur les actions √† entreprendre
- **Agit** en g√©n√©rant des r√©ponses ou en d√©clenchant des actions
- **Apprend** de ses interactions pour s'am√©liorer

**Architecture ReAct (Reason + Act)**

Propos√©e par Yao et al. (2023), l'architecture ReAct combine raisonnement et action dans un cycle it√©ratif :

```
Cycle ReAct :
1. Thought (Pens√©e) : L'agent analyse la situation
2. Action : L'agent choisit un outil et l'ex√©cute
3. Observation : L'agent examine le r√©sultat
4. R√©p√®te jusqu'√† avoir assez d'informations
5. Final Answer : L'agent g√©n√®re la r√©ponse finale
```

Avantages :
- Transparence du raisonnement (traces explicites)
- Capacit√© √† utiliser des outils externes
- D√©composition de probl√®mes complexes
- Gestion d'erreurs et r√©cup√©ration

Application au service client :
- Outil 1 : Recherche dans la base de connaissances (FAQ, proc√©dures)
- Outil 2 : Requ√™te √† l'API de transactions (v√©rification de statut)
- Outil 3 : Extraction d'entit√©s (ID transaction, num√©ros, montants)
- Outil 4 : Gestion de m√©moire conversationnelle (historique)

**Architecture RAG (Retrieval-Augmented Generation)**

Le RAG (Lewis et al., 2020) augmente la g√©n√©ration avec une r√©cup√©ration d'information :

```
Pipeline RAG :
1. Requ√™te utilisateur
2. Vectorisation de la requ√™te
3. Recherche de similarit√© dans la base vectorielle
4. R√©cup√©ration des k documents les plus pertinents
5. Enrichissement du prompt avec les documents
6. G√©n√©ration de la r√©ponse par le LLM
```

Avantages :
- R√©duction des hallucinations
- Tra√ßabilit√© (citations de sources)
- Mise √† jour facile de la base de connaissances
- Pas besoin de r√©entra√Æner le mod√®le

Composants techniques :
- **Embedding model** : paraphrase-multilingual-mpnet-base-v2 (768 dim)
- **Vector DB** : ChromaDB, Pinecone, Weaviate
- **Chunking strategy** : D√©coupage intelligent du texte (512-1024 tokens)
- **Retrieval** : Similarit√© cosinus, score de r√©ranking

**Architecture hybride : RAG + ReAct**

La combinaison RAG + ReAct offre le meilleur des deux mondes :
- RAG fournit les connaissances factuelles
- ReAct permet le raisonnement multi-√©tapes et l'utilisation d'outils

Cette architecture est particuli√®rement adapt√©e au service client o√π il faut :
- Acc√©der √† de la documentation √† jour (RAG)
- Interroger des APIs externes (ReAct)
- Raisonner sur des probl√®mes complexes (ReAct)
- Maintenir le contexte conversationnel (ReAct)

## II. Techniques de Deep Learning pour le NLP

### 1. R√©seaux de neurones r√©currents (RNN, LSTM, GRU)

**R√©seaux de Neurones R√©currents (RNN)**

Les RNN, introduits par Rumelhart et al. (1986), sont con√ßus pour traiter des s√©quences de longueur variable en maintenant un √©tat cach√© :

```
√âquations RNN :
h_t = tanh(W_hh * h_(t-1) + W_xh * x_t + b_h)
y_t = W_hy * h_t + b_y
```

O√π :
- h_t : √©tat cach√© au temps t
- x_t : entr√©e au temps t
- y_t : sortie au temps t
- W : matrices de poids
- b : vecteurs de biais

Avantages :
- Traitement de s√©quences de longueur variable
- Partage de param√®tres dans le temps
- M√©moire des entr√©es pr√©c√©dentes

Limitations :
- **Vanishing gradient** : Difficult√© d'apprentissage sur longues s√©quences
- **Biais r√©cent** : Oubli des informations lointaines
- **Parall√©lisation limit√©e** : Calcul s√©quentiel obligatoire

**Long Short-Term Memory (LSTM)**

Les LSTM (Hochreiter & Schmidhuber, 1997) r√©solvent le probl√®me du vanishing gradient avec des portes (gates) :

```
√âquations LSTM :
f_t = œÉ(W_f * [h_(t-1), x_t] + b_f)   # Forget gate
i_t = œÉ(W_i * [h_(t-1), x_t] + b_i)   # Input gate
CÃÉ_t = tanh(W_C * [h_(t-1), x_t] + b_C) # Candidate cell state
C_t = f_t ‚äô C_(t-1) + i_t ‚äô CÃÉ_t       # Cell state update
o_t = œÉ(W_o * [h_(t-1), x_t] + b_o)   # Output gate
h_t = o_t ‚äô tanh(C_t)                 # Hidden state
```

M√©canismes cl√©s :
- **Cell state** : M√©moire long-terme
- **Forget gate** : D√©cide quoi oublier
- **Input gate** : D√©cide quoi ajouter
- **Output gate** : D√©cide quoi exposer

Applications NLP :
- Mod√©lisation du langage
- Traduction automatique (seq2seq)
- Classification de texte
- Extraction d'entit√©s nomm√©es

**Gated Recurrent Unit (GRU)**

Les GRU (Cho et al., 2014) simplifient les LSTM avec moins de param√®tres :

```
√âquations GRU :
r_t = œÉ(W_r * [h_(t-1), x_t])      # Reset gate
z_t = œÉ(W_z * [h_(t-1), x_t])      # Update gate
hÃÉ_t = tanh(W * [r_t ‚äô h_(t-1), x_t]) # Candidate hidden state
h_t = (1 - z_t) ‚äô h_(t-1) + z_t ‚äô hÃÉ_t # Final hidden state
```

Avantages vs LSTM :
- Moins de param√®tres (plus rapide √† entra√Æner)
- Performance comparable sur beaucoup de t√¢ches
- Plus simple √† impl√©menter et debugger

**Applications au service client**

Pour un syst√®me de service client, les RNN/LSTM/GRU sont utilis√©s pour :

1. **Classification d'intentions** :
   - BiLSTM pour capturer le contexte bidirectionnel
   - Softmax final sur les classes d'intentions
   - Exemple : "probl√®me de transfert" ‚Üí classe: PROBLEME_TRANSACTION

2. **Extraction d'entit√©s** :
   - BiLSTM-CRF pour le NER
   - Identification de : num√©ros de t√©l√©phone, montants, op√©rateurs, IDs
   - Tagging BIO (Begin, Inside, Outside)

3. **G√©n√©ration de r√©ponses** :
   - Encoder-decoder avec LSTM
   - Attention mechanism pour focaliser sur les parties pertinentes
   - G√©n√©ration token par token

### 2. M√©canismes d'attention et Transformers

**M√©canisme d'attention**

L'attention (Bahdanau et al., 2015) permet au mod√®le de se focaliser sur les parties pertinentes de l'entr√©e :

```
Attention score :
e_ij = a(s_i, h_j)  # score entre √©tat d√©codeur i et encodeur j
Œ±_ij = exp(e_ij) / Œ£_k exp(e_ik)  # Softmax normalization
c_i = Œ£_j Œ±_ij * h_j  # Context vector (somme pond√©r√©e)
```

Types d'attention :
- **Additive (Bahdanau)** : a(s, h) = v^T * tanh(W_s * s + W_h * h)
- **Multiplicative (Luong)** : a(s, h) = s^T * W * h
- **Scaled dot-product** : a(Q, K) = softmax(Q*K^T / ‚àöd_k)

Avantages :
- Gestion des d√©pendances longue distance
- Interpr√©tabilit√© (visualisation des poids d'attention)
- Performance am√©lior√©e sur seq2seq

**Architecture Transformer**

Les Transformers (Vaswani et al., 2017) reposent enti√®rement sur l'attention, √©liminant la r√©currence :

Composants cl√©s :

1. **Multi-Head Self-Attention** :
```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W^O
o√π head_i = Attention(Q*W_i^Q, K*W_i^K, V*W_i^V)
```

Permet au mod√®le d'attendre √† diff√©rentes positions et sous-espaces.

2. **Position-wise Feed-Forward** :
```
FFN(x) = max(0, x*W_1 + b_1)*W_2 + b_2
```

Appliqu√© ind√©pendamment √† chaque position.

3. **Positional Encoding** :
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

Injecte l'information de position (ordre des tokens).

4. **Layer Normalization et Residual Connections** :
```
Output = LayerNorm(x + Sublayer(x))
```

Stabilise l'entra√Ænement et permet des r√©seaux profonds.

**Architecture compl√®te** :

```
Encoder (N layers) :
  - Multi-Head Self-Attention
  - Add & Norm
  - Feed-Forward
  - Add & Norm

Decoder (N layers) :
  - Masked Multi-Head Self-Attention
  - Add & Norm
  - Multi-Head Cross-Attention (sur l'encodeur)
  - Add & Norm
  - Feed-Forward
  - Add & Norm
```

Avantages des Transformers :
- **Parall√©lisation** : Tous les tokens trait√©s simultan√©ment
- **Longue port√©e** : Attention directe entre tous les tokens
- **Scalabilit√©** : Performance croissante avec la taille
- **Transfert** : Pr√©-entra√Ænement + fine-tuning efficace

Variantes architecturales :

- **Encoder-only** (BERT) : Bidirectionnel, pour t√¢ches de compr√©hension
- **Decoder-only** (GPT) : Autor√©gressif, pour g√©n√©ration
- **Encoder-Decoder** (T5, BART) : Complet, pour seq2seq

### 3. Mod√®les pr√©-entra√Æn√©s et Transfer Learning

**Paradigme du Transfer Learning en NLP**

Le transfer learning en NLP se d√©roule en deux phases :

**Phase 1 : Pr√©-entra√Ænement (Pretraining)**

Objectifs non supervis√©s sur large corpus :
- **Masked Language Modeling (MLM)** : BERT
  - Masquer 15% des tokens
  - Pr√©dire les tokens masqu√©s
  - Apprentissage bidirectionnel

- **Causal Language Modeling (CLM)** : GPT
  - Pr√©dire le prochain token
  - Apprentissage autor√©gressif gauche-√†-droite

- **Sequence-to-Sequence** : T5
  - Diverses t√¢ches de transformation de texte
  - Format unifi√© input ‚Üí output

Corpus typiques :
- Common Crawl (plusieurs t√©raoctets web)
- Wikipedia (haute qualit√©, multilingue)
- Livres, articles scientifiques
- Code source (pour mod√®les de code)

R√©sultat : Mod√®le avec repr√©sentations linguistiques g√©n√©rales

**Phase 2 : Fine-tuning (Ajustement fin)**

Adaptation sur t√¢che sp√©cifique avec donn√©es annot√©es :
- Classification : Ajouter une couche softmax finale
- NER : Ajouter une couche CRF
- G√©n√©ration : Ajuster le decoder

Strat√©gies de fine-tuning :

1. **Full fine-tuning** :
   - Tous les param√®tres mis √† jour
   - Meilleure performance mais co√ªteux
   - Risque d'overfitting si peu de donn√©es

2. **Feature extraction** :
   - Poids gel√©s, entra√Ænement seulement des nouvelles couches
   - Rapide mais performance limit√©e

3. **Parameter-Efficient Fine-Tuning (PEFT)** :
   - **LoRA** (Low-Rank Adaptation) : D√©composition matricielle rang faible
   - **Prefix Tuning** : Apprentissage de pr√©fixes virtuels
   - **Adapter Layers** : Petits modules ins√©r√©s entre couches
   - Avantages : R√©duit param√®tres √† entra√Æner de 99%, garde performance

**LoRA en d√©tail**

LoRA (Hu et al., 2021) d√©compose les mises √† jour de poids :

```
W_new = W_0 + ŒîW
o√π ŒîW = B * A (d√©composition rang faible)
```

- W_0 : Poids pr√©-entra√Æn√©s (gel√©s)
- B ‚àà R^(d √ó r), A ‚àà R^(r √ó k) : Matrices de rang r << min(d,k)
- r : Rang (typiquement 8-64)

Param√®tres √† entra√Æner : 2*r*(d+k) au lieu de d*k

Avantages :
- R√©duction massive des param√®tres (0.1-1% de l'original)
- Entra√Ænement plus rapide (moins de m√©moire GPU)
- Plusieurs adaptateurs LoRA pour diff√©rentes t√¢ches
- Pas de latence d'inf√©rence suppl√©mentaire

Application au service client :
- Pr√©-entra√Ænement : Llama 3.2 3B sur corpus g√©n√©ral
- Fine-tuning LoRA : Conversations EasyTransfert (3031 exemples)
- R√©sultat : Mod√®le sp√©cialis√© avec seulement ~50 MB de param√®tres additionnels

**Mod√®les multilingues pour le fran√ßais**

Mod√®les pr√©-entra√Æn√©s pertinents pour notre contexte :

1. **CamemBERT** (Martin et al., 2020) :
   - BERT entra√Æn√© sur fran√ßais (OSCAR corpus)
   - 110M ou 335M param√®tres
   - Excellent pour t√¢ches de compr√©hension

2. **mBERT** (Devlin et al., 2019) :
   - BERT multilingue (104 langues)
   - Bon pour code-switching
   - Performance fran√ßaise l√©g√®rement inf√©rieure √† CamemBERT

3. **XLM-RoBERTa** (Conneau et al., 2020) :
   - 100 langues, 270M ou 550M param√®tres
   - Excellent cross-lingual
   - Robuste au code-switching

4. **Llama 3.1 (8B, 70B)** (Meta, 2024) :
   - Multilingue incluant fran√ßais
   - G√©n√©ration et compr√©hension
   - Open-source, fine-tunable

5. **Mistral 7B** (Mistral AI, 2023) :
   - Fran√ßais renforc√©
   - Excellent rapport qualit√©/taille
   - Open-source europ√©en

## III. Applications dans le service client

### 1. Chatbots et assistants virtuels

**Taxonomie des chatbots**

Les chatbots peuvent √™tre class√©s selon plusieurs dimensions :

**Par complexit√©** :
1. **Rule-based** : Arbre de d√©cision fixe
2. **Retrieval-based** : S√©lection dans une base de r√©ponses
3. **Generative** : G√©n√©ration de nouvelles r√©ponses

**Par domaine** :
1. **Open-domain** : Conversation g√©n√©rale (difficile)
2. **Closed-domain** : Domaine sp√©cifique (service client)

**Par architecture** :
1. **Intent-based** : Classification puis r√©ponse template
2. **QA-based** : Question-answering sur documents
3. **Dialogue management** : Gestion d'√©tat et de politique
4. **End-to-end neural** : Entr√©e ‚Üí R√©ponse directe

**Architectures modernes pour le service client**

**Architecture 1 : Pipeline intent + NER + Response**

```
1. Intent Classification :
   User: "Mon transfert n'est pas arriv√©"
   ‚Üí Intent: PROBLEME_TRANSACTION (confidence: 0.95)

2. Entity Extraction :
   ‚Üí Entities: {
       "probleme_type": "non_arriv√©",
       "sujet": "transfert"
     }

3. Dialogue State Tracking :
   ‚Üí State: MISSING_INFO
   ‚Üí Slots requis: {transaction_id, phone_number, amount}

4. Response Generation :
   Template: "Je comprends que votre transfert n'est pas arriv√©. 
              Pour vous aider, j'ai besoin de votre identifiant de 
              transaction. Pouvez-vous me le fournir ?"
```

Avantages :
- Contr√¥le pr√©cis du comportement
- Debugging facile
- Performance pr√©visible

Inconv√©nients :
- Rigide et peu flexible
- N√©cessite beaucoup de r√®gles manuelles
- Difficult√© √† g√©rer l'impr√©vu

**Architecture 2 : LLM avec prompting structur√©**

```
System Prompt:
"Tu es un assistant du service client EasyTransfert.
 
Contexte: EasyTransfert permet des transferts entre 
MTN, Orange, Moov, Wave, Tr√©sor Money.

R√®gles:
- Toujours demander l'ID de transaction pour les probl√®mes
- Frais: 1-2% du montant (min 25, max 500 FCFA)
- En cas de doute, rediriger vers 2522018730
- Ton empathique avec √©mojis

Exemples:
[few-shot examples...]
"

User: "Mon transfert n'est pas arriv√©"

Response: "Je comprends votre inqui√©tude üòü. Pour vous 
aider efficacement, pourriez-vous me communiquer :
1. L'identifiant de votre transaction
2. Le montant transf√©r√©
3. Les op√©rateurs concern√©s (√©metteur et r√©cepteur)

Ainsi, je pourrai v√©rifier le statut de votre transfert."
```

Avantages :
- Tr√®s flexible et naturel
- G√®re les variations linguistiques
- Adaptation contextuelle automatique

Inconv√©nients :
- Moins de contr√¥le
- Risque d'hallucinations
- Co√ªt d'inf√©rence plus √©lev√©

**Architecture 3 : RAG pour service client**

```
1. User Query: "Quels sont les frais pour un transfert de 
                 50000 FCFA de MTN vers Orange?"

2. Query Vectorization:
   embedding_model(query) ‚Üí vector(768 dim)

3. Retrieval from Knowledge Base:
   ChromaDB.search(vector, top_k=3)
   ‚Üí Retrieved docs:
     - FAQ #4: "Frais de transfert"
     - Proc√©dure: "Calcul des frais"
     - Doc technique: "Grilles tarifaires par op√©rateur"

4. Prompt Augmentation:
   """
   Contexte pertinent:
   [Docs r√©cup√©r√©s...]
   
   Question: {user_query}
   
   R√©ponds en te basant UNIQUEMENT sur le contexte fourni.
   Cite tes sources.
   """

5. LLM Generation:
   "D'apr√®s notre grille tarifaire, pour un transfert de 
    50 000 FCFA de MTN vers Orange, les frais sont de 
    500 FCFA (soit 1% du montant, plafonn√© √† 500 FCFA).
    
    Source: FAQ #4 - Frais de transfert"
```

Avantages :
- R√©ponses factuelles et tra√ßables
- Mise √† jour facile de la base de connaissances
- R√©duit les hallucinations

**Cas d'usage typiques en service client**

1. **Information g√©n√©rale** :
   - Horaires, limites, frais
   - Proc√©dures de base
   - Compatibilit√©s op√©rateurs

2. **Support transactionnel** :
   - Suivi de transaction
   - V√©rification de statut
   - Probl√®mes d'ex√©cution

3. **R√©solution de probl√®mes** :
   - Transaction √©chou√©e
   - Argent non re√ßu
   - Erreurs techniques

4. **Guidance proc√©durale** :
   - Comment faire un transfert
   - Comment s'inscrire
   - Comment r√©cup√©rer un code oubli√©

### 2. Syst√®mes de tickets et de classification

**Classification automatique de tickets**

La classification de tickets permet de router automatiquement les requ√™tes vers les bonnes √©quipes ou workflows.

**Taxonomie des cat√©gories** (EasyTransfert) :

```
1. INFORMATION_GENERALE (30% du volume)
   - info_frais
   - info_limites
   - info_operateurs
   - info_horaires

2. PROBLEME_TRANSACTION (40% du volume)
   - transaction_echouee
   - transaction_incomplete
   - argent_non_recu
   - erreur_montant

3. PROBLEME_TECHNIQUE (15% du volume)
   - probleme_connexion
   - erreur_application
   - bug_interface

4. COMPTE_UTILISATEUR (10% du volume)
   - inscription
   - connexion
   - code_oublie
   - modification_profil

5. RECLAMATION (5% du volume)
   - insatisfaction
   - contestation_frais
   - demande_remboursement
```

**Approches de classification**

**Approche 1 : Classification supervis√©e traditionnelle**

Pipeline :
1. Pr√©traitement : nettoyage, tokenisation, stemming
2. Vectorisation : TF-IDF ou count vectorizer
3. Classification : SVM, Naive Bayes, Random Forest
4. Post-traitement : seuil de confiance, classe par d√©faut

Exemple avec SVM :
```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC

# Vectorisation
vectorizer = TfidfVectorizer(
    max_features=5000,
    ngram_range=(1, 2),
    min_df=2
)
X_train = vectorizer.fit_transform(texts_train)

# Classification
clf = SVC(kernel='linear', probability=True)
clf.fit(X_train, y_train)

# Pr√©diction
X_test = vectorizer.transform(texts_test)
predictions = clf.predict(X_test)
confidences = clf.predict_proba(X_test)
```

Performance typique :
- Accuracy : 0.82-0.88
- F1-score macro : 0.78-0.85
- Avantage : Rapide, peu de donn√©es n√©cessaires
- Limite : Pas de compr√©hension contextuelle

**Approche 2 : Classification avec embeddings + Deep Learning**

Architecture :
```
Input (text)
   ‚Üì
Embedding Layer (pr√©-entra√Æn√© CamemBERT)
   ‚Üì
BiLSTM(256 units)
   ‚Üì
Dropout(0.3)
   ‚Üì
BiLSTM(128 units)
   ‚Üì
Attention Layer
   ‚Üì
Dense(64, ReLU)
   ‚Üì
Dropout(0.3)
   ‚Üì
Dense(num_classes, Softmax)
```

Performance typique :
- Accuracy : 0.90-0.95
- F1-score macro : 0.88-0.93
- Avantage : Meilleure compr√©hension contextuelle
- Limite : Plus de donn√©es n√©cessaires, co√ªt computationnel

**Approche 3 : Zero-shot classification avec LLM**

Prompting :
```
Classifie la requ√™te client suivante dans l'une de ces cat√©gories :
- INFORMATION_GENERALE
- PROBLEME_TRANSACTION
- PROBLEME_TECHNIQUE
- COMPTE_UTILISATEUR
- RECLAMATION

Requ√™te : "{user_query}"

R√©ponds uniquement avec le nom de la cat√©gorie la plus appropri√©e.
Si aucune ne correspond, r√©ponds "AUTRE".

Cat√©gorie :
```

Performance typique :
- Accuracy : 0.85-0.92 (d√©pend du LLM)
- Avantage : Pas besoin de donn√©es d'entra√Ænement
- Limite : Co√ªt d'inf√©rence, latence

### 3. Analyse de sentiment et d√©tection d'intention

**Analyse de sentiment**

L'analyse de sentiment permet de d√©tecter l'√©motion du client (positif, n√©gatif, neutre) pour prioriser et personnaliser les r√©ponses.

**Niveaux de granularit√©** :

1. **Document-level** : Sentiment global du message
2. **Sentence-level** : Sentiment par phrase
3. **Aspect-based** : Sentiment par aspect (ex: "bon service mais frais √©lev√©s")

**Approches pour le fran√ßais** :

**Approche 1 : Lexiques de sentiments**

Utilisation de lexiques annot√©s (FEEL, Polyglot) :
```python
positive_words = {"content", "satisfait", "merci", "parfait", "rapide"}
negative_words = {"probl√®me", "insatisfait", "lent", "erreur", "nul"}

def sentiment_score(text):
    words = text.lower().split()
    pos_count = sum(1 for w in words if w in positive_words)
    neg_count = sum(1 for w in words if w in negative_words)
    
    if pos_count > neg_count:
        return "POSITIF"
    elif neg_count > pos_count:
        return "NEGATIF"
    else:
        return "NEUTRE"
```

Avantages : Simple, rapide, interpr√©table
Limites : Pas de contexte, sensible aux n√©gations

**Approche 2 : Classification avec CamemBERT**

Fine-tuning de CamemBERT sur corpus de sentiments :
```python
from transformers import CamembertForSequenceClassification

model = CamembertForSequenceClassification.from_pretrained(
    'camembert-base',
    num_labels=3  # positif, n√©gatif, neutre
)

# Fine-tuning sur corpus annot√©
# ...

# Pr√©diction
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
sentiment = torch.argmax(outputs.logits, dim=1)
```

Performance :
- Accuracy : 0.88-0.93 sur corpus fran√ßais
- Robuste aux n√©gations et sarcasme

**Application au service client** :

Priorisation des tickets :
- Sentiment NEGATIF + Mots cl√©s urgents ‚Üí Priorit√© HAUTE
- Sentiment POSITIF ‚Üí R√©ponse standard
- Sentiment NEUTRE ‚Üí Priorit√© NORMALE

Personnalisation des r√©ponses :
- NEGATIF : Ton empathique, excuse, solution rapide
- POSITIF : Ton amical, remerciement
- NEUTRE : Ton professionnel et neutre

**D√©tection d'intention**

La d√©tection d'intention identifie ce que l'utilisateur veut accomplir.

**Taxonomie d'intentions** (EasyTransfert) :

```
1. Informatives :
   - demander_frais
   - demander_limites
   - demander_horaires
   - demander_compatibilite

2. Transactionnelles :
   - verifier_statut_transaction
   - suivre_transaction
   - signaler_probleme_transaction

3. Proc√©durales :
   - demander_procedure_transfert
   - demander_procedure_inscription
   - demander_aide_generale

4. Support :
   - signaler_bug
   - contacter_agent
   - demander_remboursement
```

**Multi-intention** : Un message peut contenir plusieurs intentions :
```
"Bonjour, je veux savoir les frais pour un transfert de 
 100000 FCFA et aussi v√©rifier le statut de ma transaction 
 TX12345678"

Intentions d√©tect√©es :
- demander_frais (confiance: 0.92)
- verifier_statut_transaction (confiance: 0.95)
```

**Approche hi√©rarchique** :

```
Niveau 1 (Macro-intention) :
- INFORMATION
- ACTION
- PROBLEME

Niveau 2 (Intention sp√©cifique) :
Si macro = INFORMATION :
  - info_frais, info_limites, info_horaires...
Si macro = ACTION :
  - faire_transfert, consulter_solde...
Si macro = PROBLEME :
  - transaction_echouee, bug_app...
```

Cette approche hi√©rarchique am√©liore la pr√©cision et la robustesse.

---

*[Suite du m√©moire dans le fichier MEMOIRE_COMPLET_PARTIE3.md...]*
