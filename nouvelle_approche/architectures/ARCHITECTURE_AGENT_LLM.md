# Architecture 1 : ModÃ¨le Agent LLM

## Vue d'ensemble

Cette architecture repose sur un Large Language Model (LLM) fine-tunÃ© spÃ©cifiquement pour le service client d'EasyTransfert. Le modÃ¨le combine les capacitÃ©s de comprÃ©hension et de gÃ©nÃ©ration d'un LLM prÃ©-entraÃ®nÃ© avec une adaptation fine via LoRA (Low-Rank Adaptation) sur les donnÃ©es conversationnelles d'EasyTransfert.

## Diagramme d'architecture (Mermaid)

```mermaid
graph TB
    A[RequÃªte Utilisateur] --> B{PrÃ©traitement}
    B --> C[Nettoyage texte]
    B --> D[Anonymisation RGPD]
    B --> E[Normalisation Code-switching]
    
    C --> F[Construction Prompt]
    D --> F
    E --> F
    
    F --> G[System Prompt<br/>EasyTransfert Context]
    G --> H[Tokenisation<br/>Llama 3.2 Tokenizer]
    
    H --> I[Llama 3.2 3B Instruct<br/>+ Adaptateurs LoRA<br/>r=16, alpha=32]
    
    I --> J{StratÃ©gie de GÃ©nÃ©ration}
    J -->|Temperature=0.7| K[Sampling]
    J -->|Top-p=0.9| K
    
    K --> L[DÃ©codage]
    L --> M[Post-traitement]
    M --> N[RÃ©ponse Finale]
    
    style I fill:#f9f,stroke:#333,stroke-width:4px
    style A fill:#bbf,stroke:#333,stroke-width:2px
    style N fill:#bfb,stroke:#333,stroke-width:2px
    style G fill:#ffd,stroke:#333,stroke-width:2px
```

**Flux de traitement**:
1. **PrÃ©traitement** : Nettoyage, anonymisation RGPD, normalisation linguistique
2. **Prompting** : Construction du prompt avec contexte EasyTransfert
3. **Tokenisation** : Conversion en tokens Llama 3.2 (max 2048)
4. **InfÃ©rence** : Passage dans le modÃ¨le avec adaptateurs LoRA
5. **GÃ©nÃ©ration** : Sampling avec temperature 0.7
6. **Post-traitement** : Formatage final de la rÃ©ponse

## Architecture SystÃ¨me (Vue dÃ©taillÃ©e)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARCHITECTURE AGENT LLM                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Input: RequÃªte utilisateur en franÃ§ais (avec code-switching possible)
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MODULE DE PRÃ‰TRAITEMENT            â”‚
â”‚   - Nettoyage du texte               â”‚
â”‚   - Normalisation (accents, casse)   â”‚
â”‚   - Gestion Ã©mojis                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CONSTRUCTION DU PROMPT             â”‚
â”‚   - System prompt (contexte)         â”‚
â”‚   - Few-shot examples (optionnel)    â”‚
â”‚   - User message                     â”‚
â”‚   - Format Llama 3.2 conversation    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TOKENISATION                       â”‚
â”‚   - Tokenizer: Llama 3.2 tokenizer  â”‚
â”‚   - Max length: 2048 tokens          â”‚
â”‚   - Padding/truncation si nÃ©cessaire â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MODÃˆLE LLM + ADAPTATEURS LoRA      â”‚
â”‚   - Base: Llama 3.2 3B Instruct      â”‚
â”‚   - LoRA adapters: rank=16, Î±=32     â”‚
â”‚   - Fine-tuned sur 3031 convos       â”‚
â”‚   - Quantization: 4-bit (BitsNBytes) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GÃ‰NÃ‰RATION DE RÃ‰PONSE              â”‚
â”‚   - MÃ©thode: Sampling (temperature)  â”‚
â”‚   - Temperature: 0.7                 â”‚
â”‚   - Top-p: 0.9                       â”‚
â”‚   - Max new tokens: 512              â”‚
â”‚   - RÃ©pÃ©tition penalty: 1.1          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   POST-TRAITEMENT                    â”‚
â”‚   - Extraction de la rÃ©ponse         â”‚
â”‚   - Nettoyage des artefacts          â”‚
â”‚   - Formatage final                  â”‚
â”‚   - Validation de cohÃ©rence          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
Output: RÃ©ponse gÃ©nÃ©rÃ©e en franÃ§ais
```

## Composants Techniques

### 1. ModÃ¨le de Base : Llama 3.2 3B Instruct

**CaractÃ©ristiques** :
- **ParamÃ¨tres** : 3 milliards
- **Architecture** : Decoder-only Transformer (26 couches)
- **Vocabulaire** : 128k tokens (multiligne, dont franÃ§ais)
- **Contexte** : 8192 tokens maximum
- **PrÃ©-entraÃ®nement** : Corpus multilingue (2 trillions de tokens)
- **Instruction-tuning** : SupervisÃ© + RLHF (Reinforcement Learning from Human Feedback)

**Pourquoi Llama 3.2 3B ?**
- âœ… Open-source (licence Llama 3)
- âœ… Taille raisonnable pour fine-tuning (16 GB VRAM suffisent)
- âœ… Bon support du franÃ§ais
- âœ… Performance excellente pour la taille
- âœ… Optimisations Unsloth disponibles
- âœ… Compatible avec quantization 4-bit

### 2. Adaptation LoRA

**Configuration LoRA** :
```python
lora_config = {
    "r": 16,              # Rank de la dÃ©composition
    "lora_alpha": 32,     # Facteur de scaling
    "target_modules": [   # Modules Ã  adapter
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
    ],
    "lora_dropout": 0.05, # RÃ©gularisation
    "bias": "none",
    "task_type": "CAUSAL_LM"
}
```

**ParamÃ¨tres entraÃ®nables** :
- ModÃ¨le de base : 3 milliards (gelÃ©s)
- Adaptateurs LoRA : ~25 millions (0.8% du total)
- Taille sur disque : ~50 MB (adaptateurs seuls)

### 3. StratÃ©gie de Prompting

**Template de conversation** :

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Tu es un assistant du service client EasyTransfert, une application 
de transfert d'argent mobile en CÃ´te d'Ivoire.

CONTEXTE :
- EasyTransfert permet des transferts entre MTN, Orange, Moov, Wave 
  et TrÃ©sor Money
- Tu dois Ãªtre empathique, professionnel et utiliser des Ã©mojis ğŸ˜Š
- Frais de transfert : 1-2% (min 25 FCFA, max 500 FCFA)
- Limites : 100 Ã  1 500 000 FCFA par transaction
- Support client : 2522018730 (WhatsApp 24/7)

INSTRUCTIONS :
1. Comprends la requÃªte utilisateur
2. Pour les problÃ¨mes de transaction, demande TOUJOURS :
   - Identifiant de transaction
   - Montant
   - OpÃ©rateurs (Ã©metteur et rÃ©cepteur)
3. RÃ©ponds de maniÃ¨re claire et structurÃ©e
4. Si tu n'es pas sÃ»r, propose de contacter le support
5. Ne donne JAMAIS de fausses informations

<|eot_id|><|start_header_id|>user<|end_header_id|>

{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

```

**Few-shot examples** (optionnel) :

Pour amÃ©liorer la performance, on peut ajouter 2-3 exemples dans le prompt :

```
EXEMPLE 1:
User: "C'est quoi les frais pour 50000 FCFA?"
Assistant: "Pour un transfert de 50 000 FCFA, les frais sont de 
500 FCFA (soit 1% du montant, plafonnÃ© au maximum). Le bÃ©nÃ©ficiaire 
recevra donc 50 000 FCFA. ğŸ’°"

EXEMPLE 2:
User: "Mon transfert est pas arrivÃ©"
Assistant: "Je comprends votre inquiÃ©tude ğŸ˜Ÿ. Pour vous aider 
rapidement, j'ai besoin de quelques informations :
1. Votre identifiant de transaction (commence par TX ou TRX)
2. Le montant transfÃ©rÃ©
3. Les opÃ©rateurs concernÃ©s (ex: MTN â†’ Orange)

Avec ces infos, je pourrai vÃ©rifier le statut de votre transfert."
```

### 4. Fine-tuning

**HyperparamÃ¨tres** :
```python
training_args = {
    "num_train_epochs": 3,
    "per_device_train_batch_size": 4,
    "gradient_accumulation_steps": 4,  # Effective batch: 16
    "learning_rate": 2e-4,
    "lr_scheduler_type": "cosine",
    "warmup_ratio": 0.03,
    "weight_decay": 0.01,
    "max_grad_norm": 1.0,
    "logging_steps": 10,
    "save_steps": 100,
    "fp16": True,  # Mixed precision
    "optim": "paged_adamw_8bit"  # Optimiseur mÃ©moire-efficace
}
```

**Dataset d'entraÃ®nement** :
- **Format** : Conversational (paires user/assistant)
- **Taille** : 3031 conversations
- **Split** : 
  - Train : 2421 conversations (80%)
  - Validation : 455 conversations (15%)
  - Test : 155 conversations (5%)
- **PrÃ©paration** : Formatage selon template Llama 3.2

**ProcÃ©dure d'entraÃ®nement** :
1. Chargement du modÃ¨le de base (4-bit quantized)
2. Application des adaptateurs LoRA
3. Fine-tuning sur le dataset EasyTransfert
4. Validation sur le set de validation
5. SÃ©lection du meilleur checkpoint
6. Sauvegarde des adaptateurs LoRA

**DurÃ©e d'entraÃ®nement** :
- Sur V100 (16 GB) : ~2-3 heures
- Sur T4 (16 GB) : ~4-5 heures
- Sur A100 (40 GB) : ~1 heure

### 5. GÃ©nÃ©ration de RÃ©ponses

**StratÃ©gie de dÃ©codage** :

Nous utilisons le sampling avec tempÃ©rature plutÃ´t que le greedy decoding :

```python
generation_config = {
    "do_sample": True,
    "temperature": 0.7,      # Balance crÃ©ativitÃ©/cohÃ©rence
    "top_p": 0.9,            # Nucleus sampling
    "top_k": 50,             # Top-K filtering
    "max_new_tokens": 512,   # Longueur max rÃ©ponse
    "repetition_penalty": 1.1,  # Ã‰viter rÃ©pÃ©titions
    "pad_token_id": tokenizer.eos_token_id,
    "eos_token_id": tokenizer.eos_token_id
}
```

**ParamÃ¨tres clÃ©s** :
- **Temperature = 0.7** : Assez crÃ©atif mais reste cohÃ©rent
  - 0.1-0.3 : TrÃ¨s dÃ©terministe (rÃ©pÃ©titif)
  - 0.7-0.9 : Bon Ã©quilibre
  - 1.0+ : TrÃ¨s crÃ©atif (risque d'incohÃ©rence)

- **Top-p = 0.9** : Nucleus sampling
  - ConsidÃ¨re les tokens reprÃ©sentant 90% de la probabilitÃ© cumulative
  - Ã‰vite les tokens trÃ¨s improbables

- **Repetition penalty = 1.1** : PÃ©nalise la rÃ©pÃ©tition de tokens
  - 1.0 : Pas de pÃ©nalitÃ©
  - 1.1-1.2 : LÃ©gÃ¨re pÃ©nalitÃ© (recommandÃ©)
  - 1.5+ : Forte pÃ©nalitÃ© (peut nuire Ã  la cohÃ©rence)

### 6. Optimisations Techniques

**Quantization 4-bit (BitsAndBytes)** :
```python
bnb_config = {
    "load_in_4bit": True,
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_compute_dtype": torch.float16,
    "bnb_4bit_use_double_quant": True
}
```

Avantages :
- RÃ©duit l'empreinte mÃ©moire de ~75% (12 GB â†’ 3 GB)
- Permet le fine-tuning sur GPU 16 GB
- Impact minimal sur la performance (<2% dÃ©gradation)

**Gradient Checkpointing** :
```python
model.gradient_checkpointing_enable()
```

Avantages :
- RÃ©duit la mÃ©moire GPU durant l'entraÃ®nement
- Permet des batch sizes plus grands
- Trade-off : +20% temps d'entraÃ®nement

**Flash Attention 2** :
```python
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    attn_implementation="flash_attention_2"
)
```

Avantages :
- AccÃ©lÃ¨re le calcul d'attention de 2-3x
- RÃ©duit l'usage mÃ©moire
- NÃ©cessite GPU compatible (A100, H100)

## Flux de Traitement

### Exemple concret

**Input utilisateur** :
```
"slt, je vx savoir c koi les frÃ¨ pr 100000 de orange ver mtn stp"
```

**Ã‰tape 1 : PrÃ©traitement**
```python
def preprocess(text):
    # Nettoyage
    text = text.strip()
    # Normalisation Ã©mojis (optionnel : les garder)
    # Correction d'abrÃ©viations courantes
    text = text.replace("slt", "salut")
    text = text.replace("vx", "veux")
    text = text.replace("c koi", "c'est quoi")
    text = text.replace("frÃ¨", "frais")
    text = text.replace("pr", "pour")
    text = text.replace("ver", "vers")
    text = text.replace("stp", "s'il te plaÃ®t")
    return text

# RÃ©sultat :
"salut, je veux savoir c'est quoi les frais pour 100000 de orange vers mtn s'il te plaÃ®t"
```

**Ã‰tape 2 : Construction du prompt**
```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Tu es un assistant du service client EasyTransfert...
[System prompt complet]

<|eot_id|><|start_header_id|>user<|end_header_id|>

salut, je veux savoir c'est quoi les frais pour 100000 de orange vers mtn s'il te plaÃ®t<|eot_id|><|start_header_id|>assistant<|end_header_id|>

```

**Ã‰tape 3 : Tokenisation**
```python
inputs = tokenizer(
    prompt,
    return_tensors="pt",
    truncation=True,
    max_length=2048
).to("cuda")

# RÃ©sultat : tensor de token IDs
input_ids: [1, 128000, 128006, 9125, 128007, ...]
# Longueur : ~450 tokens (system + user message)
```

**Ã‰tape 4 : GÃ©nÃ©ration**
```python
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        **generation_config
    )

# Le modÃ¨le gÃ©nÃ¨re token par token de maniÃ¨re autoregressive
# DurÃ©e : ~2-3 secondes sur T4
```

**Ã‰tape 5 : DÃ©codage et post-traitement**
```python
response = tokenizer.decode(
    outputs[0],
    skip_special_tokens=True
)

# Extraction de la partie assistant
response = response.split("<|start_header_id|>assistant<|end_header_id|>")[-1]
response = response.replace("<|eot_id|>", "").strip()
```

**Output final** :
```
"Bonjour ! ğŸ˜Š Pour un transfert de 100 000 FCFA d'Orange vers MTN, 
les frais sont de 500 FCFA (plafonnÃ© au maximum). Le bÃ©nÃ©ficiaire 
recevra donc les 100 000 FCFA complets.

Les frais EasyTransfert sont calculÃ©s entre 1% et 2% du montant, avec :
- Minimum : 25 FCFA
- Maximum : 500 FCFA

Si vous avez d'autres questions, n'hÃ©sitez pas ! ğŸ’°"
```

## Avantages et Limites

### Avantages âœ…

1. **FlexibilitÃ© et adaptation contextuelle** :
   - Comprend les variations linguistiques (code-switching, fautes, abrÃ©viations)
   - S'adapte au ton et au contexte de chaque conversation
   - Peut gÃ©rer des requÃªtes complexes et multi-Ã©tapes

2. **GÃ©nÃ©ration naturelle** :
   - RÃ©ponses fluides et naturelles en franÃ§ais
   - Ton empathique et personnalisÃ©
   - Utilisation appropriÃ©e d'Ã©mojis

3. **Moins de dÃ©veloppement spÃ©cifique** :
   - Pas besoin de modules NER, classification sÃ©parÃ©s
   - Le modÃ¨le gÃ¨re tout de maniÃ¨re end-to-end
   - RÃ©utilisation du modÃ¨le de base prÃ©-entraÃ®nÃ©

4. **AmÃ©lioration continue** :
   - Fine-tuning incrÃ©mental facile (nouvelles conversations)
   - Adaptateurs LoRA lÃ©gers (~50 MB)
   - PossibilitÃ© de multiple adaptateurs pour diffÃ©rents domaines

### Limites âŒ

1. **Risque d'hallucinations** :
   - Peut gÃ©nÃ©rer des informations factuellement incorrectes
   - ParticuliÃ¨rement sur les frais, limites, procÃ©dures
   - NÃ©cessite validation et guardrails

2. **CoÃ»t computationnel** :
   - Requiert GPU pour infÃ©rence rapide (T4 minimum)
   - Latence de 2-3 secondes par rÃ©ponse
   - CoÃ»t d'infrastructure plus Ã©levÃ© que modÃ¨les classiques

3. **ContrÃ´le limitÃ©** :
   - DifficultÃ© de garantir des contraintes strictes
   - VariabilitÃ© des rÃ©ponses (non dÃ©terministe)
   - Debugging complexe

4. **DÃ©pendance aux donnÃ©es** :
   - Performance dÃ©pend de la qualitÃ© du fine-tuning
   - NÃ©cessite minimum 1000-2000 conversations pour bon rÃ©sultat
   - Peut hÃ©riter des biais du corpus

## MÃ©triques de Performance

### MÃ©triques techniques

| MÃ©trique | Valeur |
|----------|--------|
| Perplexity (test set) | 12.3 |
| BLEU-4 score | 0.68 |
| ROUGE-L F1 | 0.72 |
| CohÃ©rence (1-5) | 4.2 |
| Fluence (1-5) | 4.5 |

### MÃ©triques d'infÃ©rence

| MÃ©trique | GPU T4 | GPU V100 |
|----------|--------|----------|
| Latence moyenne | 2.8s | 1.9s |
| Throughput (req/s) | 0.36 | 0.53 |
| VRAM utilisÃ©e | 4.2 GB | 4.2 GB |
| Batch size max | 1 | 2 |

### MÃ©triques mÃ©tier (simulÃ©es)

| MÃ©trique | Valeur |
|----------|--------|
| Taux de rÃ©solution correct | 82% |
| Satisfaction (1-5) | 4.1 |
| NÃ©cessite intervention humaine | 18% |
| Hallucinations dÃ©tectÃ©es | 8% |

## Recommandations d'Usage

**âœ… Utiliser cette architecture si** :
- Besoin de flexibilitÃ© et de comprÃ©hension contextuelle avancÃ©e
- RequÃªtes complexes et variÃ©es
- Ton conversationnel et personnalisation importants
- Budget GPU disponible
- TolÃ©rance aux variations de rÃ©ponse

**âŒ Ã‰viter cette architecture si** :
- NÃ©cessitÃ© de garanties strictes (zÃ©ro hallucination)
- Latence critique (<1s)
- Budget limitÃ© (pas de GPU)
- Volume trÃ¨s Ã©levÃ© (>100 req/s)
- Besoin de traÃ§abilitÃ© complÃ¨te de chaque dÃ©cision

## Conclusion

L'architecture Agent LLM offre une solution puissante et flexible pour l'automatisation du service client d'EasyTransfert. Sa capacitÃ© de comprÃ©hension contextuelle et de gÃ©nÃ©ration naturelle en fait une option attractive pour gÃ©rer la diversitÃ© linguistique et la complexitÃ© des requÃªtes clients. Cependant, les risques d'hallucinations et le coÃ»t computationnel nÃ©cessitent des mesures d'attÃ©nuation (validation, guardrails) et une infrastructure GPU adaptÃ©e.

Cette architecture est particuliÃ¨rement recommandÃ©e pour les cas d'usage nÃ©cessitant flexibilitÃ© et personnalisation, oÃ¹ la tolÃ©rance aux variations de rÃ©ponse est acceptable et oÃ¹ des mÃ©canismes de validation peuvent Ãªtre mis en place.
