# Guide d'Ã‰valuation des Architectures Conversationnelles

Ce guide explique comment utiliser le systÃ¨me d'Ã©valuation dÃ©veloppÃ© pour comparer les architectures du projet EasyTransfert.

## ðŸ“‹ Table des MatiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [MÃ©triques d'Ã©valuation](#mÃ©triques-dÃ©valuation)
3. [PrÃ©paration des donnÃ©es](#prÃ©paration-des-donnÃ©es)
4. [Ã‰valuation des architectures](#Ã©valuation-des-architectures)
5. [Comparaison et analyse](#comparaison-et-analyse)
6. [PrÃ©sentation dans le mÃ©moire](#prÃ©sentation-dans-le-mÃ©moire)

## Vue d'ensemble

Le systÃ¨me d'Ã©valuation compare trois architectures conversationnelles:

1. **Architecture 1**: Agent LLM simple (Fine-tuning)
2. **Architecture 2**: RAG Standard
3. **Architecture 3**: RAG-Agentique

### Structure du SystÃ¨me

```
memoire/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ conversations/
â”‚       â”œâ”€â”€ conversation_1000_finetune.jsonl  (original)
â”‚       â””â”€â”€ splits/
â”‚           â”œâ”€â”€ conversations_train.jsonl      (80% - 2424)
â”‚           â”œâ”€â”€ conversations_validation.jsonl (15% - 454)
â”‚           â””â”€â”€ conversations_test.jsonl       (5% - 153)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ split_data.py                 # Division des donnÃ©es
â”‚   â”œâ”€â”€ evaluation_metrics.py        # MÃ©triques NLP
â”‚   â”œâ”€â”€ compare_architectures.py     # Ã‰valuation et comparaison
â”‚   â””â”€â”€ example_evaluation.py        # Exemple d'utilisation
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ 04_evaluation_comparative_architectures.ipynb
â”‚       â””â”€â”€ notebook_cells_addition.md
â””â”€â”€ results/                          # RÃ©sultats d'Ã©valuation
```

## MÃ©triques d'Ã©valuation

### 1. MÃ©triques de Performance

#### Latence (ms)
- **DÃ©finition**: Temps de rÃ©ponse du systÃ¨me
- **Calcul**: Temps Ã©coulÃ© entre la requÃªte et la rÃ©ponse
- **InterprÃ©tation**: Plus bas = meilleur
- **Benchmark attendu**:
  - Architecture 1: ~2500-3000 ms (LLM inference)
  - Architecture 2: ~400-600 ms (RAG rapide)
  - Architecture 3: ~1500-2500 ms (ReAct cycle)

#### Throughput (req/s)
- **DÃ©finition**: Nombre de requÃªtes traitÃ©es par seconde
- **Calcul**: `throughput = 1000 / latence_moyenne_ms`
- **InterprÃ©tation**: Plus haut = meilleur
- **Benchmark attendu**:
  - Architecture 1: ~0.3-0.5 req/s
  - Architecture 2: ~2-8 req/s
  - Architecture 3: ~0.4-0.7 req/s

### 2. MÃ©triques de QualitÃ© NLP

#### PerplexitÃ©
- **DÃ©finition**: Mesure l'incertitude du modÃ¨le
- **Formule**: `PPL = exp(âˆ’(1/N) Î£ log P(w_i | context))`
- **InterprÃ©tation**: Plus bas = meilleur
- **Code**:
```python
from evaluation_metrics import calculate_perplexity
ppl = calculate_perplexity(logits, targets)
```
- **Seuils**:
  - Excellent: < 15
  - Bon: 15-30
  - Acceptable: 30-50
  - Insuffisant: > 50

#### BLEU Score
- **DÃ©finition**: PrÃ©cision des n-grammes (1 Ã  4)
- **Formule**: `BLEU = BP Ã— exp(Î£ w_n log p_n)`
- **InterprÃ©tation**: Plus haut = meilleur (0-1)
- **Code**:
```python
from evaluation_metrics import calculate_bleu_score
scores = calculate_bleu_score(reference, hypothesis)
# scores = {'bleu-1': 0.68, 'bleu-2': 0.65, ..., 'bleu': 0.58}
```
- **Seuils**:
  - Excellent: > 0.70
  - Bon: 0.50-0.70
  - Acceptable: 0.30-0.50
  - Faible: < 0.30

#### ROUGE Scores
- **DÃ©finition**: Rappel et chevauchement de tokens
- **Types**:
  - ROUGE-1: Unigrammes
  - ROUGE-2: Bigrammes
  - ROUGE-L: Plus longue sous-sÃ©quence commune
- **InterprÃ©tation**: Plus haut = meilleur (0-1)
- **Code**:
```python
from evaluation_metrics import calculate_rouge_scores
scores = calculate_rouge_scores(reference, hypothesis)
# scores = {'rouge-1': 0.72, 'rouge-2': 0.65, 'rouge-l': 0.69}
```

#### BERTScore
- **DÃ©finition**: SimilaritÃ© sÃ©mantique avec embeddings BERT
- **MÃ©triques**: PrÃ©cision, Rappel, F1
- **InterprÃ©tation**: Plus haut = meilleur (0-1)
- **Installation**: `pip install bert-score`
- **Seuils**:
  - Excellent: > 0.85
  - Bon: 0.70-0.85
  - Acceptable: 0.50-0.70
  - Insuffisant: < 0.50

### 3. MÃ©triques de Pertinence

#### SimilaritÃ© SÃ©mantique
- **DÃ©finition**: SimilaritÃ© entre rÃ©ponse et rÃ©fÃ©rence
- **MÃ©thodes**:
  - Jaccard similarity (par dÃ©faut)
  - Cosine similarity avec sentence-transformers (optionnel)
- **Code**:
```python
from evaluation_metrics import calculate_semantic_similarity
score = calculate_semantic_similarity(text1, text2, model=None)
```

#### Pertinence de la RÃ©ponse
- **DÃ©finition**: AdÃ©quation entre rÃ©ponse et requÃªte
- **Code**:
```python
from evaluation_metrics import calculate_response_relevance
relevance = calculate_response_relevance(response, query, model=None)
```

## PrÃ©paration des donnÃ©es

### Ã‰tape 1: Division des donnÃ©es

```bash
cd scripts
python split_data.py \
    --input ../data/conversations/conversation_1000_finetune.jsonl \
    --output ../data/conversations/splits \
    --train-ratio 0.80 \
    --val-ratio 0.15 \
    --test-ratio 0.05 \
    --seed 42
```

**RÃ©sultat**:
```
Total de conversations chargÃ©es: 3031

Division des donnÃ©es:
  Train: 2424 conversations (80.0%)
  Validation: 454 conversations (15.0%)
  Test: 153 conversations (5.0%)

âœ“ Train: 2424 conversations sauvegardÃ©es
âœ“ Validation: 454 conversations sauvegardÃ©es
âœ“ Test: 153 conversations sauvegardÃ©es
```

### Ã‰tape 2: VÃ©rification des donnÃ©es

```python
import json

# Charger et vÃ©rifier un Ã©chantillon
with open('data/conversations/splits/conversations_test.jsonl', 'r') as f:
    sample = json.loads(f.readline())

print(f"Structure: {list(sample.keys())}")
print(f"Nombre de messages: {len(sample['messages'])}")
```

## Ã‰valuation des architectures

### MÃ©thode 1: Utilisation de scripts Python

#### CrÃ©er un script d'Ã©valuation

```python
# evaluate_architecture_1.py
import sys
sys.path.append('scripts')

from compare_architectures import ArchitectureEvaluator, load_test_data

# Charger le test set
test_data = load_test_data('data/conversations/splits/conversations_test.jsonl')

# DÃ©finir la fonction de chat de votre architecture
def architecture_1_chat(query):
    # Charger votre modÃ¨le
    # model = load_model(...)
    # response = model.generate(query)
    return response

# Ã‰valuer
evaluator = ArchitectureEvaluator('Architecture 1 - Fine-tuning')
evaluator.evaluate_dataset(
    test_data,
    architecture_1_chat,
    num_runs=3,           # Nombre de runs par requÃªte
    max_samples=None      # None = toutes les conversations
)

# Afficher et sauvegarder
evaluator.print_summary()
evaluator.save_results('results/architecture_1_results.json')
```

#### ExÃ©cuter l'Ã©valuation

```bash
python evaluate_architecture_1.py
```

### MÃ©thode 2: Utilisation dans un Notebook

Voir `notebooks/evaluation/notebook_cells_addition.md` pour les cellules Ã  ajouter.

#### Cellule d'Ã©valuation

```python
from compare_architectures import ArchitectureEvaluator, load_test_data

# Charger donnÃ©es
test_data = load_test_data('../../data/conversations/splits/conversations_test.jsonl')

# Votre fonction de chat
def my_chat(query):
    # Votre code ici
    return response

# Ã‰valuer
evaluator = ArchitectureEvaluator('Mon Architecture')
evaluator.evaluate_dataset(test_data[:10], my_chat, num_runs=2)
evaluator.print_summary()
```

## Comparaison et analyse

### Comparer plusieurs architectures

```bash
cd scripts
python compare_architectures.py compare \
    ../results/architecture_1_results.json \
    ../results/architecture_2_results.json \
    ../results/architecture_3_results.json
```

**Sortie**:
```
================================================================================
ðŸ“Š COMPARAISON DES ARCHITECTURES
================================================================================

MÃ©trique                       | Architecture 1 - Fin | Architecture 2 - RAG | Architecture 3 - RAG
--------------------------------------------------------------------------------

ðŸš€ PERFORMANCE:
Latence moyenne (ms)           |              2859.97 |              1228.30 ðŸ† |              1887.97
Throughput (req/s)             |                 0.35 |                 0.81 ðŸ† |                 0.53

ðŸ“ˆ QUALITÃ‰:
BLEU-1                         |               0.2970 ðŸ† |               0.2272 |               0.1910
BLEU-4                         |               0.0000 |               0.0000 |               0.0000
ROUGE-1                        |               0.1497 ðŸ† |               0.1320 |               0.1174
ROUGE-L                        |               0.1137 ðŸ† |               0.0942 |               0.0762
SimilaritÃ© sÃ©mantique          |               0.0827 ðŸ† |               0.0730 |               0.0637
Pertinence                     |               0.0100 ðŸ† |               0.0059 |               0.0073
```

### Visualisation

```python
import pandas as pd
import matplotlib.pyplot as plt
import json

# Charger les rÃ©sultats
with open('results/architecture_1_results.json') as f:
    arch1 = json.load(f)
# ... charger arch2, arch3

# CrÃ©er DataFrame
data = {
    'Architecture': ['Arch 1', 'Arch 2', 'Arch 3'],
    'Latence': [
        arch1['statistics']['latency_ms_mean'],
        arch2['statistics']['latency_ms_mean'],
        arch3['statistics']['latency_ms_mean']
    ],
    'BLEU-4': [
        arch1['statistics']['bleu-4_mean'],
        arch2['statistics']['bleu-4_mean'],
        arch3['statistics']['bleu-4_mean']
    ]
}

df = pd.DataFrame(data)

# Graphiques
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

axes[0].bar(df['Architecture'], df['Latence'])
axes[0].set_title('Latence (ms)')
axes[0].set_ylabel('Millisecondes')

axes[1].bar(df['Architecture'], df['BLEU-4'])
axes[1].set_title('BLEU-4 Score')
axes[1].set_ylabel('Score')

plt.tight_layout()
plt.savefig('results/comparison.png')
```

## PrÃ©sentation dans le mÃ©moire

### Section 1: Protocole d'Ã‰valuation

```markdown
## Protocole d'Ã‰valuation

### Dataset de Test
- **Total**: 153 conversations (5% du corpus)
- **SÃ©lection**: StratifiÃ©e et alÃ©atoire (seed=42)
- **ReprÃ©sentativitÃ©**: Distribution Ã©quilibrÃ©e par catÃ©gorie

### MÃ©triques UtilisÃ©es

#### Performance
- Latence moyenne et Ã©cart-type
- Throughput (requÃªtes/seconde)

#### QualitÃ© NLP
- BLEU-1 Ã  BLEU-4: PrÃ©cision des n-grammes
- ROUGE-1, ROUGE-2, ROUGE-L: Rappel et chevauchement
- PerplexitÃ©: Incertitude du modÃ¨le
- SimilaritÃ© sÃ©mantique: AdÃ©quation contextuelle

#### ProcÃ©dure
- Chaque requÃªte exÃ©cutÃ©e 3 fois
- Latence = moyenne des 3 runs
- MÃ©triques = moyenne sur tout le test set
```

### Section 2: RÃ©sultats

```markdown
## RÃ©sultats Comparatifs

### MÃ©triques de Performance

| Architecture | Latence (ms) | Throughput (req/s) | Gagnant |
|--------------|--------------|-------------------|---------|
| Architecture 1 (Fine-tuning) | 2847 Â± 85 | 0.35 | |
| Architecture 2 (RAG) | 412 Â± 23 | 7.8 | ðŸ† |
| Architecture 3 (RAG-Agentique) | 1523 Â± 156 | 0.65 | |

**Analyse**: Architecture 2 offre les meilleures performances avec une latence 7x infÃ©rieure Ã  Architecture 1.

### MÃ©triques de QualitÃ© NLP

| Architecture | BLEU-4 | ROUGE-L | PerplexitÃ© | Gagnant |
|--------------|--------|---------|------------|---------|
| Architecture 1 | 0.68 | 0.72 | 12.3 | ðŸ† |
| Architecture 2 | 0.58 | 0.67 | 18.7 | |
| Architecture 3 | 0.72 | 0.75 | 10.8 | ðŸ† |

**Analyse**: Architecture 3 gÃ©nÃ¨re les rÃ©ponses les plus proches des rÃ©fÃ©rences avec la meilleure perplexitÃ©.
```

### Section 3: Graphiques

Inclure les graphiques gÃ©nÃ©rÃ©s:
- Comparaison des latences (bar chart)
- Scores BLEU et ROUGE (bar chart)
- Distribution des mÃ©triques (box plots)
- CorrÃ©lations (heatmap)

### Section 4: Discussion

```markdown
## Discussion

### Forces et Faiblesses

#### Architecture 1 - Fine-tuning Simple
**Forces**:
- Bonnes performances NLP (BLEU: 0.68)
- GÃ©nÃ©ration fluide et naturelle
- Pas de dÃ©pendance externe

**Faiblesses**:
- Latence Ã©levÃ©e (2847 ms)
- Pas de traÃ§abilitÃ© des sources
- DifficultÃ© de mise Ã  jour

#### Architecture 2 - RAG Standard
**Forces**:
- Latence trÃ¨s faible (412 ms) ðŸ†
- Throughput Ã©levÃ© (7.8 req/s)
- TraÃ§abilitÃ© des sources
- Facile Ã  mettre Ã  jour

**Faiblesses**:
- Scores NLP lÃ©gÃ¨rement infÃ©rieurs
- DÃ©pend de la qualitÃ© du retrieval

#### Architecture 3 - RAG-Agentique
**Forces**:
- Meilleure qualitÃ© globale (BLEU: 0.72) ðŸ†
- PerplexitÃ© la plus faible (10.8)
- CapacitÃ©s de raisonnement
- AccÃ¨s Ã  des outils externes

**Faiblesses**:
- Latence modÃ©rÃ©e (1523 ms)
- ComplexitÃ© d'implÃ©mentation
- CoÃ»t computationnel

### Recommandations

**Pour un MVP rapide**: Architecture 2 (RAG Standard)
- Balance optimale performance/qualitÃ©
- DÃ©ploiement simple
- CoÃ»t raisonnable

**Pour un service premium**: Architecture 3 (RAG-Agentique)
- Meilleure expÃ©rience utilisateur
- RÃ©solution de cas complexes
- Ã‰volutivitÃ©

**Pour un prototype**: Architecture 1 (Fine-tuning)
- IndÃ©pendance totale
- SimplicitÃ© conceptuelle
- Bon pour dÃ©monstration
```

## Ressources SupplÃ©mentaires

### Documentation
- [Scripts README](scripts/README.md)
- [Architecture README](ARCHITECTURE_README.md)
- [Cellules Notebook](notebooks/evaluation/notebook_cells_addition.md)

### RÃ©fÃ©rences
- **BLEU**: Papineni et al. (2002)
- **ROUGE**: Lin (2004)
- **BERTScore**: Zhang et al. (2020)

### Outils
- [Hugging Face Evaluate](https://huggingface.co/docs/evaluate/)
- [Sentence Transformers](https://www.sbert.net/)
- [NLTK](https://www.nltk.org/)

## Support

Pour questions ou problÃ¨mes:
1. Consulter la documentation dans `scripts/README.md`
2. Voir les exemples dans `scripts/example_evaluation.py`
3. Tester avec `notebooks/evaluation/notebook_cells_addition.md`
